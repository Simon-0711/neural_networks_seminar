{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Pytorch\n",
    "to create endocder decoder ocr model with cnn and lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: /Users/I518302/Library/CloudStorage/OneDrive-SAPSE/SAPDevelop/University/Seq2Seq/neural_networks_seminar\n",
      "Absolute path: /Users/I518302/Library/CloudStorage/OneDrive-SAPSE/SAPDevelop/University/Seq2Seq/neural_networks_seminar/PubTabNet_cropped\n"
     ]
    }
   ],
   "source": [
    "# Setup path in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Get absolut path to proeject root\n",
    "load_dotenv()\n",
    "project_root_dir = os.path.dirname(os.path.abspath(\"./\"))\n",
    "print(\"Project root dir:\", project_root_dir)\n",
    "\n",
    "data_dir = os.getenv(\"PUBTABNET_CROPPED_DATA_DIR\")\n",
    "absolute_dir = project_root_dir + data_dir\n",
    "print(\"Absolute path:\", absolute_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_train = f\"{absolute_dir}/train\"\n",
    "image_dir_val = f\"{absolute_dir}/val\"\n",
    "label_file_train = f\"{absolute_dir}/PubTabNet_2.0.0_train_separated.json\"\n",
    "label_file_val = f\"{absolute_dir}/PubTabNet_2.0.0_val_separated.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from torch.utils.data import DataLoader\n",
    "label_file_small = f\"{absolute_dir}/subset_small.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_file_train = f\"{absolute_dir}/PubTabNet_2.0.0_train.json\"\n",
    "# label_file_val = f\"{absolute_dir}/PubTabNet_2.0.0_val.json\"\n",
    "\n",
    "# # Function to generate labels\n",
    "# import json\n",
    "# def generate_labels(json_data):\n",
    "#     labels = []\n",
    "#     for key, value in json_data.items():\n",
    "#         cells = value[\"html\"][\"cells\"]\n",
    "#         for i, cell in enumerate(cells):\n",
    "#             bbox = cell.get('bbox')\n",
    "#             tokens = cell.get('tokens')\n",
    "#             if bbox is None:\n",
    "#                 continue\n",
    "#             label = key.replace(\".png\", f\"_bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.png\")\n",
    "#             label = {\n",
    "#                 \"filename\": key.replace(\".png\", f\"_bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.png\"),\n",
    "#                 \"split\": value[\"split\"],\n",
    "#                 \"imgid\": value[\"imgid\"],\n",
    "#                 \"tokens\": tokens,\n",
    "#                 \"bbox\": bbox,\n",
    "#             }\n",
    "#             labels.append(label)\n",
    "#     return labels\n",
    "\n",
    "# # Generate labels\n",
    "# with open(label_file_val, 'r') as f:\n",
    "#         labels = json.load(f)\n",
    "#         result = generate_labels(labels)\n",
    "\n",
    "# # Specify the output file name\n",
    "# output_file_name = f\"{absolute_dir}/PubTabNet_2.0.0_val_separated.json\"\n",
    "\n",
    "# # Write the generated labels to a new JSON file\n",
    "# with open(output_file_name, 'w') as output_file:\n",
    "#     json.dump(result, output_file, indent=4)\n",
    "\n",
    "# print(f\"Generated labels have been written to {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform_list =  [transforms.Grayscale(1),\n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.5,), (0.5,))]\n",
    "transform = transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'label': 'Species', 'image': tensor([[[ 1.0000,  0.9451,  0.5765,  0.7569,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  0.9294,  0.8902,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.0196,  0.9686,  0.5922,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  0.8824,  0.8353,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000, -0.2941,  0.8353,  1.0000,  0.8980,  0.5765,  0.3961,\n",
      "           0.7412,  1.0000,  0.8039,  0.5529,  0.9608,  0.9294,  0.4745,\n",
      "           0.6392,  0.7490,  0.7804,  1.0000,  0.7020,  0.6078,  1.0000,\n",
      "           0.7020,  0.5608,  1.0000],\n",
      "         [ 1.0000,  0.6235, -0.3804,  0.4588,  0.7804, -0.2000,  0.6157,\n",
      "          -0.2706,  0.8980,  0.0118,  0.3490,  0.3725,  0.1765,  0.9765,\n",
      "           0.6863,  0.4275,  0.4039,  0.5608,  0.2314,  0.0588,  0.7020,\n",
      "           0.0588,  0.7255,  1.0000],\n",
      "         [ 0.9451,  0.9686,  0.9294, -0.2706,  0.9765, -0.0353,  1.0000,\n",
      "           0.0196,  0.6235,  0.2392,  1.0000,  0.9765, -0.1294,  1.0000,\n",
      "           1.0000,  0.5373,  0.4039,  0.2314,  0.6784,  1.0000,  1.0000,\n",
      "           0.2627, -0.1529,  1.0000],\n",
      "         [ 0.9137,  0.1294,  0.6078,  0.2627,  0.9765, -0.3333,  0.6314,\n",
      "           0.3725,  0.9686, -0.2471,  0.3333,  0.7569,  0.0824, -0.0275,\n",
      "           0.4745,  0.3333,  0.2078,  0.7098, -0.2078,  0.4275,  0.6941,\n",
      "           0.4275,  0.1294,  0.9686],\n",
      "         [ 1.0000,  0.9843,  0.9608,  1.0000,  0.9765, -0.0431,  0.9294,\n",
      "           1.0000,  1.0000,  0.9922,  0.9529,  1.0000,  1.0000,  0.9608,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  0.9686,  0.9843,  1.0000,\n",
      "           0.9529,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  0.7176,  0.0980,  0.9059,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000]]])}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "from custom_ocr_cnn_lstm.dataset import OCRDataset\n",
    "from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "train_dataset = OCRDatasetCropped(label_file_train, image_dir_train, transform=transform)\n",
    "val_dataset = OCRDatasetCropped(label_file_val, image_dir_val, transform=transform)\n",
    "\n",
    "# Accessing a sample\n",
    "sample = train_dataset[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Height: 84\n",
      "Maximum Width: 429\n"
     ]
    }
   ],
   "source": [
    "max_height = 0\n",
    "max_width = 0\n",
    "\n",
    "for item in train_dataset:\n",
    "    image = item['image']\n",
    "    height, width = image.shape[1], image.shape[2]\n",
    "    max_height = max(max_height, height)\n",
    "    max_width = max(max_width, width)\n",
    "\n",
    "print(f\"Maximum Height: {max_height}\")\n",
    "print(f\"Maximum Width: {max_width}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SynthCollator(object):\n",
    "    \n",
    "#     def __call__(self, batch):\n",
    "#         width = [item['image'].shape[2] for item in batch]\n",
    "#         indexes = [item['idx'] for item in batch]\n",
    "#         imgs = torch.ones([len(batch), batch[0]['image'].shape[0], batch[0]['image'].shape[1], \n",
    "#                            max(width)], dtype=torch.float32)\n",
    "#         for idx, item in enumerate(batch):\n",
    "#             try:\n",
    "#                 imgs[idx, :, :, 0:item['image'].shape[2]] = item['image']\n",
    "#             except:\n",
    "#                 continue\n",
    "#                 #print(imgs.shape)\n",
    "#         item = {'image': imgs, 'idx':indexes}\n",
    "#         if 'label' in batch[0].keys():\n",
    "#             labels = [item['label'] for item in batch]\n",
    "#             item['label'] = labels\n",
    "#         return item\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCollator(object):\n",
    "    \n",
    "    def __init__(self, target_height = max_height, target_width= max_width):\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        indexes = [item['idx'] for item in batch]\n",
    "        \n",
    "        # Resize images to the target size\n",
    "        resized_images = []\n",
    "        for item in batch:\n",
    "            image = item['image']\n",
    "            resized_image = F.interpolate(image.unsqueeze(0), size=(self.target_height, self.target_width), mode='bilinear', align_corners=False)\n",
    "            resized_images.append(resized_image.squeeze(0))\n",
    "        \n",
    "        # Stack resized images\n",
    "        imgs = torch.stack(resized_images, dim=0)\n",
    "        \n",
    "        item = {'image': imgs, 'idx': indexes}\n",
    "        \n",
    "        if 'label' in batch[0].keys():\n",
    "            labels = [item['label'] for item in batch]\n",
    "            item['label'] = labels\n",
    "        \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "from custom_ocr_cnn_lstm.dataset import OCRDataset\n",
    "from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=CustomCollator())\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=True, collate_fn=CustomCollator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "torch.Size([10, 1, 84, 429])\n",
      "0 tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]) 1\n"
     ]
    }
   ],
   "source": [
    "# Vielleicht liegt der Fehler hier. \n",
    "# Batch enthält jeweils 3 items die dann 10 elemente jeweils enthalten\n",
    "for batch_idx, samples in enumerate(train_loader):\n",
    "    print(\"------\")\n",
    "    print(samples[\"image\"].shape)\n",
    "    print(batch_idx, samples[\"image\"][0], samples[\"label\"][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_xml_tags(input_string):\n",
    "    pattern = r'<[^>]+>'\n",
    "    return re.sub(pattern, '', input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth labels loaded:  13690 [' S p e c i e s ', ' A n a j a ́ s ', ' P o r t e l ', ' S S B V ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' T o t a l ', ' ( % ) ', ' E v a n d r o m y i a   w a l k e r i ', '4 4', '4 0', '1', '1 5 6', '2 9 6', '1', '5 1', '1', '2', '5 9 2', '6 8 . 8 4', ' E v a n d r o m y i a   i n f r a s p i n o s a ', '4 4', '0', '3', '8 2', '1', '0', '0', '0', '0', '1 3 0', '1 5 . 1 2', ' N y s s o m y i a   a n t u n e s i  a  ', '1 1', '3', '3', '2 0', '3', '0', '1', '0', '0', '4 1', '4 . 7 7', ' M i c r o p y g o m y i a   r o r o t a e n s i s ', '2 0', '1', '0', '4', '0', '0', '2', '0', '0', '2 7', '3 . 1 4', ' S c i o p e m y i a   s o r d e l l i i ', '7', '1', '0', '1 3', '2', '0', '2', '0', '0', '2 5', '2 . 9 1', ' B i c h r o m o m y i a   f l a v i s c u t e l l a t a  a  ', '0', '0', '0', '4', '0', '0', '1 6', '0', '0', '2 0', '2 . 3 2', ' N y s s o m y i a   y u i l l i   y u i l l i ', '4', '0', '0', '0', '0', '0', '0', '0', '0', '4', '0 . 4 6', ' P s a t h y r o m y i a   a r a g a o i ']\n",
      "Character alphabet file created at: /Users/I518302/Library/CloudStorage/OneDrive-SAPSE/SAPDevelop/University/Seq2Seq/neural_networks_seminar/PubTabNet_cropped/character_alphabet.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def create_alphabet_file(alphabet, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        # f.write(\"START\\n\")\n",
    "        for char in alphabet:\n",
    "            f.write(char + \"\\n\")\n",
    "        # f.write(\"END\\n\")\n",
    "\n",
    "# Replace these with your actual ground truth labels\n",
    "train_labels = train_dataset.labels\n",
    "val_labels = train_dataset.labels\n",
    "# Concat both dicts\n",
    "labels = train_labels + val_labels\n",
    "\n",
    "alph_labels = []\n",
    "for label in labels:\n",
    "    label_tokens = label.get('tokens')\n",
    "    label = \" \".join(\" \".join(tokens) for tokens in label_tokens)\n",
    "    label = remove_xml_tags(label)\n",
    "    alph_labels.append(label)\n",
    "print(\"Ground truth labels loaded: \", len(alph_labels), alph_labels[:100])\n",
    "\n",
    "unique_chars = set()\n",
    "unique_radicals = set()\n",
    "\n",
    "# Loop through ground truth labels to extract unique characters and radicals\n",
    "for label in alph_labels:\n",
    "    for char in label:\n",
    "        unique_chars.add(char)\n",
    "        # You might need to extract radicals from each character here if using decomposition\n",
    "\n",
    "# Define the file paths for character and radical alphabets\n",
    "char_alphabet_file_path = f\"{absolute_dir}/character_alphabet.txt\"\n",
    "radical_alphabet_file_path = f\"{absolute_dir}/radical_alphabet.txt\"\n",
    "\n",
    "# Create character alphabet file\n",
    "create_alphabet_file(unique_chars, char_alphabet_file_path)\n",
    "print(f\"Character alphabet file created at: {char_alphabet_file_path}\")\n",
    "\n",
    "# Create radical alphabet file (if needed)\n",
    "# create_alphabet_file(unique_radicals, radical_alphabet_file_path)\n",
    "# print(f\"Radical alphabet file created at: {radical_alphabet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet loaded: 111 ['’', \"'\", 'S', '—', 'i', '×', '§', 'k', '′', '3', 'e', 'γ', '†', 'a', 'x', '/', '<', ' ', '%', 'Q', '0', 'A', ':', 'u', '°', 'G', '⁄', 'p', '>', 'Z', ']', 'w', 'E', 'L', 'c', 'N', 'y', 'α', 'b', '9', 'β', 'X', 'P', 's', '2', '7', 'q', '“', 'm', '(', '-', '−', 'r', 'W', '?', '5', '≤', 'j', '=', 'Y', 'o', '_', 'μ', '‐', 'h', 'f', '→', '·', 'B', 'C', ';', 'z', 't', 'H', 'n', 'O', ')', 'v', '4', '8', '≥', '`', '.', 'F', 'g', '&', 'K', ',', 'd', '6', '1', 'Δ', 'l', '”', 'V', '±', '#', '́', '+', 'R', 'I', 'J', '–', '*', 'M', 'D', '[', 'U', '$', 'λ', 'T']\n"
     ]
    }
   ],
   "source": [
    "# Load alpabet from file\n",
    "def load_alphabet(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        alphabet = f.read().splitlines()\n",
    "    return alphabet\n",
    "\n",
    "alphabet = load_alphabet(char_alphabet_file_path)\n",
    "print(\"Alphabet loaded:\", len(alphabet), alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum width in dataset: 454\n"
     ]
    }
   ],
   "source": [
    "def find_maximum_width(dataset):\n",
    "    # Initialize a list to store image widths\n",
    "    image_widths = []\n",
    "\n",
    "    # Iterate through the dataset and collect image widths\n",
    "    for sample in dataset:\n",
    "        image_width = sample['image'].shape[2]  # Get the width of the image\n",
    "        image_widths.append(image_width)\n",
    "\n",
    "    # Find the maximum width across all images\n",
    "    max_width = max(image_widths)\n",
    "\n",
    "    return max_width\n",
    "\n",
    "# Example usage:\n",
    "max_width_train = find_maximum_width(train_dataset)\n",
    "max_width_val = find_maximum_width(val_dataset)\n",
    "\n",
    "max_width = max(max_width_train, max_width_val)\n",
    "\n",
    "print(\"Maximum width in dataset:\", max_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomCTCLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m model \u001b[39m=\u001b[39m CNNLSTM_OCR(params)\n\u001b[1;32m     57\u001b[0m converter \u001b[39m=\u001b[39m OCRLabelConverter(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(params[\u001b[39m'\u001b[39m\u001b[39malphabet\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m---> 58\u001b[0m criterion \u001b[39m=\u001b[39m CustomCTCLoss()\n\u001b[1;32m     59\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     60\u001b[0m scheduler \u001b[39m=\u001b[39m CosineAnnealingLR(optimizer, T_max\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomCTCLoss' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup environment for training\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from utils import OCRLabelConverter\n",
    "from tqdm import *\n",
    "from cnn_lstm_ocr import CNNLSTM_OCR\n",
    "\n",
    "# params = {\n",
    "#     \"input_dim\": 256,\n",
    "#     \"hidden_dim\": 256,\n",
    "#     \"output_dim\": 1,\n",
    "#     \"input_planes\": 1,\n",
    "#     \"planes\": 1,\n",
    "#     \"schedule\": False,\n",
    "#     'image_height':32,\n",
    "#     'number_channels':1,\n",
    "#     'number_hidden_layers':256,\n",
    "#     'len_alphabet':len(alphabet),\n",
    "#     'learning_rate':0.001,\n",
    "#     'epochs':4,\n",
    "#     'batch_size':10,\n",
    "#     'model_dir':'model_history',\n",
    "#     'log_dir':'logs',\n",
    "#     'resume':False,\n",
    "#     'cuda':False,\n",
    "#     'schedule':False    \n",
    "# }\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": 1,  # Assuming grayscale images\n",
    "    \"alphabet\": alphabet,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"output_dim\": len(alphabet) + 1,  # Number of classes (including a blank symbol)\n",
    "    \"input_planes\": 1,  # Assuming grayscale images\n",
    "    \"planes\": 32,  # Adjust this value as needed\n",
    "    'image_height': 32,  # Adjust as needed\n",
    "    'number_channels': 1,  # Assuming grayscale images\n",
    "    'number_hidden_layers': 2,  # Adjust as needed\n",
    "    'len_alphabet': len(alphabet),  # Number of classes (excluding a blank symbol)\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 4,\n",
    "    'batch_size': 10,\n",
    "    'model_dir': 'model_history',\n",
    "    'log_dir': 'logs',\n",
    "    'resume': False,\n",
    "    'cuda': False,\n",
    "    'schedule': False,\n",
    "    'max_width': max_width\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = CNNLSTM_OCR(params)\n",
    "converter = OCRLabelConverter(''.join(params['alphabet']))\n",
    "criterion = CustomCTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'])\n",
    "batch_size = params['batch_size']\n",
    "count = 1\n",
    "epochs = params['epochs']\n",
    "cuda = params['cuda']\n",
    "print(\"Params:\", params)\n",
    "\n",
    "def train_dataloader():\n",
    "        # logging.info('training data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_train,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn,\n",
    "                shuffle=True)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from custom_ocr_cnn_lstm.cnn_lstm_ocr import CNNLSTM_OCR\n",
    "\n",
    "\n",
    "# Define your model, criterion, optimizer, and other parameters here\n",
    "\n",
    "# Initialize your model, criterion, and optimizer\n",
    "model = CNNLSTM_OCR(params)  # Assuming you've already defined CNNLSTM_OCR class\n",
    "# criterion = nn.CTCLoss()\n",
    "criterion = CustomCTCLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'])\n",
    "\n",
    "# Define training loop\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = batch['image'].to(device)\n",
    "        targets = batch['label']\n",
    "        # print(targets)\n",
    "        targets, lengths = converter.encode(targets)\n",
    "        print(\"len targets: \", len(targets))\n",
    "        # print(targets, lengths)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"cnn input inputs.shape:\", inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.contiguous().cpu()\n",
    "        outputs = torch.nn.functional.log_softmax(outputs, 2)\n",
    "        # b is batch size, T is input sequence and h is hidden size\n",
    "        b, T, h = outputs.size()\n",
    "        print(\"outputs shape:\", outputs)\n",
    "        # print(\"targets shape:\", targets.size())\n",
    "        pred_sizes = torch.LongTensor([T for i in range(b)])\n",
    "        print(\"pred_sizes shape:\", pred_sizes)\n",
    "        # Calculate the CTC loss\n",
    "        # output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "        print(outputs.shape, targets.shape, pred_sizes, lengths)\n",
    "        print(len(pred_sizes), len(lengths), len(targets), len(outputs))\n",
    "        loss = criterion(outputs, targets, pred_sizes, lengths)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Define validation method\n",
    "# def validate(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "#             inputs = batch['image'].to(device)\n",
    "#             targets = batch['label']\n",
    "#             # Assuming targets need to be converted to tensor, e.g., using a function `text_to_tensor`\n",
    "#             targets = text_to_tensor(targets) \n",
    "\n",
    "#             outputs = model(inputs)\n",
    "            \n",
    "#             # Calculate the CTC loss\n",
    "#             output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "#             loss = criterion(outputs, targets, input_lengths=None, target_lengths=output_lengths)\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if params['cuda'] and torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_dataloader = train_loader  # Assuming you have train_loader defined\n",
    "val_dataloader = val_loader  # Assuming you have val_loader defined\n",
    "\n",
    "for epoch in range(params['epochs']):\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "    # val_loss = validate(model, val_dataloader, criterion, device)\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{params['epochs']} - Train Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), params['model_dir'] + '/model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from itertools import groupby\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import torchvision.transforms.functional as TF\n",
    "from colorama import Fore\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nfor i in range(number_of_sequences):\\n    random_indices = np.random.randint(len(emnist_dataset.data), size=(digits_per_sequence,))\\n    random_digits_images = emnist_dataset.data[random_indices]\\n    transformed_random_digits_images = []\\n\\n    for img in random_digits_images:\\n        img = transforms.ToPILImage()(img)\\n        img = TF.rotate(img, -90, fill=0)\\n        img = TF.hflip(img)\\n        img = transforms.RandomAffine(degrees=10, translate=(0.2, 0.15), scale=(0.8, 1.1))(img)\\n        img = transforms.ToTensor()(img).numpy()\\n        transformed_random_digits_images.append(img)\\n\\n    random_digits_images = np.array(transformed_random_digits_images)\\n    random_digits_labels = emnist_dataset.targets[random_indices]\\n    random_sequence = np.hstack(random_digits_images.reshape((digits_per_sequence, 28, 28)))\\n    random_labels = np.hstack(random_digits_labels.reshape(digits_per_sequence, 1))\\n    dataset_sequences.append(random_sequence / 255)\\n    dataset_labels.append(random_labels)\\n\\ndataset_data = torch.Tensor(np.array(dataset_sequences))\\ndataset_labels = torch.IntTensor(np.array(dataset_labels))\\n\\nseq_dataset = data_utils.TensorDataset(dataset_data, dataset_labels)\\ntrain_set, val_set = torch.utils.data.random_split(seq_dataset,\\n                                                   [int(len(seq_dataset) * 0.8), int(len(seq_dataset) * 0.2)])\\n\\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\\nval_loader = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=True)\\n '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================= PREPARING DATASET ======================================================\n",
    "epochs = 3\n",
    "num_classes = len(alphabet)\n",
    "blank_label = 10\n",
    "image_height = 28\n",
    "gru_hidden_size = 128\n",
    "gru_num_layers = 2\n",
    "cnn_output_height = 29\n",
    "cnn_output_width = 93\n",
    "digits_per_sequence = 5\n",
    "number_of_sequences = 10000\n",
    "#emnist_dataset = datasets.EMNIST('./EMNIST', split=\"digits\", train=True, download=True)\n",
    "dataset_sequences = []\n",
    "dataset_labels = []\n",
    "\n",
    "\"\"\" \n",
    "for i in range(number_of_sequences):\n",
    "    random_indices = np.random.randint(len(emnist_dataset.data), size=(digits_per_sequence,))\n",
    "    random_digits_images = emnist_dataset.data[random_indices]\n",
    "    transformed_random_digits_images = []\n",
    "\n",
    "    for img in random_digits_images:\n",
    "        img = transforms.ToPILImage()(img)\n",
    "        img = TF.rotate(img, -90, fill=0)\n",
    "        img = TF.hflip(img)\n",
    "        img = transforms.RandomAffine(degrees=10, translate=(0.2, 0.15), scale=(0.8, 1.1))(img)\n",
    "        img = transforms.ToTensor()(img).numpy()\n",
    "        transformed_random_digits_images.append(img)\n",
    "\n",
    "    random_digits_images = np.array(transformed_random_digits_images)\n",
    "    random_digits_labels = emnist_dataset.targets[random_indices]\n",
    "    random_sequence = np.hstack(random_digits_images.reshape((digits_per_sequence, 28, 28)))\n",
    "    random_labels = np.hstack(random_digits_labels.reshape(digits_per_sequence, 1))\n",
    "    dataset_sequences.append(random_sequence / 255)\n",
    "    dataset_labels.append(random_labels)\n",
    "\n",
    "dataset_data = torch.Tensor(np.array(dataset_sequences))\n",
    "dataset_labels = torch.IntTensor(np.array(dataset_labels))\n",
    "\n",
    "seq_dataset = data_utils.TensorDataset(dataset_data, dataset_labels)\n",
    "train_set, val_set = torch.utils.data.random_split(seq_dataset,\n",
    "                                                   [int(len(seq_dataset) * 0.8), int(len(seq_dataset) * 0.2)])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=True)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Assume pubtabnet_json contains the data in the format you provided\n",
    "# Load the JSON data\n",
    "with open('/Users/I518302/Library/CloudStorage/OneDrive-SAPSE/SAPDevelop/University/Seq2Seq/neural_networks_seminar/custom_ocr_cnn_lstm/PubTabNet_2.0.0_train_separated.json', 'r') as f:\n",
    "    pubtabnet_data = json.load(f)\n",
    "\n",
    "# Build a set of unique characters from the tokens in the PubTabNet data\n",
    "unique_chars = set()\n",
    "for entry in pubtabnet_data:\n",
    "    unique_chars.update(entry['tokens'])\n",
    "unique_chars = sorted(list(unique_chars))\n",
    "\n",
    "# Create a mapping from characters to labels (including a blank label)\n",
    "char_to_label = {char: i for i, char in enumerate(unique_chars)}\n",
    "char_to_label['<blank>'] = len(unique_chars)\n",
    "num_classes = len(unique_chars) + 1  # Number of classes including the blank label\n",
    "\n",
    "# Define necessary parameters\n",
    "image_height = 128  # Height of the input images (you may need to adjust this based on your specific case)\n",
    "tokens_per_sequence = 5  # Number of tokens (characters) in each sequence\n",
    "number_of_sequences = len(pubtabnet_data)  # Total number of sequences in the dataset\n",
    "\n",
    "# Transformation to resize/crop the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_height, image_height)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_sequences = []\n",
    "dataset_labels = []\n",
    "\n",
    "# Iterate through the PubTabNet data\n",
    "for entry in pubtabnet_data:\n",
    "    filename = entry['filename']\n",
    "    bbox = entry['bbox']\n",
    "    tokens = entry['tokens']\n",
    "\n",
    "    # Load the image and apply the bounding box to crop\n",
    "    img = Image.open(os.path.join('/Users/I518302/Library/CloudStorage/OneDrive-SAPSE/SAPDevelop/University/Seq2Seq/neural_networks_seminar/PubTabNet_cropped/train', filename))\n",
    "    img_cropped = img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "    img_tensor = transform(img_cropped)\n",
    "\n",
    "    # Pad the image if needed to match the desired image_height\n",
    "    if img_tensor.shape[1] < image_height:\n",
    "        pad = torch.zeros((img_tensor.shape[0], image_height - img_tensor.shape[1], img_tensor.shape[2]))\n",
    "        img_tensor = torch.cat((img_tensor, pad), dim=1)\n",
    "\n",
    "    dataset_sequences.append(img_tensor)\n",
    "    \n",
    "    # Convert tokens to their respective labels\n",
    "    labels = [char_to_label[char] for char in tokens]\n",
    "    labels.extend([char_to_label['<blank>']] * (tokens_per_sequence - len(labels)))  # Pad with blank labels if necessary\n",
    "    dataset_labels.append(labels)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "def pad_sequence(sequence, max_length, padding_value):\n",
    "    pad_len = max(0, max_length - len(sequence))\n",
    "    return sequence + [padding_value] * pad_len\n",
    "\n",
    "# Define the maximum sequence length (adjust as needed)\n",
    "max_sequence_length = 470\n",
    "\n",
    "# Iterate through the PubTabNet data and pad the sequences\n",
    "padded_dataset_labels = []\n",
    "for labels in dataset_labels:\n",
    "    padded_labels = pad_sequence(labels, max_sequence_length, char_to_label['<blank>'])\n",
    "    padded_dataset_labels.append(padded_labels)\n",
    "\n",
    "# Convert to tensors\n",
    "dataset_sequences = torch.stack(dataset_sequences) / 255.0\n",
    "dataset_labels = torch.tensor(padded_dataset_labels)\n",
    "\n",
    "# Create TensorDataset\n",
    "seq_dataset = TensorDataset(dataset_sequences, dataset_labels)\n",
    "\n",
    "# Split into train and validation sets (adjust the split ratio as needed)\n",
    "train_set_size = int(0.8 * len(seq_dataset))\n",
    "train_set, val_set = torch.utils.data.random_split(seq_dataset, [train_set_size, len(seq_dataset) - train_set_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================= MODEL ==============================================================\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3))\n",
    "        self.norm1 = nn.InstanceNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=2)\n",
    "        self.norm2 = nn.InstanceNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
    "        self.norm3 = nn.InstanceNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=2)\n",
    "        self.norm4 = nn.InstanceNorm2d(64)\n",
    "        self.gru_input_size = cnn_output_height * 64\n",
    "        self.gru = nn.GRU(self.gru_input_size, gru_hidden_size, gru_num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(gru_hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.norm3(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.norm4(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        \n",
    "        out = out.permute(0, 3, 2, 1)\n",
    "        # Calculate the correct value for the -1 dimension\n",
    "        total_elements = out.numel()\n",
    "        correct_value_for_minus_one = total_elements // (batch_size * self.gru_input_size)\n",
    "\n",
    "        # Reshape with the correct value for -1\n",
    "        out = out.reshape(batch_size, correct_value_for_minus_one, self.gru_input_size)\n",
    "        # out = out.reshape(batch_size, -1, self.gru_input_size)\n",
    "        out, _ = self.gru(out)\n",
    "        out = torch.stack([F.log_softmax(self.fc(out[i]), dim=-1) for i in range(out.shape[0])])\n",
    "        return out\n",
    "\n",
    "\n",
    "model = CRNN().to(device)\n",
    "criterion = nn.CTCLoss(blank=blank_label, reduction='mean', zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = batch['image'].to(device)\n",
    "        targets = batch['label']\n",
    "        # print(targets)\n",
    "        targets, lengths = converter.encode(targets)\n",
    "        print(\"len targets: \", len(targets))\n",
    "        # print(targets, lengths)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"cnn input inputs.shape:\", inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.contiguous().cpu()\n",
    "        outputs = torch.nn.functional.log_softmax(outputs, 2)\n",
    "        # b is batch size, T is input sequence and h is hidden size\n",
    "        b, T, h = outputs.size()\n",
    "        print(\"outputs shape:\", outputs)\n",
    "        # print(\"targets shape:\", targets.size())\n",
    "        pred_sizes = torch.LongTensor([T for i in range(b)])\n",
    "        print(\"pred_sizes shape:\", pred_sizes)\n",
    "        # Calculate the CTC loss\n",
    "        # output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "        print(outputs.shape, targets.shape, pred_sizes, lengths)\n",
    "        print(len(pred_sizes), len(lengths), len(targets), len(outputs))\n",
    "        loss = criterion(outputs, targets, pred_sizes, lengths)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52110dc6593443e91969da8a3caf2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m val_dataloader \u001b[39m=\u001b[39m val_loader  \u001b[39m# Assuming you have val_loader defined\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, criterion, optimizer, device)\n\u001b[1;32m      7\u001b[0m     \u001b[39m# val_loss = validate(model, val_dataloader, criterion, device)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()  \u001b[39m# Adjust learning rate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     inputs \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     targets \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m     \u001b[39m# print(targets)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "train_dataloader = train_loader  # Assuming you have train_loader defined\n",
    "val_dataloader = val_loader  # Assuming you have val_loader defined\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "    # val_loss = validate(model, val_dataloader, criterion, device)\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{params['epochs']} - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(model.state_dict(), params['model_dir'] + '/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aec3f3e0747403295670582f6b08276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|\u001b[32m          \u001b[39m| 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING. Correct:  0 / 5476 = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf46e51e6c046e28e992620008780ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|\u001b[32m          \u001b[39m| 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING. Correct:  0 / 5476 = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87d913134044eb287c333605c556336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|\u001b[32m          \u001b[39m| 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING. Correct:  0 / 5476 = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'     val_correct = 0\\n    val_total = 0\\n    for x_val, y_val in tqdm(val_loader,\\n                             position=0, leave=True,\\n                             file=sys.stdout, bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.BLUE, Fore.RESET)):\\n        batch_size = x_val.shape[0]\\n        x_val = x_val.view(x_val.shape[0], 1, x_val.shape[2], x_val.shape[3])\\n        y_pred = model(x_val)\\n        y_pred = y_pred.permute(1, 0, 2)\\n        input_lengths = torch.IntTensor(batch_size).fill_(cnn_output_width)\\n        target_lengths = torch.IntTensor([len(t) for t in y_val])\\n        criterion(y_pred, y_val, input_lengths, target_lengths)\\n        _, max_index = torch.max(y_pred, dim=2)\\n        for i in range(batch_size):\\n            raw_prediction = list(max_index[:, i].detach().cpu().numpy())\\n            prediction = torch.IntTensor([c for c, _ in groupby(raw_prediction) if c != blank_label])\\n            if len(prediction) == len(y_val[i]) and torch.all(prediction.eq(y_val[i])):\\n                val_correct += 1\\n            val_total += 1\\n    print(\\'TESTING. Correct: \\', val_correct, \\'/\\', val_total, \\'=\\', val_correct / val_total) '"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================================ TRAINING MODEL ======================================================\n",
    "for _ in range(epochs):\n",
    "    # ============================================ TRAINING ============================================================\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for x_train, y_train in tqdm(train_loader,\n",
    "                                 position=0, leave=True,\n",
    "                                 file=sys.stdout, bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.GREEN, Fore.RESET)):\n",
    "        batch_size = x_train.shape[0]  # x_train.shape == torch.Size([64, 28, 140])\n",
    "        # print(\"x_train_shape: \", x_train.shape)\n",
    "        x_train = x_train.view(x_train.shape[0], 1, x_train.shape[2], -1)\n",
    "        # print(\"x_train_shape: \", x_train.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)\n",
    "        y_pred = y_pred.permute(1, 0, 2)  # y_pred.shape == torch.Size([64, 32, 11])\n",
    "        input_lengths = torch.IntTensor(batch_size).fill_(cnn_output_width)\n",
    "        target_lengths = torch.IntTensor([len(t) for t in y_train])\n",
    "        loss = criterion(y_pred, y_train, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, max_index = torch.max(y_pred, dim=2)  # max_index.shape == torch.Size([32, 64])\n",
    "        for i in range(batch_size):\n",
    "            raw_prediction = list(max_index[:, i].detach().cpu().numpy())  # len(raw_prediction) == 32\n",
    "            prediction = torch.IntTensor([c for c, _ in groupby(raw_prediction) if c != blank_label])\n",
    "            if len(prediction) == len(y_train[i]) and torch.all(prediction.eq(y_train[i])):\n",
    "                train_correct += 1\n",
    "            train_total += 1\n",
    "    print('TRAINING. Correct: ', train_correct, '/', train_total, '=', train_correct / train_total)\n",
    "\n",
    "    # ============================================ VALIDATION ==========================================================\n",
    "\"\"\"     val_correct = 0\n",
    "    val_total = 0\n",
    "    for x_val, y_val in tqdm(val_loader,\n",
    "                             position=0, leave=True,\n",
    "                             file=sys.stdout, bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.BLUE, Fore.RESET)):\n",
    "        batch_size = x_val.shape[0]\n",
    "        x_val = x_val.view(x_val.shape[0], 1, x_val.shape[2], x_val.shape[3])\n",
    "        y_pred = model(x_val)\n",
    "        y_pred = y_pred.permute(1, 0, 2)\n",
    "        input_lengths = torch.IntTensor(batch_size).fill_(cnn_output_width)\n",
    "        target_lengths = torch.IntTensor([len(t) for t in y_val])\n",
    "        criterion(y_pred, y_val, input_lengths, target_lengths)\n",
    "        _, max_index = torch.max(y_pred, dim=2)\n",
    "        for i in range(batch_size):\n",
    "            raw_prediction = list(max_index[:, i].detach().cpu().numpy())\n",
    "            prediction = torch.IntTensor([c for c, _ in groupby(raw_prediction) if c != blank_label])\n",
    "            if len(prediction) == len(y_val[i]) and torch.all(prediction.eq(y_val[i])):\n",
    "                val_correct += 1\n",
    "            val_total += 1\n",
    "    print('TESTING. Correct: ', val_correct, '/', val_total, '=', val_correct / val_total) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:  [ 32  14  12  18  27 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121 121\n",
      " 121 121]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 128, 128) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mactual: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(y_test[j]\u001b[39m.\u001b[39mnumpy()))\n\u001b[1;32m     16\u001b[0m mpl\u001b[39m.\u001b[39mrcParams[\u001b[39m\"\u001b[39m\u001b[39mfont.size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n\u001b[0;32m---> 17\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(x_test[j]\u001b[39m.\u001b[39;49msqueeze(), cmap\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgray\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m mpl\u001b[39m.\u001b[39mrcParams[\u001b[39m\"\u001b[39m\u001b[39mfont.size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m18\u001b[39m\n\u001b[1;32m     20\u001b[0m plt\u001b[39m.\u001b[39mgcf()\u001b[39m.\u001b[39mtext(x\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, s\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mActual: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(y_test[j]\u001b[39m.\u001b[39mnumpy()))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/linguistik/lib/python3.9/site-packages/matplotlib/pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[1;32m   2690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[1;32m   2691\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2692\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2693\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[1;32m   2694\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2695\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[1;32m   2696\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[1;32m   2697\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[1;32m   2698\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[1;32m   2699\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n\u001b[1;32m   2700\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m   2701\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   2702\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2703\u001b[0m     sci(__ret)\n\u001b[1;32m   2704\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/linguistik/lib/python3.9/site-packages/matplotlib/__init__.py:1446\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1445\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1446\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1448\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1449\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1450\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/linguistik/lib/python3.9/site-packages/matplotlib/axes/_axes.py:5663\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5656\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5657\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5658\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5659\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5660\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5661\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5663\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5664\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5665\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5666\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/linguistik/lib/python3.9/site-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 128, 128) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================ TESTING =================================================================\n",
    "number_of_test_imgs = 1\n",
    "test_loader = train_loader # torch.utils.data.DataLoader(val_set, batch_size=number_of_test_imgs, shuffle=True)\n",
    "test_preds = []\n",
    "(x_test, y_test) = next(iter(test_loader))\n",
    "y_pred = model(x_test.view(x_test.shape[0], 1, x_test.shape[2], -1))\n",
    "y_pred = y_pred.permute(1, 0, 2)\n",
    "_, max_index = torch.max(y_pred, dim=2)\n",
    "for i in range(x_test.shape[0]):\n",
    "    raw_prediction = list(max_index[:, i].detach().cpu().numpy())\n",
    "    prediction = torch.IntTensor([c for c, _ in groupby(raw_prediction) if c != blank_label])\n",
    "    test_preds.append(prediction)\n",
    "\n",
    "for j in range(len(x_test)):\n",
    "    print(\"actual: \", str(y_test[j].numpy()))\n",
    "    mpl.rcParams[\"font.size\"] = 8\n",
    "    plt.imshow(x_test[j].squeeze(), cmap='gray')\n",
    "    mpl.rcParams[\"font.size\"] = 18\n",
    "    \n",
    "    plt.gcf().text(x=0.1, y=0.1, s=\"Actual: \" + str(y_test[j].numpy()))\n",
    "    plt.gcf().text(x=0.1, y=0.2, s=\"Predicted: \" + str(test_preds[j].numpy()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s_learning_seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6a6a2f4e9b3d29343f27ef6aef311b54e340d4b6fc29835f858df5c984e5196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
