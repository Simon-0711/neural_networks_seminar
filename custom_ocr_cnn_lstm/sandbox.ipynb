{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Pytorch\n",
    "to create endocder decoder ocr model with cnn and lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar\n",
      "Absolute path: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/pubtabnet\n"
     ]
    }
   ],
   "source": [
    "# Setup path in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Get absolut path to proeject root\n",
    "load_dotenv()\n",
    "data_path = os.getenv(\"PUBTABNET_DATA_DIR\")\n",
    "\n",
    "project_root_dir = os.path.dirname(os.path.abspath(\"./\"))\n",
    "print(\"Project root dir:\", project_root_dir)\n",
    "\n",
    "data_dir = os.getenv(\"PUBTABNET_DATA_DIR\")\n",
    "absolute_dir = project_root_dir + data_dir\n",
    "print(\"Absolute path:\", absolute_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = f\"{absolute_dir}/train\"\n",
    "label_file = f\"{absolute_dir}/PubTabNet_2.0.0.jsonl\"\n",
    "output_file = f\"{absolute_dir}/subset_val.jsonl\"\n",
    "subset_size =10  # Number of entries in the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image names initialised\n",
      "Label data loaded\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.paddle_dataset import PaddleOCRDataset, ResizeNormalize\n",
    "from torch.utils.data import DataLoader\n",
    "transform = ResizeNormalize(size=(256, 256))\n",
    "label_file_small = f\"{absolute_dir}/subset_small.json\"\n",
    "train_dataset = PaddleOCRDataset(image_dir=image_dir, label_file=label_file_small, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth labels loaded:  10 ['  S p e c i e s     A n a j a ́ s     P o r t e l     S S B V                 W i l d     R u r a l     U r b a n     W i l d     R u r a l     U r b a n     W i l d     R u r a l     U r b a n     T o t a l     ( % )   <i> E v a n d r o m y i a   w a l k e r i </i> 4 4 4 0 1 1 5 6 2 9 6 1 5 1 1 2 5 9 2 6 8 . 8 4 <i> E v a n d r o m y i a   i n f r a s p i n o s a </i> 4 4 0 3 8 2 1 0 0 0 0 1 3 0 1 5 . 1 2 <i> N y s s o m y i a   a n t u n e s i <sup> a </sup> </i> 1 1 3 3 2 0 3 0 1 0 0 4 1 4 . 7 7 <i> M i c r o p y g o m y i a   r o r o t a e n s i s </i> 2 0 1 0 4 0 0 2 0 0 2 7 3 . 1 4 <i> S c i o p e m y i a   s o r d e l l i i </i> 7 1 0 1 3 2 0 2 0 0 2 5 2 . 9 1 <i> B i c h r o m o m y i a   f l a v i s c u t e l l a t a <sup> a </sup> </i> 0 0 0 4 0 0 1 6 0 0 2 0 2 . 3 2 <i> N y s s o m y i a   y u i l l i   y u i l l i </i> 4 0 0 0 0 0 0 0 0 4 0 . 4 6 <i> P s a t h y r o m y i a   a r a g a o i </i> 2 0 0 2 0 0 0 0 0 4 0 . 4 6 <i> P s a t h y r o m y i a   d e n d r o p h i l a </i> 4 0 0 0 0 0 0 0 0 4 0 . 4 6 <i> B r u m p t o m y i a </i>   s p . 1 0 0 1 0 0 0 0 0 2 0 . 2 3 <i> M i c r o p y g o m y i a   p i l o s a </i> 2 0 0 0 0 0 0 0 0 2 0 . 2 3 <i> P i n t o m y i a   n e v e s i </i> 0 0 0 2 0 0 0 0 0 2 0 . 2 3 <i> T h r i c o p y g o m y i a   t r i c h o p y g a </i> 0 0 0 2 0 0 0 0 0 2 0 . 2 3 <i> E v a n d r o m y i a   b a c u l a </i> 0 0 0 1 0 0 0 0 0 1 0 . 1 2 <i> E v a n d r o m y i a   f u r c a t a </i> 0 0 0 0 0 0 0 1 0 1 0 . 1 2 <i> E v a n d r o m y i a   m o n s t r u o s a </i> 0 0 0 1 0 0 0 0 0 1 0 . 1 2 <i> P s a t h y r o m y i a </i>   ( s e r i e s   <i> s h a n n o n i </i> )   s p . 0 0 0 0 0 0 1 0 0 1 0 . 1 2 <i> T r i c h o p h o r o m y i a </i>   s p .   n . 0 1 0 0 0 0 0 0 0 1 0 . 1 2 S u b t o t a l 1 3 9 4 6 7 2 8 8 3 0 2 1 7 3 2 2     T o t a l 1 9 2 5 9 1 7 7 8 6 0 1 0 0', '  M e t h o d     F u l l   S a m p l e   ( N   =   1 , 9 3 6 )     V o u c h e r   A r e a s   ( N   =   9 6 1 )     N o n - V o u c h e r   A r e a s   ( N   =   9 7 5 )     <i> P </i>   V a l u e   N o n e 7 1 . 8 7 3 . 7 7 0 . 0 . 0 7 T r a d i t i o n a l 4 . 4 3 . 9 4 . 8 . 3 5 M o d e r n 2 3 . 8 2 2 . 4 2 5 . 2 . 1 4   S h o r t - a c t i n g   m e t h o d s 2 1 . 3 2 0 . 6 2 2 . 1 . 4 4 P i l l / e m e r g e n c y   p i l l 1 1 . 7 1 0 . 6 1 2 . 8 . 1 3 M a l e / f e m a l e   c o n d o m s 1 . 0 0 . 7 1 . 2 . 2 6 I n j e c t a b l e s 8 . 6 9 . 3 8 . 1 . 3 2   L A R C s 1 . 7 1 . 4 1 . 9 . 3 0 I U D 1 . 1 0 . 4 1 . 6 . 0 0 8 I m p l a n t s 0 . 6 1 . 0 0 . 3 . 0 8   P e r m a n e n t   m e t h o d s 0 . 8 0 . 4 1 . 2 . 0 5', '  S t r a i n     S T   t y p e     P F G E   t y p e     G 4 7     G 3 7     G 1 1     G 6     G 5 7     G 1 8     G 5 1     G 3 2     G 2 0     G 4 3     G 3     G 2 1     G 3 3     G 2 3     G 4 6     G 6 3     G 8   A B 0 0 5 7 1 n d 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 A Y E 1 n d 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 1 A 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 3 8 9 1 1 B 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 3 8 8 7 1 C 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 2 9 7 9 2 0 D 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 3 1 3 0 2 0 E 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 A C I C U 2 n d 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 2 1 0 5 2 F 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 2 6 3 8 2 F 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 3 8 9 2 2 F 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 3 9 9 0 2 F 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 2 7 3 5 2 F 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 3 8 5 8 2 F 2 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 3 8 8 9 2 G 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 4 0 2 6 2 H 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 4 0 3 0 2 I 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 4 0 0 9 2 J 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 4 0 2 5 3 K 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 3 8 9 0 2 5 L 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 3 8 6 5 2 5 M 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 4 1 9 0 2 5 N 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 A T C C 1 7 9 7 8 7 7 n d 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 3 9 0 9 7 8 O 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 3 9 1 1 7 8 O 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 3 8 6 8 1 5 P 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 3 8 7 1 8 4 P 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1', '   P a t h o l o g i c   c o m p l e t e   r e s p o n s e   r a t e     T h e   d u r a t i o n   o f   t h e   p a t i e n t s     B l i n d   C o e f f i c i e n t −   0 . 3 1 4 −   0 . 3 6 5 0 . 2 4 3 S t a n d a r d   e r r o r 0 . 7 1 5 3 0 . 7 7 1 5 0 . 7 3 4 9 <i> P </i>   v a l u e 0 . 6 6 7 3 0 . 6 4 2 7 0 . 7 4 5 7 R D O R 0 . 7 3 0 . 6 9 1 . 2 7 [ 9 5 %   C I ] ( 0 . 1 6   t o   3 . 3 6 ) ( 0 . 1 3   t o   3 . 5 9 ) ( 0 . 2 7   t o   6 . 1 1 )', '  F o o d s     O l d w a y ’ s   P r e s e r v a t i o n   a n d   T r u s t   ( 2 0 0 9 )   [ 2 1 ]     M e d i t e r r a n e a n   D i e t   F o u n d a t i o n   ( 2 0 1 1 )   [ 5 ]     1 9 9 9   G r e e k   D i e t a r y   G u i d e l i n e s   ( 1 9 9 9 )   [ 2 2 ]   <sup> 1 </sup>   O l i v e   o i l E v e r y   m e a l E v e r y   m e a l M a i n   a d d e d   l i p i d V e g e t a b l e s E v e r y   m e a l ≥ 2   s e r v e s   e v e r y   m e a l 6   s e r v e s   d a i l y F r u i t s E v e r y   m e a l 1 – 2   s e r v e s   e v e r y   m e a l 3   s e r v e s   d a i l y B r e a d s   a n d   c e r e a l s E v e r y   m e a l 1 – 2   s e r v e s   e v e r y   m e a l 8   s e r v e s   d a i l y L e g u m e s E v e r y   m e a l ≥ 2   s e r v e s   w e e k l y 3 – 4   s e r v e s   w e e k l y N u t s E v e r y   m e a l 1 – 2   s e r v e s   d a i l y 3 – 4   s e r v e s   w e e k l y F i s h / S e a f o o d O f t e n ,   a t   l e a s t   t w o   t i m e s   p e r   w e e k ≥ 2   s e r v e s   w e e k l y 5 – 6   s e r v i n g s   w e e k l y E g g s M o d e r a t e   p o r t i o n s ,   d a i l y   t o   w e e k l y 2 – 4   s e r v e s   w e e k l y 3   s e r v i n g s   w e e k l y P o u l t r y M o d e r a t e   p o r t i o n s ,   d a i l y   t o   w e e k l y 2   s e r v e s   w e e k l y 4   s e r v i n g s   w e e k l y D a i r y   f o o d s M o d e r a t e   p o r t i o n s ,   d a i l y   t o   w e e k l y 2   s e r v e s   d a i l y 2   s e r v e s   d a i l y R e d   m e a t L e s s   o f t e n < 2   s e r v e s / w e e k 4   s e r v i n g s   m o n t h l y S w e e t s L e s s   o f t e n < 2   s e r v e s / w e e k 3   s e r v i n g s   w e e k l y R e d   w i n e I n   m o d e r a t i o n I n   m o d e r a t i o n   a n d   r e s p e c t i n g   s o c i a l   b e l i e f s D a i l y   i n   m o d e r a t i o n', '  C o u n t r y     N u m b e r   o f   p a s s e n g e r s <sup> a </sup>     A r r i v a l   d a t e s     I l l n e s s   o n s e t   d a t e     A d d i t i o n a l   r e f e r e n c e <sup> b </sup>   U n i t e d   S t a t e s 2 , 4 7 4 , 8 9 7 - M a r c h   2 8 - C a n a d a 1 0 1 , 3 1 3 A p r i l   8 A p r i l   1 1 - E l   S a l v a d o r 1 5 , 0 9 0 A p r i l   1 9 - - C o l o m b i a 2 4 , 5 3 5 - A p r i l   1 4 [ 2 6 ] U n i t e d   K i n g d o m 2 0 , 5 1 3 A p r i l   2 1 A p r i l   2 4 - S p a i n 6 5 , 7 2 4 A p r i l   2 2 A p r i l   2 5 - F r a n c e 6 1 , 9 6 0 - A p r i l   2 3 [ 1 8 ] C o s t a   R i c a 1 6 , 9 5 0 A p r i l   2 5 A p r i l   2 5 - A r g e n t i n a 2 4 , 6 0 9 A p r i l   2 5 A p r i l   2 7 [ 2 7 ] C u b a 4 2 , 8 0 2 A p r i l   2 5 - - T h e   N e t h e r l a n d s 2 7 , 6 4 0 A p r i l   2 7 - - G e r m a n y 3 5 , 7 7 2 - A p r i l   2 8 - H o n g   K o n g 3 5 , 7 0 6 A p r i l   3 0 A p r i l   3 0 [ 3 0 ] I t a l y 1 2 , 0 6 0 A p r i l   2 9 M a y   3 [ 3 1 ] G u a t e m a l a 3 9 , 4 6 0 - M a y   1 -', '  S .   n u m b e r     C o g g i n g   t o r q u e   r e d u c t i o n   m e t h o d     C o g g i n g   t o r q u e   r e d u c t i o n   ( % )     I n d u c e d   e m f   r e d u c t i o n   ( % )   1 C h a n g i n g   s l o t   o p e n i n g   w i d t h 5 1 . 1 0 1 . 4 8 2 C h a n g i n g   m a g n e t   p o l e   a r c   w i d t h 3 3 . 9 9 2 . 3 6 3 S h i f t i n g   o f   s l o t   o p e n i n g s 6 4 . 3 0 1 . 0 8 4 C o m b i n e d   e f f e c t   8 9 . 3 3 2 . 8 8', '   H o l m i c h   e t   a l .   [ 2 3 ]     M c C a r t h y   &   V i c e n z i n o   [ 2 4 ]     R o d r i g u e z   e t   a l .   [ 1 4 ]     W o l l i n   &   L o v e l l   [ 2 5 ]     V e r r a l l   e t   a l .   [ 1 5 ]     A i m   <i> T o   c o m p a r e   a n   a c t i v e   t r a i n i n g   p r o g r a m   w i t h   a   c o n v e n t i o n a l   p h y s i o t h e r a p y   p r o g r a m   i n   t h e   t r e a t m e n t   o f   s e v e r e   &   i n c a p a c i t a t i n g   a d d u c t o r - r e l a t e d   g r o i n   p a i n   i n   a t h l e t e s </i> <i> T o   d e s c r i b e   a n   a l t e r n a t e   a p p r o a c h   t o   a s s e s s m e n t   &   t r e a t m e n t   o f   o s t e i t i s   p u b i s </i> <i> T o   d e s c r i b e   t h e   p a t h o m e c h a n i c s ,   d i a g n o s t i c   p r o c e d u r e s ,   c l a s s i f i c a t i o n   &   c o n s e r v a t i v e   m a n a g e m e n t   o f   o s t e i t i s   p u b i s   s y n d r o m e   i n   e l i t e   s o c c e r   p l a y e r s </i> <i> T o   r e p o r t   o n   s u c c e s s f u l   r e h a b i l i t a t i o n   o u t c o m e s   a n d   t w o   n e w   p o s s i b l e   c l i n i c a l   i n d i c a t o r s   f o r   r e t u r n   t o   f o o t b a l l   p o s t   o s t e i t i s   p u b i s </i> <i> T o   d e t e r m i n e   t h e   o u t c o m e   o f   t r e a t i n g   c h r o n i c   g r o i n   i n j u r y   u s i n g   a   c o n s e r v a t i v e   ( n o n s u r g i c a l )   t r e a t m e n t   p r o g r a m </i>   S t u d y   D e s i g n   R a n d o m i s e d   C l i n i c a l   T r i a l C a s e   R e p o r t C a s e   R e p o r t C a s e   S e r i e s C a s e   S e r i e s   S a m p l e   S i z e   6 8 1 3 5   ( o v e r   8   y e a r   p e r i o d ) 4 2 7   L o c a t i o n   o f   S t u d y   D e n m a r k A u s t r a l i a M e x i c o A u s t r a l i a A u s t r a l i a   S p o r t s   I d e n t i f i e d   S o c c e r   &   o t h e r * G a e l i c   f o o t b a l l S o c c e r S o c c e r A u s t r a l i a n   r u l e s   F o o t b a l l', '  D o m a i n     F r e q u e n c y ( n ,   % )   N   =   1 1 2     E x a m p l e   S i t u a t i o n a l   a w a r e n e s s 5 8 ( 5 1 . 8 ) “ P a t i e n t   r e t u r n e d   f r o m   t h e a t r e   a f t e r   N O F   r e p a i r   a t   1 2 . 4 5 .   V i t a l   s i g n s   a t   1 4 . 1 5   s h o w   h y p o t e n s i o n   ( s e a g u l l   s i g n ) .   N o   f u r t h e r   o b s e r v a t i o n s   r e c o r d e d .   A t   1 4 . 3 0 ,   n u r s i n g   n o t e s   s t a t e   t h e   p a t i e n t   h a s   n o t   p a s s e d   u r i n e .   E x a m i n a t i o n   o f   t h e   f l u i d   b a l a n c e   c h a r t   s u g g e s t s   t h e   p a t i e n t   h a s   n o t   p a s s e d   u r i n e   a t   a l l   t h a t   d a y .   D i d   h e   i n   t h e a t r e ?   N o t   a c c o r d i n g   t o   t h e   a n a e s t h e t i c   c h a r t .   F l u i d   p r e s c r i p t i o n   c h a r t   s h o w s   5   b a g s   o f   f l u i d   g i v e n   n o t   r e f l e c t e d   o n   f l u i d   b a l a n c e   c h a r t [ s i c ] ” C o m m u n i c a t i o n   a n d   t e a m w o r k 2 3 ( 2 0 . 5 ) “ P a t i e n t s   c o n d i t i o n s   d e t e r i o r a t e d   a t   1 7 0 0   D r   P   [ s t a f f n a m e ]   i n f o r m e d .   H e   a t t e n d e d   t o   p a t i e n t   a n d   t r i e d   t o   c o n t a c t   t h e   o r t h o p a e d i c s   t e a m   w h i c h   h e   t r i e d   f o r   f o u r   h o u r s   t h e n   t o   f i n d   h e   w a s   n o t   o n   c a l l   a n d   w a s   o n   h o l i d a y   a b r o a d .   P a t i e n t ’ s   c o n d i t i o n   d e t e r i o r a t e d   f u r t h e r .   A n   a n a e s t h e t i s t   w a s   c o n t a c t   a n d   s a w   p a t i e n t . ” L e a d e r s h i p 1 8 ( 1 6 . 1 ) “ P a t i e n t   a d m i t t e d   w i t h   t r a u m a   t o   h i s   r i g h t   l o w e r   l e g   w a s   a d m i n i s t e r e d   a n t i - h y p e r t e n s i v e s   a n d   o t h e r   m e d i c a t i o n   p r e s c r i b e d   f o r   a n o t h e r   p a t i e n t .   T h e   p a t i e n t ’ s   c o n d i t i o n   d e t e r i o r a t e d   6   h o u r s   l a t e r   r e q u i r i n g   t r a n s f e r   t o   c r i t i c a l   c a r e   w h e r e   h e   s u b s e q u e n t l y   d i e d   a p p r o x i m a t e l y   3 8   h o u r s   f o l l o w i n g   t h e   m e d i c a t i o n   e r r o r .   N o   l e a d e r s h i p   o n   o r t h o p a e d i c s   w a r d   [ s i c ] ” D e c i s i o n - m a k i n g 1 3 ( 1 1 . 6 ) “ a d m i t t e d   f o r   N O F   r e p a i r ,   u n w e l l   f r o m   A &   E ,   s h o u l d   h a v e   h a d   3   l i t r e s   o f   f l u i d   a n d   2   u n i t s   o f   b l o o d   o v e r n i g h t   w i t h   r e p e a t e d   A B G s   a t   4   p m .   O v e r n i g h t   a p p a r e n t l y   u n r e c o r d a b l e   B P   b u t   n o   m e d i c a l   o p i n i o n   w a s   s o u g h t .   7   a m   A B G s   d o n e   b y   H O .   C o n d i t i o n   w o r s e n i n g - a t t e n t i o n   t h e n   b r o u g h t   f o r   l o w   B P .   S P R   u n s u r e   [ a b o u t ]   c o m i n g   t o   w a r d . . . b e c a u s e   o f   s e v e r i t y   o f   i l l n e s s   a n d   s t a f f i n g   l e v e l s .   S P R   c a l l e d   a n d   c e n t r a l   l i n e   i n s e r t e d   a n d   I V   f l u i d s   g i v e n .   P a t i e n t   d i e d   a t   1 3 . 0 0   h r s   R I P   [ s i c ] ”', '   n .   p t s .     %   <i> P a t i e n t s </i>   M i d d l e   a g e   ( r a n g e ) 6 8 . 3   y e a r s   ( 3 6 - 8 3 ) M a l e   ( m e a n   a g e ) 2 1   ( 7 3   y e a r s ) F e m a l e   ( m e a n   a g e ) 1 9   ( 6 5 . 9 ) P r e s e n c e   o f   r i s k   f a c t o r s     B i l i a r y   l i t h i a s i s 7 1 7 . 5   O t h e r   t u m o u r s 5 1 2 . 5 S y m p t o m a t i c   a t   d i a g n o s i s 2 9 7 2 . 5   A c u t e   P a n c r e a t i t i s 1 8 4 5 . 0   A b d o m i n a l   p a i n 4 1 0 . 0   D i a b e t e s 1 2 . 5   D i a r r h e a 1 2 . 5   J a u n d i c e 1 2 . 5   A n o r e x i a 1 2 . 5 T u m o r   m a r k e r s     R a i s e d   C a   1 9 . 9 3 7 . 5   R a i s e d   C E A 0 ']\n",
      "Character alphabet file created at: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/pubtabnet/character_alphabet.txt\n"
     ]
    }
   ],
   "source": [
    "def create_alphabet_file(alphabet, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"START\\n\")\n",
    "        for char in alphabet:\n",
    "            f.write(char + \"\\n\")\n",
    "        f.write(\"END\\n\")\n",
    "\n",
    "# Replace these with your actual ground truth labels\n",
    "gt_labels = train_loader.dataset.labels\n",
    "alph_labels = []\n",
    "for label in gt_labels:\n",
    "    label_tokens = [cell[\"tokens\"] for cell in label[\"html\"][\"cells\"]]\n",
    "    label = \" \".join(\" \".join(tokens) for tokens in label_tokens)\n",
    "    label = label.replace(\"<b>\", \" \").replace(\"</b>\", \" \")\n",
    "    alph_labels.append(label)\n",
    "print(\"Ground truth labels loaded: \", len(alph_labels), alph_labels)\n",
    "\n",
    "unique_chars = set()\n",
    "unique_radicals = set()\n",
    "\n",
    "# Loop through ground truth labels to extract unique characters and radicals\n",
    "for label in alph_labels:\n",
    "    for char in label:\n",
    "        unique_chars.add(char)\n",
    "        # You might need to extract radicals from each character here if using decomposition\n",
    "\n",
    "# Define the file paths for character and radical alphabets\n",
    "char_alphabet_file_path = f\"{absolute_dir}/character_alphabet.txt\"\n",
    "radical_alphabet_file_path = f\"{absolute_dir}/radical_alphabet.txt\"\n",
    "\n",
    "# Create character alphabet file\n",
    "create_alphabet_file(unique_chars, char_alphabet_file_path)\n",
    "print(f\"Character alphabet file created at: {char_alphabet_file_path}\")\n",
    "\n",
    "# Create radical alphabet file (if needed)\n",
    "# create_alphabet_file(unique_radicals, radical_alphabet_file_path)\n",
    "# print(f\"Radical alphabet file created at: {radical_alphabet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss\n",
    "from torch import nn\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "    def forward(self, input):\n",
    "        self.rnn.flatten_parameters()\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, opt, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        assert opt['imgH'] % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = opt['nChannels'] if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential()\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(opt['nHidden']*2, opt['nHidden'], opt['nHidden']),\n",
    "            BidirectionalLSTM(opt['nHidden'], opt['nHidden'], opt['nClasses']))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        # rnn features\n",
    "        output = self.rnn(conv)\n",
    "        output = output.transpose(1,0) #Tbh to bth\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CNN_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, params, leakyRelu=False):\n",
    "        super(CNN_LSTM_OCR, self).__init__()\n",
    "        self.input_dim = params.input_dim\n",
    "        self.hidden_dim = params.hidden_dim\n",
    "        self.output_dim = params.output_dim\n",
    "        self.input_planes = params.input_planes\n",
    "        self.planes = params.planes\n",
    "\n",
    "        # Define the CNN layers\n",
    "        # Use 1x1 convolutions for the remaining layers\n",
    "        self.conv_layer_1 = nn.Conv2d(self.input_planes, self.planes, kernel_size=1, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Use 3x3 convolutions for the remaining layers\n",
    "        self.conv_layer_2 = nn.Conv2d(self.input_planes, self.planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the CNN layers\n",
    "        out = self.conv_layer_1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool_1(out)\n",
    "        out = self.conv_layer_2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool_2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LSTM_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, params, leakyRelu=False):\n",
    "        super(LSTM_Decoder, self).__init__()\n",
    "        self.input_dim = params.input_dim\n",
    "        self.hidden_dim = params.hidden_dim\n",
    "        self.output_dim = params.output_dim\n",
    "\n",
    "        # Define the LSTM layers\n",
    "        self.lstm = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(self.hidden_dim * 2, self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the LSTM layers\n",
    "        output = self.lstm(x)\n",
    "        output = output.transpose(1,0) #Tbh to bth\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM_OCR(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(CNNLSTM_OCR, self).__init__()\n",
    "        self.cnn_encoder = CNN_Encoder(params)\n",
    "        self.lstm_decoder = LSTM_Decoder(params)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply the CNN encoder\n",
    "        out = self.cnn_encoder(x)\n",
    "        # Apply the LSTM decoder\n",
    "        out = self.lstm_decoder(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss\n",
    "class CustomCTCLoss(torch.nn.Module):\n",
    "    # T x B x H => Softmax on dimension 2\n",
    "    def __init__(self, dim=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "\n",
    "    def forward(self, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        EPS = 1e-7\n",
    "        loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes)\n",
    "        loss = self.sanitize(loss)\n",
    "        return self.debug(loss, logits, labels, prediction_sizes, target_sizes)\n",
    "    \n",
    "    def sanitize(self, loss):\n",
    "        EPS = 1e-7\n",
    "        if abs(loss.item() - float('inf')) < EPS:\n",
    "            return torch.zeros_like(loss)\n",
    "        if math.isnan(loss.item()):\n",
    "            return torch.zeros_like(loss)\n",
    "        return loss\n",
    "\n",
    "    def debug(self, loss, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Loss:\", loss)\n",
    "            print(\"logits:\", logits)\n",
    "            print(\"labels:\", labels)\n",
    "            print(\"prediction_sizes:\", prediction_sizes)\n",
    "            print(\"target_sizes:\", target_sizes)\n",
    "            raise Exception(\"NaN loss obtained. But why?\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment for training\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import six\n",
    "import random\n",
    "import lmdb\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "import logging\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import *\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": 1,\n",
    "    \"hidden_dim\": 1,\n",
    "    \"output_dim\": 1,\n",
    "    \"input_planes\": 1,\n",
    "    \"planes\": 1,\n",
    "    \"schedule\": False,\n",
    "\n",
    "    'image_height':32,\n",
    "    'number_channels':1,\n",
    "    'number_hidden_layers':256,\n",
    "    'len_alphabet':len(alphabet),\n",
    "    'learning_rate':0.001,\n",
    "    'epochs':4,\n",
    "    'batch_size':32,\n",
    "    'model_dir':'model_history',\n",
    "    'log_dir':'logs',\n",
    "    'resume':False,\n",
    "    'cuda':False,\n",
    "    'schedule':False    \n",
    "}\n",
    "\n",
    "\n",
    "model = CNNLSTM_OCR(params)\n",
    "criterion = CustomCTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=opt['epochs'])\n",
    "batch_size = params['batch_size']\n",
    "count = 1\n",
    "epochs = params['epochs']\n",
    "cuda = params['cuda']\n",
    "\n",
    "def init_meters(self):\n",
    "    self.avgTrainLoss = AverageMeter(\"Train loss\")\n",
    "    self.avgTrainCharAccuracy = AverageMeter(\"Train Character Accuracy\")\n",
    "    self.avgTrainWordAccuracy = AverageMeter(\"Train Word Accuracy\")\n",
    "    self.avgValLoss = AverageMeter(\"Validation loss\")\n",
    "    self.avgValCharAccuracy = AverageMeter(\"Validation Character Accuracy\")\n",
    "    self.avgValWordAccuracy = AverageMeter(\"Validation Word Accuracy\")\n",
    "\n",
    "def forward(self, x):\n",
    "    logits = self.model(x)\n",
    "    return logits.transpose(1, 0)\n",
    "\n",
    "def loss_fn(self, logits, targets, pred_sizes, target_sizes):\n",
    "    loss = self.criterion(logits, targets, pred_sizes, target_sizes)\n",
    "    return loss\n",
    "\n",
    "def step(self):\n",
    "    self.max_grad_norm = 0.05\n",
    "    clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "    self.optimizer.step()\n",
    "\n",
    "def schedule_lr(self):\n",
    "    if self.schedule:\n",
    "        self.scheduler.step()\n",
    "\n",
    "def _run_batch(self, batch, report_accuracy=False, validation=False):\n",
    "    input_, targets = batch['img'], batch['label']\n",
    "    targets, lengths = self.converter.encode(targets)\n",
    "    logits = self.forward(input_)\n",
    "    logits = logits.contiguous().cpu()\n",
    "    logits = torch.nn.functional.log_softmax(logits, 2)\n",
    "    T, B, H = logits.size()\n",
    "    pred_sizes = torch.LongTensor([T for i in range(B)])\n",
    "    targets= targets.view(-1).contiguous()\n",
    "    loss = self.loss_fn(logits, targets, pred_sizes, lengths)\n",
    "    if report_accuracy:\n",
    "        probs, preds = logits.max(2)\n",
    "        preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "        sim_preds = self.converter.decode(preds.data, pred_sizes.data, raw=False)\n",
    "        ca = np.mean((list(map(self.evaluator.char_accuracy, list(zip(sim_preds, batch['label']))))))\n",
    "        wa = np.mean((list(map(self.evaluator.word_accuracy, list(zip(sim_preds, batch['label']))))))\n",
    "    return loss, ca, wa\n",
    "\n",
    "def run_epoch(self, validation=False):\n",
    "    if not validation:\n",
    "        loader = self.train_dataloader()\n",
    "        pbar = tqdm(loader, desc='Epoch: [%d]/[%d] Training'%(self.count, \n",
    "            self.epochs), leave=True)\n",
    "        self.model.train()\n",
    "    else:\n",
    "        loader = self.val_dataloader()\n",
    "        pbar = tqdm(loader, desc='Validating', leave=True)\n",
    "        self.model.eval()\n",
    "    outputs = []\n",
    "    for batch_nb, batch in enumerate(pbar):\n",
    "        if not validation:\n",
    "            output = self.training_step(batch)\n",
    "        else:\n",
    "            output = self.validation_step(batch)\n",
    "        pbar.set_postfix(output)\n",
    "        outputs.append(output)\n",
    "    self.schedule_lr()\n",
    "    if not validation:\n",
    "        result = self.train_end(outputs)\n",
    "    else:\n",
    "        result = self.validation_end(outputs)\n",
    "    return result\n",
    "\n",
    "def training_step(self, batch):\n",
    "    loss, ca, wa = self._run_batch(batch, report_accuracy=True)\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.step()\n",
    "    output = OrderedDict({\n",
    "        'loss': abs(loss.item()),\n",
    "        'train_ca': ca.item(),\n",
    "        'train_wa': wa.item()\n",
    "        })\n",
    "    return output\n",
    "\n",
    "def validation_step(self, batch):\n",
    "    loss, ca, wa = self._run_batch(batch, report_accuracy=True, validation=True)\n",
    "    output = OrderedDict({\n",
    "        'val_loss': abs(loss.item()),\n",
    "        'val_ca': ca.item(),\n",
    "        'val_wa': wa.item()\n",
    "        })\n",
    "    return output\n",
    "\n",
    "def train_dataloader(self):\n",
    "    # logging.info('training data loader called')\n",
    "    loader = torch.utils.data.DataLoader(self.data_train,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            shuffle=True)\n",
    "    return loader\n",
    "    \n",
    "def val_dataloader(self):\n",
    "    # logging.info('val data loader called')\n",
    "    loader = torch.utils.data.DataLoader(self.data_val,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn)\n",
    "    return loader\n",
    "\n",
    "def train_end(self, outputs):\n",
    "    for output in outputs:\n",
    "        self.avgTrainLoss.add(output['loss'])\n",
    "        self.avgTrainCharAccuracy.add(output['train_ca'])\n",
    "        self.avgTrainWordAccuracy.add(output['train_wa'])\n",
    "\n",
    "    train_loss_mean = abs(self.avgTrainLoss.compute())\n",
    "    train_ca_mean = self.avgTrainCharAccuracy.compute()\n",
    "    train_wa_mean = self.avgTrainWordAccuracy.compute()\n",
    "\n",
    "    result = {'train_loss': train_loss_mean, 'train_ca': train_ca_mean,\n",
    "    'train_wa': train_wa_mean}\n",
    "    # result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': train_loss_mean}\n",
    "    return result\n",
    "\n",
    "def validation_end(self, outputs):\n",
    "    for output in outputs:\n",
    "        self.avgValLoss.add(output['val_loss'])\n",
    "        self.avgValCharAccuracy.add(output['val_ca'])\n",
    "        self.avgValWordAccuracy.add(output['val_wa'])\n",
    "\n",
    "    val_loss_mean = abs(self.avgValLoss.compute())\n",
    "    val_ca_mean = self.avgValCharAccuracy.compute()\n",
    "    val_wa_mean = self.avgValWordAccuracy.compute()\n",
    "\n",
    "    result = {'val_loss': val_loss_mean, 'val_ca': val_ca_mean,\n",
    "    'val_wa': val_wa_mean}\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
