{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Pytorch\n",
    "to create endocder decoder ocr model with cnn and lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar\n",
      "Absolute path: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/PubTabNet_cropped\n"
     ]
    }
   ],
   "source": [
    "# Setup path in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Get absolut path to proeject root\n",
    "load_dotenv()\n",
    "project_root_dir = os.path.dirname(os.path.abspath(\"./\"))\n",
    "print(\"Project root dir:\", project_root_dir)\n",
    "\n",
    "data_dir = os.getenv(\"PUBTABNET_CROPPED_DATA_DIR\")\n",
    "absolute_dir = project_root_dir + data_dir\n",
    "print(\"Absolute path:\", absolute_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_train = f\"{absolute_dir}/train\"\n",
    "image_dir_val = f\"{absolute_dir}/val\"\n",
    "label_file_train = f\"{absolute_dir}/PubTabNet_2.0.0_train_separated.json\"\n",
    "label_file_val = f\"{absolute_dir}/PubTabNet_2.0.0_val_separated.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from torch.utils.data import DataLoader\n",
    "label_file_small = f\"{absolute_dir}/subset_small.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_file_train = f\"{absolute_dir}/PubTabNet_2.0.0_train.json\"\n",
    "# label_file_val = f\"{absolute_dir}/PubTabNet_2.0.0_val.json\"\n",
    "\n",
    "# # Function to generate labels\n",
    "# import json\n",
    "# def generate_labels(json_data):\n",
    "#     labels = []\n",
    "#     for key, value in json_data.items():\n",
    "#         cells = value[\"html\"][\"cells\"]\n",
    "#         for i, cell in enumerate(cells):\n",
    "#             bbox = cell.get('bbox')\n",
    "#             tokens = cell.get('tokens')\n",
    "#             if bbox is None:\n",
    "#                 continue\n",
    "#             label = key.replace(\".png\", f\"_bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.png\")\n",
    "#             label = {\n",
    "#                 \"filename\": key.replace(\".png\", f\"_bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.png\"),\n",
    "#                 \"split\": value[\"split\"],\n",
    "#                 \"imgid\": value[\"imgid\"],\n",
    "#                 \"tokens\": tokens,\n",
    "#                 \"bbox\": bbox,\n",
    "#             }\n",
    "#             labels.append(label)\n",
    "#     return labels\n",
    "\n",
    "# # Generate labels\n",
    "# with open(label_file_val, 'r') as f:\n",
    "#         labels = json.load(f)\n",
    "#         result = generate_labels(labels)\n",
    "\n",
    "# # Specify the output file name\n",
    "# output_file_name = f\"{absolute_dir}/PubTabNet_2.0.0_val_separated.json\"\n",
    "\n",
    "# # Write the generated labels to a new JSON file\n",
    "# with open(output_file_name, 'w') as output_file:\n",
    "#     json.dump(result, output_file, indent=4)\n",
    "\n",
    "# print(f\"Generated labels have been written to {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform_list =  [transforms.Grayscale(1),\n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.5,), (0.5,))]\n",
    "transform = transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'label': 'Species', 'image': tensor([[[ 1.0000,  0.9451,  0.5765,  0.7569,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  0.9294,  0.8902,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.0196,  0.9686,  0.5922,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  0.8824,  0.8353,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000, -0.2941,  0.8353,  1.0000,  0.8980,  0.5765,  0.3961,\n",
      "           0.7412,  1.0000,  0.8039,  0.5529,  0.9608,  0.9294,  0.4745,\n",
      "           0.6392,  0.7490,  0.7804,  1.0000,  0.7020,  0.6078,  1.0000,\n",
      "           0.7020,  0.5608,  1.0000],\n",
      "         [ 1.0000,  0.6235, -0.3804,  0.4588,  0.7804, -0.2000,  0.6157,\n",
      "          -0.2706,  0.8980,  0.0118,  0.3490,  0.3725,  0.1765,  0.9765,\n",
      "           0.6863,  0.4275,  0.4039,  0.5608,  0.2314,  0.0588,  0.7020,\n",
      "           0.0588,  0.7255,  1.0000],\n",
      "         [ 0.9451,  0.9686,  0.9294, -0.2706,  0.9765, -0.0353,  1.0000,\n",
      "           0.0196,  0.6235,  0.2392,  1.0000,  0.9765, -0.1294,  1.0000,\n",
      "           1.0000,  0.5373,  0.4039,  0.2314,  0.6784,  1.0000,  1.0000,\n",
      "           0.2627, -0.1529,  1.0000],\n",
      "         [ 0.9137,  0.1294,  0.6078,  0.2627,  0.9765, -0.3333,  0.6314,\n",
      "           0.3725,  0.9686, -0.2471,  0.3333,  0.7569,  0.0824, -0.0275,\n",
      "           0.4745,  0.3333,  0.2078,  0.7098, -0.2078,  0.4275,  0.6941,\n",
      "           0.4275,  0.1294,  0.9686],\n",
      "         [ 1.0000,  0.9843,  0.9608,  1.0000,  0.9765, -0.0431,  0.9294,\n",
      "           1.0000,  1.0000,  0.9922,  0.9529,  1.0000,  1.0000,  0.9608,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  0.9686,  0.9843,  1.0000,\n",
      "           0.9529,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  0.7176,  0.0980,  0.9059,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000]]])}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "from custom_ocr_cnn_lstm.dataset import OCRDataset\n",
    "from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "train_dataset = OCRDatasetCropped(label_file_train, image_dir_train, transform=transform)\n",
    "val_dataset = OCRDatasetCropped(label_file_val, image_dir_val, transform=transform)\n",
    "\n",
    "# Accessing a sample\n",
    "sample = train_dataset[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Height: 84\n",
      "Maximum Width: 429\n"
     ]
    }
   ],
   "source": [
    "max_height = 0\n",
    "max_width = 0\n",
    "\n",
    "for item in train_dataset:\n",
    "    image = item['image']\n",
    "    height, width = image.shape[1], image.shape[2]\n",
    "    max_height = max(max_height, height)\n",
    "    max_width = max(max_width, width)\n",
    "\n",
    "print(f\"Maximum Height: {max_height}\")\n",
    "print(f\"Maximum Width: {max_width}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SynthCollator(object):\n",
    "    \n",
    "#     def __call__(self, batch):\n",
    "#         width = [item['image'].shape[2] for item in batch]\n",
    "#         indexes = [item['idx'] for item in batch]\n",
    "#         imgs = torch.ones([len(batch), batch[0]['image'].shape[0], batch[0]['image'].shape[1], \n",
    "#                            max(width)], dtype=torch.float32)\n",
    "#         for idx, item in enumerate(batch):\n",
    "#             try:\n",
    "#                 imgs[idx, :, :, 0:item['image'].shape[2]] = item['image']\n",
    "#             except:\n",
    "#                 continue\n",
    "#                 #print(imgs.shape)\n",
    "#         item = {'image': imgs, 'idx':indexes}\n",
    "#         if 'label' in batch[0].keys():\n",
    "#             labels = [item['label'] for item in batch]\n",
    "#             item['label'] = labels\n",
    "#         return item\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCollator(object):\n",
    "    \n",
    "    def __init__(self, target_height = max_height, target_width= max_width):\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        indexes = [item['idx'] for item in batch]\n",
    "        \n",
    "        # Resize images to the target size\n",
    "        resized_images = []\n",
    "        for item in batch:\n",
    "            image = item['image']\n",
    "            resized_image = F.interpolate(image.unsqueeze(0), size=(self.target_height, self.target_width), mode='bilinear', align_corners=False)\n",
    "            resized_images.append(resized_image.squeeze(0))\n",
    "        \n",
    "        # Stack resized images\n",
    "        imgs = torch.stack(resized_images, dim=0)\n",
    "        \n",
    "        item = {'image': imgs, 'idx': indexes}\n",
    "        \n",
    "        if 'label' in batch[0].keys():\n",
    "            labels = [item['label'] for item in batch]\n",
    "            item['label'] = labels\n",
    "        \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "from custom_ocr_cnn_lstm.dataset import OCRDataset\n",
    "from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=CustomCollator())\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=True, collate_fn=CustomCollator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "torch.Size([10, 1, 84, 429])\n",
      "0 tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]) 40\n"
     ]
    }
   ],
   "source": [
    "# Vielleicht liegt der Fehler hier. \n",
    "# Batch enthält jeweils 3 items die dann 10 elemente jeweils enthalten\n",
    "for batch_idx, samples in enumerate(train_loader):\n",
    "    print(\"------\")\n",
    "    print(samples[\"image\"].shape)\n",
    "    print(batch_idx, samples[\"image\"][0], samples[\"label\"][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_xml_tags(input_string):\n",
    "    pattern = r'<[^>]+>'\n",
    "    return re.sub(pattern, '', input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth labels loaded:  13690 [' S p e c i e s ', ' A n a j a ́ s ', ' P o r t e l ', ' S S B V ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' T o t a l ', ' ( % ) ', ' E v a n d r o m y i a   w a l k e r i ', '4 4', '4 0', '1', '1 5 6', '2 9 6', '1', '5 1', '1', '2', '5 9 2', '6 8 . 8 4', ' E v a n d r o m y i a   i n f r a s p i n o s a ', '4 4', '0', '3', '8 2', '1', '0', '0', '0', '0', '1 3 0', '1 5 . 1 2', ' N y s s o m y i a   a n t u n e s i  a  ', '1 1', '3', '3', '2 0', '3', '0', '1', '0', '0', '4 1', '4 . 7 7', ' M i c r o p y g o m y i a   r o r o t a e n s i s ', '2 0', '1', '0', '4', '0', '0', '2', '0', '0', '2 7', '3 . 1 4', ' S c i o p e m y i a   s o r d e l l i i ', '7', '1', '0', '1 3', '2', '0', '2', '0', '0', '2 5', '2 . 9 1', ' B i c h r o m o m y i a   f l a v i s c u t e l l a t a  a  ', '0', '0', '0', '4', '0', '0', '1 6', '0', '0', '2 0', '2 . 3 2', ' N y s s o m y i a   y u i l l i   y u i l l i ', '4', '0', '0', '0', '0', '0', '0', '0', '0', '4', '0 . 4 6', ' P s a t h y r o m y i a   a r a g a o i ']\n",
      "Character alphabet file created at: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/PubTabNet_cropped/character_alphabet.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def create_alphabet_file(alphabet, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        # f.write(\"START\\n\")\n",
    "        for char in alphabet:\n",
    "            f.write(char + \"\\n\")\n",
    "        # f.write(\"END\\n\")\n",
    "\n",
    "# Replace these with your actual ground truth labels\n",
    "train_labels = train_dataset.labels\n",
    "val_labels = train_dataset.labels\n",
    "# Concat both dicts\n",
    "labels = train_labels + val_labels\n",
    "\n",
    "alph_labels = []\n",
    "for label in labels:\n",
    "    label_tokens = label.get('tokens')\n",
    "    label = \" \".join(\" \".join(tokens) for tokens in label_tokens)\n",
    "    label = remove_xml_tags(label)\n",
    "    alph_labels.append(label)\n",
    "print(\"Ground truth labels loaded: \", len(alph_labels), alph_labels[:100])\n",
    "\n",
    "unique_chars = set()\n",
    "unique_radicals = set()\n",
    "\n",
    "# Loop through ground truth labels to extract unique characters and radicals\n",
    "for label in alph_labels:\n",
    "    for char in label:\n",
    "        unique_chars.add(char)\n",
    "        # You might need to extract radicals from each character here if using decomposition\n",
    "\n",
    "# Define the file paths for character and radical alphabets\n",
    "char_alphabet_file_path = f\"{absolute_dir}/character_alphabet.txt\"\n",
    "radical_alphabet_file_path = f\"{absolute_dir}/radical_alphabet.txt\"\n",
    "\n",
    "# Create character alphabet file\n",
    "create_alphabet_file(unique_chars, char_alphabet_file_path)\n",
    "print(f\"Character alphabet file created at: {char_alphabet_file_path}\")\n",
    "\n",
    "# Create radical alphabet file (if needed)\n",
    "# create_alphabet_file(unique_radicals, radical_alphabet_file_path)\n",
    "# print(f\"Radical alphabet file created at: {radical_alphabet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss\n",
    "class CustomCTCLoss(torch.nn.Module):\n",
    "    # T x B x H => Softmax on dimension 2\n",
    "    def __init__(self, dim=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "\n",
    "    def forward(self, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        EPS = 1e-7\n",
    "        loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes)\n",
    "        loss = self.sanitize(loss)\n",
    "        return self.debug(loss, logits, labels, prediction_sizes, target_sizes)\n",
    "    \n",
    "    def sanitize(self, loss):\n",
    "        EPS = 1e-7\n",
    "        if abs(loss.item() - float('inf')) < EPS:\n",
    "            return torch.zeros_like(loss)\n",
    "        if math.isnan(loss.item()):\n",
    "            return torch.zeros_like(loss)\n",
    "        return loss\n",
    "\n",
    "    def debug(self, loss, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Loss:\", loss)\n",
    "            print(\"logits:\", logits)\n",
    "            print(\"labels:\", labels)\n",
    "            print(\"prediction_sizes:\", prediction_sizes)\n",
    "            print(\"target_sizes:\", target_sizes)\n",
    "            raise Exception(\"NaN loss obtained. But why?\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet loaded: 111 [')', 'v', '9', 'u', '“', '1', 'm', '=', 'U', 'd', '%', 'e', 'b', 'l', 'Y', 'O', '5', 'E', \"'\", '#', 'R', ':', 'J', 'o', '°', '`', '-', 'β', 'n', '→', 'L', 'q', '+', 'I', '−', '6', 'A', ';', '·', 'S', '—', '–', 'α', '≤', 'h', '⁄', 'c', 'f', 'p', '0', '(', 'i', 'X', '&', 'F', '≥', 'r', '’', 'x', 'V', 'λ', 'P', 'W', 'N', 'G', '.', ',', 'B', 'μ', '′', '>', ' ', 'Z', '4', 't', '_', '$', '7', 'γ', 'M', '?', '”', 'a', 'j', 'D', '×', '§', '́', 'K', '[', 'H', 'z', 'g', 'T', 'C', 'y', '2', '*', '<', 'w', '†', 'k', '3', '±', 'Δ', '‐', '8', 's', ']', '/', 'Q']\n"
     ]
    }
   ],
   "source": [
    "# Load alpabet from file\n",
    "def load_alphabet(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        alphabet = f.read().splitlines()\n",
    "    return alphabet\n",
    "\n",
    "alphabet = load_alphabet(char_alphabet_file_path)\n",
    "print(\"Alphabet loaded:\", len(alphabet), alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum width in dataset: 454\n"
     ]
    }
   ],
   "source": [
    "def find_maximum_width(dataset):\n",
    "    # Initialize a list to store image widths\n",
    "    image_widths = []\n",
    "\n",
    "    # Iterate through the dataset and collect image widths\n",
    "    for sample in dataset:\n",
    "        image_width = sample['image'].shape[2]  # Get the width of the image\n",
    "        image_widths.append(image_width)\n",
    "\n",
    "    # Find the maximum width across all images\n",
    "    max_width = max(image_widths)\n",
    "\n",
    "    return max_width\n",
    "\n",
    "# Example usage:\n",
    "max_width_train = find_maximum_width(train_dataset)\n",
    "max_width_val = find_maximum_width(val_dataset)\n",
    "\n",
    "max_width = max(max_width_train, max_width_val)\n",
    "\n",
    "print(\"Maximum width in dataset:\", max_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(32, 256, bidirectional=True)\n",
      "Params: {'input_dim': 32, 'alphabet': [')', 'v', '9', 'u', '“', '1', 'm', '=', 'U', 'd', '%', 'e', 'b', 'l', 'Y', 'O', '5', 'E', \"'\", '#', 'R', ':', 'J', 'o', '°', '`', '-', 'β', 'n', '→', 'L', 'q', '+', 'I', '−', '6', 'A', ';', '·', 'S', '—', '–', 'α', '≤', 'h', '⁄', 'c', 'f', 'p', '0', '(', 'i', 'X', '&', 'F', '≥', 'r', '’', 'x', 'V', 'λ', 'P', 'W', 'N', 'G', '.', ',', 'B', 'μ', '′', '>', ' ', 'Z', '4', 't', '_', '$', '7', 'γ', 'M', '?', '”', 'a', 'j', 'D', '×', '§', '́', 'K', '[', 'H', 'z', 'g', 'T', 'C', 'y', '2', '*', '<', 'w', '†', 'k', '3', '±', 'Δ', '‐', '8', 's', ']', '/', 'Q'], 'hidden_dim': 256, 'output_dim': 112, 'input_planes': 1, 'planes': 32, 'image_height': 32, 'number_channels': 1, 'number_hidden_layers': 2, 'len_alphabet': 111, 'learning_rate': 0.001, 'epochs': 4, 'batch_size': 10, 'model_dir': 'model_history', 'log_dir': 'logs', 'resume': False, 'cuda': False, 'schedule': False, 'max_width': 454}\n"
     ]
    }
   ],
   "source": [
    "# Setup environment for training\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from utils import OCRLabelConverter\n",
    "from tqdm import *\n",
    "from cnn_lstm_ocr import CNNLSTM_OCR\n",
    "\n",
    "# params = {\n",
    "#     \"input_dim\": 256,\n",
    "#     \"hidden_dim\": 256,\n",
    "#     \"output_dim\": 1,\n",
    "#     \"input_planes\": 1,\n",
    "#     \"planes\": 1,\n",
    "#     \"schedule\": False,\n",
    "#     'image_height':32,\n",
    "#     'number_channels':1,\n",
    "#     'number_hidden_layers':256,\n",
    "#     'len_alphabet':len(alphabet),\n",
    "#     'learning_rate':0.001,\n",
    "#     'epochs':4,\n",
    "#     'batch_size':10,\n",
    "#     'model_dir':'model_history',\n",
    "#     'log_dir':'logs',\n",
    "#     'resume':False,\n",
    "#     'cuda':False,\n",
    "#     'schedule':False    \n",
    "# }\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": 1,  # Assuming grayscale images\n",
    "    \"alphabet\": alphabet,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"output_dim\": len(alphabet) + 1,  # Number of classes (including a blank symbol)\n",
    "    \"input_planes\": 1,  # Assuming grayscale images\n",
    "    \"planes\": 32,  # Adjust this value as needed\n",
    "    'image_height': 32,  # Adjust as needed\n",
    "    'number_channels': 1,  # Assuming grayscale images\n",
    "    'number_hidden_layers': 2,  # Adjust as needed\n",
    "    'len_alphabet': len(alphabet),  # Number of classes (excluding a blank symbol)\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 4,\n",
    "    'batch_size': 10,\n",
    "    'model_dir': 'model_history',\n",
    "    'log_dir': 'logs',\n",
    "    'resume': False,\n",
    "    'cuda': False,\n",
    "    'schedule': False,\n",
    "    'max_width': max_width\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = CNNLSTM_OCR(params)\n",
    "converter = OCRLabelConverter(''.join(params['alphabet']))\n",
    "criterion = CustomCTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'])\n",
    "batch_size = params['batch_size']\n",
    "count = 1\n",
    "epochs = params['epochs']\n",
    "cuda = params['cuda']\n",
    "print(\"Params:\", params)\n",
    "\n",
    "def train_dataloader():\n",
    "        # logging.info('training data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_train,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn,\n",
    "                shuffle=True)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import transforms\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from tqdm import tqdm\n",
    "# from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "# # Define a method for training the OCR model\n",
    "# def train_ocr_model(model, train_loader, optimizer, criterion, device, epoch, epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     progress_bar = tqdm(train_loader, desc='Epoch: [%d]/[%d] Training'%(epoch, \n",
    "#                 epochs), leave=True)\n",
    "\n",
    "#     for idx, batch in enumerate(progress_bar):\n",
    "#         optimizer.zero_grad()\n",
    "#         images, labels = batch['image'], batch['label']\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "        \n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         loss.backward()\n",
    "#         clip_grad_norm_(model.parameters(), max_norm=5)  # Clip gradients to prevent exploding gradients\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     return avg_loss\n",
    "\n",
    "# # Define a method for evaluating the OCR model on the validation dataset\n",
    "# def evaluate_ocr_model(model, val_loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(val_loader)\n",
    "#     return avg_loss\n",
    "\n",
    "# # Define a method for OCR inference\n",
    "# def ocr_inference(model, image, transform, device):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Preprocess the input image using the same transform as during training\n",
    "#         image = transform(image).unsqueeze(0).to(device)\n",
    "#         outputs = model(image)\n",
    "        \n",
    "#         # Perform any necessary post-processing on the model's outputs to get the OCR result\n",
    "#         # You may need to implement this part based on your specific OCR task\n",
    "    \n",
    "#     return ocr_result\n",
    "\n",
    "\n",
    "# # Set device (CPU or GPU) for training\n",
    "# device = torch.device(\"cuda\" if params['cuda'] and torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(params['epochs']):\n",
    "#     print(f\"Epoch {epoch + 1}/{params['epochs']}\")\n",
    "    \n",
    "#     # Train the model\n",
    "#     train_loss = train_ocr_model(model, train_loader, optimizer, criterion, device, epoch, params['epochs'])\n",
    "#     print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "#     # Validate the model\n",
    "#     val_loss = evaluate_ocr_model(model, val_loader, criterion, device)\n",
    "#     print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# # After training, you can use the trained model for inference on new images\n",
    "# # Example usage for OCR inference:\n",
    "# sample_image, sample_label = train_dataset[0]  # Load a sample image and label\n",
    "# ocr_result = ocr_inference(model, sample_image, transform, device)\n",
    "# print(\"OCR Result:\", ocr_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a mapping from characters to integers\n",
    "# char_to_int = {}\n",
    "# int_to_char = {}\n",
    "# with open(char_alphabet_file_path, 'r') as f:\n",
    "#     alphabet_lines = f.read().splitlines()\n",
    "\n",
    "# for i, char in enumerate(alphabet_lines[1:-1]):  # Skip \"START\" and \"END\" lines\n",
    "#     char_to_int[char] = i\n",
    "#     int_to_char[i] = char\n",
    "\n",
    "# # Define a special key for space character\n",
    "# SPACE_INT = len(char_to_int)  # Use the last index in the alphabet\n",
    "\n",
    "# # Implement text_to_tensor method\n",
    "# def text_to_tensor(text):\n",
    "#     # Convert text to a list of integers using the char_to_int mapping\n",
    "#     tensor = [char_to_int[char] if char in char_to_int else SPACE_INT for char in text]\n",
    "#     length = [len(text)]\n",
    "#     # return tensor\n",
    "#     return (torch.IntTensor(tensor), torch.IntTensor(length))\n",
    "\n",
    "# # Implement tensor_to_text method\n",
    "# def tensor_to_text(tensor):\n",
    "#     # Convert a tensor (list of integers) back to text using the int_to_char mapping\n",
    "#     text = \"\".join([int_to_char[idx] if idx != SPACE_INT else ' ' for idx in tensor])\n",
    "#     return text\n",
    "\n",
    "# # Example usage:\n",
    "# text = \"HELLO 5 7\"\n",
    "# tensor = text_to_tensor(text)\n",
    "# print(\"Text to Tensor:\", tensor)\n",
    "# text_reconstructed = tensor_to_text(tensor)\n",
    "# print(\"Tensor to Text:\", text_reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(32, 256, bidirectional=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/685 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len targets:  35\n",
      "cnn input x.shape: torch.Size([10, 1, 84, 429])\n",
      "Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Out shape:  torch.Size([10, 32, 1, 2247])\n",
      "cnn out.size(): 10, 32, 1, 2247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/685 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0639,  0.0610, -0.0938,  ...,  0.0248, -0.1122, -0.0470],\n",
      "        [ 0.0751,  0.0528, -0.0827,  ...,  0.0812, -0.0708, -0.0422],\n",
      "        [ 0.0774,  0.0524, -0.0876,  ...,  0.0847, -0.0572, -0.0472],\n",
      "        ...,\n",
      "        [-0.1025, -0.0141, -0.0914,  ...,  0.0726, -0.0042, -0.1355],\n",
      "        [-0.0956, -0.0131, -0.0859,  ...,  0.0708, -0.0072, -0.1272],\n",
      "        [-0.0864, -0.0103, -0.0781,  ...,  0.0666, -0.0154, -0.1172]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "outputs shape: tensor([[[-4.6532, -4.6561, -4.8111,  ..., -4.6924, -4.8294, -4.7641],\n",
      "         [-4.6472, -4.6694, -4.8051,  ..., -4.6411, -4.7932, -4.7644],\n",
      "         [-4.6465, -4.6714, -4.8117,  ..., -4.6392, -4.7812, -4.7710],\n",
      "         ...,\n",
      "         [-4.8151, -4.7268, -4.8041,  ..., -4.6401, -4.7169, -4.8482],\n",
      "         [-4.8087, -4.7263, -4.7990,  ..., -4.6424, -4.7204, -4.8404],\n",
      "         [-4.8006, -4.7245, -4.7923,  ..., -4.6476, -4.7296, -4.8314]],\n",
      "\n",
      "        [[-4.6532, -4.6561, -4.8109,  ..., -4.6923, -4.8293, -4.7641],\n",
      "         [-4.6472, -4.6695, -4.8049,  ..., -4.6410, -4.7931, -4.7645],\n",
      "         [-4.6465, -4.6715, -4.8114,  ..., -4.6391, -4.7810, -4.7711],\n",
      "         ...,\n",
      "         [-4.8151, -4.7268, -4.8041,  ..., -4.6401, -4.7169, -4.8482],\n",
      "         [-4.8087, -4.7263, -4.7990,  ..., -4.6424, -4.7204, -4.8404],\n",
      "         [-4.8006, -4.7245, -4.7923,  ..., -4.6476, -4.7296, -4.8314]],\n",
      "\n",
      "        [[-4.6532, -4.6561, -4.8109,  ..., -4.6923, -4.8293, -4.7641],\n",
      "         [-4.6472, -4.6695, -4.8049,  ..., -4.6410, -4.7931, -4.7645],\n",
      "         [-4.6465, -4.6715, -4.8114,  ..., -4.6391, -4.7810, -4.7711],\n",
      "         ...,\n",
      "         [-4.8151, -4.7268, -4.8041,  ..., -4.6401, -4.7169, -4.8482],\n",
      "         [-4.8087, -4.7263, -4.7990,  ..., -4.6424, -4.7204, -4.8404],\n",
      "         [-4.8006, -4.7245, -4.7923,  ..., -4.6476, -4.7296, -4.8314]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.6532, -4.6561, -4.8109,  ..., -4.6923, -4.8293, -4.7641],\n",
      "         [-4.6472, -4.6695, -4.8049,  ..., -4.6410, -4.7931, -4.7645],\n",
      "         [-4.6465, -4.6715, -4.8114,  ..., -4.6391, -4.7810, -4.7711],\n",
      "         ...,\n",
      "         [-4.8151, -4.7268, -4.8041,  ..., -4.6401, -4.7169, -4.8482],\n",
      "         [-4.8087, -4.7263, -4.7990,  ..., -4.6424, -4.7204, -4.8404],\n",
      "         [-4.8006, -4.7245, -4.7923,  ..., -4.6476, -4.7296, -4.8314]],\n",
      "\n",
      "        [[-4.6532, -4.6561, -4.8109,  ..., -4.6923, -4.8293, -4.7641],\n",
      "         [-4.6472, -4.6695, -4.8049,  ..., -4.6410, -4.7931, -4.7645],\n",
      "         [-4.6465, -4.6715, -4.8114,  ..., -4.6391, -4.7810, -4.7711],\n",
      "         ...,\n",
      "         [-4.8151, -4.7268, -4.8041,  ..., -4.6401, -4.7169, -4.8482],\n",
      "         [-4.8087, -4.7263, -4.7990,  ..., -4.6424, -4.7204, -4.8404],\n",
      "         [-4.8006, -4.7245, -4.7923,  ..., -4.6476, -4.7296, -4.8314]],\n",
      "\n",
      "        [[-4.6532, -4.6561, -4.8111,  ..., -4.6924, -4.8294, -4.7641],\n",
      "         [-4.6472, -4.6694, -4.8051,  ..., -4.6411, -4.7932, -4.7644],\n",
      "         [-4.6465, -4.6714, -4.8117,  ..., -4.6392, -4.7812, -4.7710],\n",
      "         ...,\n",
      "         [-4.8151, -4.7268, -4.8041,  ..., -4.6401, -4.7169, -4.8482],\n",
      "         [-4.8087, -4.7263, -4.7990,  ..., -4.6424, -4.7204, -4.8404],\n",
      "         [-4.8006, -4.7245, -4.7923,  ..., -4.6476, -4.7296, -4.8314]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "pred_sizes shape: tensor([2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247])\n",
      "torch.Size([10, 2247, 112]) torch.Size([35]) tensor([2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247]) tensor([4, 2, 1, 3, 5, 2, 9, 4, 1, 4], dtype=torch.int32)\n",
      "10 10 35 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input_lengths must be of size batch_size",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb Cell 23\u001B[0m in \u001B[0;36m<cell line: 83>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=80'>81</a>\u001B[0m val_dataloader \u001B[39m=\u001B[39m val_loader  \u001B[39m# Assuming you have val_loader defined\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=82'>83</a>\u001B[0m \u001B[39mfor\u001B[39;00m epoch \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(params[\u001B[39m'\u001B[39m\u001B[39mepochs\u001B[39m\u001B[39m'\u001B[39m]):\n\u001B[0;32m---> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=83'>84</a>\u001B[0m     train_loss \u001B[39m=\u001B[39m train(model, train_dataloader, criterion, optimizer, device)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=84'>85</a>\u001B[0m     \u001B[39m# val_loss = validate(model, val_dataloader, criterion, device)\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=85'>86</a>\u001B[0m     scheduler\u001B[39m.\u001B[39mstep()  \u001B[39m# Adjust learning rate\u001B[39;00m\n",
      "\u001B[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb Cell 23\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, dataloader, criterion, optimizer, device)\u001B[0m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=43'>44</a>\u001B[0m \u001B[39mprint\u001B[39m(outputs\u001B[39m.\u001B[39mshape, targets\u001B[39m.\u001B[39mshape, pred_sizes, lengths)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=44'>45</a>\u001B[0m \u001B[39mprint\u001B[39m(\u001B[39mlen\u001B[39m(pred_sizes), \u001B[39mlen\u001B[39m(lengths), \u001B[39mlen\u001B[39m(targets), \u001B[39mlen\u001B[39m(outputs))\n\u001B[0;32m---> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=45'>46</a>\u001B[0m loss \u001B[39m=\u001B[39m criterion(outputs, targets, pred_sizes, lengths)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=47'>48</a>\u001B[0m loss\u001B[39m.\u001B[39mbackward()\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=48'>49</a>\u001B[0m optimizer\u001B[39m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49m\u001B[39minput\u001B[39;49m, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1195\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "\u001B[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb Cell 23\u001B[0m in \u001B[0;36mCustomCTCLoss.forward\u001B[0;34m(self, logits, labels, prediction_sizes, target_sizes)\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, logits, labels,\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001B[0m         prediction_sizes, target_sizes):\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001B[0m     EPS \u001B[39m=\u001B[39m \u001B[39m1e-7\u001B[39m\n\u001B[0;32m---> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001B[0m     loss \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mctc_loss(logits, labels, prediction_sizes, target_sizes)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=12'>13</a>\u001B[0m     loss \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39msanitize(loss)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X56sZmlsZQ%3D%3D?line=13'>14</a>\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdebug(loss, logits, labels, prediction_sizes, target_sizes)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49m\u001B[39minput\u001B[39;49m, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1195\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/torch/nn/modules/loss.py:1756\u001B[0m, in \u001B[0;36mCTCLoss.forward\u001B[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001B[0m\n\u001B[1;32m   1755\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m Tensor:\n\u001B[0;32m-> 1756\u001B[0m     \u001B[39mreturn\u001B[39;00m F\u001B[39m.\u001B[39;49mctc_loss(log_probs, targets, input_lengths, target_lengths, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mblank, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mreduction,\n\u001B[1;32m   1757\u001B[0m                       \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mzero_infinity)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/torch/nn/functional.py:2628\u001B[0m, in \u001B[0;36mctc_loss\u001B[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001B[0m\n\u001B[1;32m   2621\u001B[0m \u001B[39mif\u001B[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001B[1;32m   2622\u001B[0m     \u001B[39mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   2623\u001B[0m         ctc_loss,\n\u001B[1;32m   2624\u001B[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001B[1;32m   2625\u001B[0m         log_probs, targets, input_lengths, target_lengths,\n\u001B[1;32m   2626\u001B[0m         blank\u001B[39m=\u001B[39mblank, reduction\u001B[39m=\u001B[39mreduction, zero_infinity\u001B[39m=\u001B[39mzero_infinity\n\u001B[1;32m   2627\u001B[0m     )\n\u001B[0;32m-> 2628\u001B[0m \u001B[39mreturn\u001B[39;00m torch\u001B[39m.\u001B[39;49mctc_loss(\n\u001B[1;32m   2629\u001B[0m     log_probs, targets, input_lengths, target_lengths, blank, _Reduction\u001B[39m.\u001B[39;49mget_enum(reduction), zero_infinity\n\u001B[1;32m   2630\u001B[0m )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: input_lengths must be of size batch_size"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from custom_ocr_cnn_lstm.cnn_lstm_ocr import CNNLSTM_OCR\n",
    "\n",
    "\n",
    "# Define your model, criterion, optimizer, and other parameters here\n",
    "\n",
    "# Initialize your model, criterion, and optimizer\n",
    "model = CNNLSTM_OCR(params)  # Assuming you've already defined CNNLSTM_OCR class\n",
    "# criterion = nn.CTCLoss()\n",
    "criterion = CustomCTCLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'])\n",
    "\n",
    "# Define training loop\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = batch['image'].to(device)\n",
    "        targets = batch['label']\n",
    "        # print(targets)\n",
    "        targets, lengths = converter.encode(targets)\n",
    "        print(\"len targets: \", len(targets))\n",
    "        # print(targets, lengths)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"cnn input inputs.shape:\", inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.contiguous().cpu()\n",
    "        outputs = torch.nn.functional.log_softmax(outputs, 2)\n",
    "        # b is batch size, T is input sequence and h is hidden size\n",
    "        b, T, h = outputs.size()\n",
    "        print(\"outputs shape:\", outputs)\n",
    "        # print(\"targets shape:\", targets.size())\n",
    "        pred_sizes = torch.LongTensor([T for i in range(b)])\n",
    "        print(\"pred_sizes shape:\", pred_sizes)\n",
    "        # Calculate the CTC loss\n",
    "        # output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "        print(outputs.shape, targets.shape, pred_sizes, lengths)\n",
    "        print(len(pred_sizes), len(lengths), len(targets), len(outputs))\n",
    "        loss = criterion(outputs, targets, pred_sizes, lengths)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Define validation method\n",
    "# def validate(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "#             inputs = batch['image'].to(device)\n",
    "#             targets = batch['label']\n",
    "#             # Assuming targets need to be converted to tensor, e.g., using a function `text_to_tensor`\n",
    "#             targets = text_to_tensor(targets) \n",
    "\n",
    "#             outputs = model(inputs)\n",
    "            \n",
    "#             # Calculate the CTC loss\n",
    "#             output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "#             loss = criterion(outputs, targets, input_lengths=None, target_lengths=output_lengths)\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if params['cuda'] and torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_dataloader = train_loader  # Assuming you have train_loader defined\n",
    "val_dataloader = val_loader  # Assuming you have val_loader defined\n",
    "\n",
    "for epoch in range(params['epochs']):\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "    # val_loss = validate(model, val_dataloader, criterion, device)\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{params['epochs']} - Train Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), params['model_dir'] + '/model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s_learning_seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6a6a2f4e9b3d29343f27ef6aef311b54e340d4b6fc29835f858df5c984e5196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
