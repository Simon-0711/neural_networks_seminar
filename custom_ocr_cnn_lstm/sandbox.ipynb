{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Pytorch\n",
    "to create endocder decoder ocr model with cnn and lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: /Users/simon/Documents/neural_networks_ocr_project/neural_networks_seminar\n",
      "Absolute path: /Users/simon/Documents/neural_networks_ocr_project/neural_networks_seminar/data/PubTabNet_cropped\n"
     ]
    }
   ],
   "source": [
    "# Setup path in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Get absolut path to proeject root\n",
    "load_dotenv()\n",
    "project_root_dir = os.path.dirname(os.path.abspath(\"./\"))\n",
    "print(\"Project root dir:\", project_root_dir)\n",
    "\n",
    "data_dir = os.getenv(\"PUBTABNET_CROPPED_DATA_DIR\")\n",
    "absolute_dir = project_root_dir + data_dir\n",
    "print(\"Absolute path:\", absolute_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_train = f\"{absolute_dir}/train\"\n",
    "image_dir_val = f\"{absolute_dir}/val\"\n",
    "label_file_train = f\"{absolute_dir}/PubTabNet_2.0.0_train_separated.json\"\n",
    "label_file_val = f\"{absolute_dir}/PubTabNet_2.0.0_val_separated.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from torch.utils.data import DataLoader\n",
    "label_file_small = f\"{absolute_dir}/subset_small.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_file_train = f\"{absolute_dir}/PubTabNet_2.0.0_train.json\"\n",
    "# label_file_val = f\"{absolute_dir}/PubTabNet_2.0.0_val.json\"\n",
    "\n",
    "# # Function to generate labels\n",
    "# import json\n",
    "# def generate_labels(json_data):\n",
    "#     labels = []\n",
    "#     for key, value in json_data.items():\n",
    "#         cells = value[\"html\"][\"cells\"]\n",
    "#         for i, cell in enumerate(cells):\n",
    "#             bbox = cell.get('bbox')\n",
    "#             tokens = cell.get('tokens')\n",
    "#             if bbox is None:\n",
    "#                 continue\n",
    "#             label = key.replace(\".png\", f\"_bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.png\")\n",
    "#             label = {\n",
    "#                 \"filename\": key.replace(\".png\", f\"_bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.png\"),\n",
    "#                 \"split\": value[\"split\"],\n",
    "#                 \"imgid\": value[\"imgid\"],\n",
    "#                 \"tokens\": tokens,\n",
    "#                 \"bbox\": bbox,\n",
    "#             }\n",
    "#             labels.append(label)\n",
    "#     return labels\n",
    "\n",
    "# # Generate labels\n",
    "# with open(label_file_val, 'r') as f:\n",
    "#         labels = json.load(f)\n",
    "#         result = generate_labels(labels)\n",
    "\n",
    "# # Specify the output file name\n",
    "# output_file_name = f\"{absolute_dir}/PubTabNet_2.0.0_val_separated.json\"\n",
    "\n",
    "# # Write the generated labels to a new JSON file\n",
    "# with open(output_file_name, 'w') as output_file:\n",
    "#     json.dump(result, output_file, indent=4)\n",
    "\n",
    "# print(f\"Generated labels have been written to {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthCollator(object):\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        width = [item['image'].shape[2] for item in batch]\n",
    "        indexes = [item['idx'] for item in batch]\n",
    "        imgs = torch.ones([len(batch), batch[0]['image'].shape[0], batch[0]['image'].shape[1], \n",
    "                           max(width)], dtype=torch.float32)\n",
    "        for idx, item in enumerate(batch):\n",
    "            try:\n",
    "                imgs[idx, :, :, 0:item['image'].shape[2]] = item['image']\n",
    "            except:\n",
    "                continue\n",
    "                #print(imgs.shape)\n",
    "        item = {'image': imgs, 'idx':indexes}\n",
    "        if 'label' in batch[0].keys():\n",
    "            labels = [item['label'] for item in batch]\n",
    "            item['label'] = labels\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform_list =  [transforms.Grayscale(1),\n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.5,), (0.5,))]\n",
    "transform = transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'label': '<b> S p e c i e s </b>', 'image': tensor([[[ 1.0000,  0.9451,  0.5765,  0.7569,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  0.9294,  0.8902,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.0196,  0.9686,  0.5922,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  0.8824,  0.8353,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000, -0.2941,  0.8353,  1.0000,  0.8980,  0.5765,  0.3961,\n",
      "           0.7412,  1.0000,  0.8039,  0.5529,  0.9608,  0.9294,  0.4745,\n",
      "           0.6392,  0.7490,  0.7804,  1.0000,  0.7020,  0.6078,  1.0000,\n",
      "           0.7020,  0.5608,  1.0000],\n",
      "         [ 1.0000,  0.6235, -0.3804,  0.4588,  0.7804, -0.2000,  0.6157,\n",
      "          -0.2706,  0.8980,  0.0118,  0.3490,  0.3725,  0.1765,  0.9765,\n",
      "           0.6863,  0.4275,  0.4039,  0.5608,  0.2314,  0.0588,  0.7020,\n",
      "           0.0588,  0.7255,  1.0000],\n",
      "         [ 0.9451,  0.9686,  0.9294, -0.2706,  0.9765, -0.0353,  1.0000,\n",
      "           0.0196,  0.6235,  0.2392,  1.0000,  0.9765, -0.1294,  1.0000,\n",
      "           1.0000,  0.5373,  0.4039,  0.2314,  0.6784,  1.0000,  1.0000,\n",
      "           0.2627, -0.1529,  1.0000],\n",
      "         [ 0.9137,  0.1294,  0.6078,  0.2627,  0.9765, -0.3333,  0.6314,\n",
      "           0.3725,  0.9686, -0.2471,  0.3333,  0.7569,  0.0824, -0.0275,\n",
      "           0.4745,  0.3333,  0.2078,  0.7098, -0.2078,  0.4275,  0.6941,\n",
      "           0.4275,  0.1294,  0.9686],\n",
      "         [ 1.0000,  0.9843,  0.9608,  1.0000,  0.9765, -0.0431,  0.9294,\n",
      "           1.0000,  1.0000,  0.9922,  0.9529,  1.0000,  1.0000,  0.9608,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  0.9686,  0.9843,  1.0000,\n",
      "           0.9529,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  0.7176,  0.0980,  0.9059,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000]]])}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "from custom_ocr_cnn_lstm.dataset import OCRDataset\n",
    "from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "train_dataset = OCRDatasetCropped(label_file_train, image_dir_train, transform=transform)\n",
    "# test.__getitem__(100)[\"image\"].shape\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=SynthCollator())\n",
    "val_dataset = OCRDatasetCropped(label_file_val, image_dir_val, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=True, collate_fn=SynthCollator())\n",
    "\n",
    "# Accessing a sample\n",
    "sample = train_dataset[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "torch.Size([10, 1, 10, 62])\n",
      "0 tensor([[[ 1.0000,  1.0000,  1.0000,  0.8431,  1.0000,  1.0000,  0.6784,\n",
      "           0.6706,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  0.8980, -0.2706,  1.0000,  0.4824,  0.6471,\n",
      "           0.5294,  0.5137,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  0.2078,  0.2549,  1.0000,  0.4431,  0.6235,\n",
      "           0.5294,  0.5608,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.3961,  0.8039,  0.2863,  1.0000,  0.7647,  0.1059,\n",
      "           0.0980,  0.8196,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.3020,  0.3804, -0.1216,  0.5529,  0.2078,  1.0000,\n",
      "           1.0000,  0.2392,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  0.2863,  1.0000,  0.4275,  0.3804,\n",
      "           0.2392,  0.5608,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.9373,\n",
      "           0.9608,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]]]) 4 8\n"
     ]
    }
   ],
   "source": [
    "# Vielleicht liegt der Fehler hier. \n",
    "# Batch enthält jeweils 3 items die dann 10 elemente jeweils enthalten\n",
    "for batch_idx, samples in enumerate(train_loader):\n",
    "    print(\"------\")\n",
    "    print(samples[\"image\"].shape)\n",
    "    print(batch_idx, samples[\"image\"][0], samples[\"label\"][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth labels loaded:  13690 ['  S p e c i e s  ', '  A n a j a ́ s  ', '  P o r t e l  ', '  S S B V  ', '  W i l d  ', '  R u r a l  ', '  U r b a n  ', '  W i l d  ', '  R u r a l  ', '  U r b a n  ', '  W i l d  ', '  R u r a l  ', '  U r b a n  ', '  T o t a l  ', '  ( % )  ', '< i > E v a n d r o m y i a   w a l k e r i < / i >', '4 4', '4 0', '1', '1 5 6', '2 9 6', '1', '5 1', '1', '2', '5 9 2', '6 8 . 8 4', '< i > E v a n d r o m y i a   i n f r a s p i n o s a < / i >', '4 4', '0', '3', '8 2', '1', '0', '0', '0', '0', '1 3 0', '1 5 . 1 2', '< i > N y s s o m y i a   a n t u n e s i < s u p > a < / s u p > < / i >', '1 1', '3', '3', '2 0', '3', '0', '1', '0', '0', '4 1', '4 . 7 7', '< i > M i c r o p y g o m y i a   r o r o t a e n s i s < / i >', '2 0', '1', '0', '4', '0', '0', '2', '0', '0', '2 7', '3 . 1 4', '< i > S c i o p e m y i a   s o r d e l l i i < / i >', '7', '1', '0', '1 3', '2', '0', '2', '0', '0', '2 5', '2 . 9 1', '< i > B i c h r o m o m y i a   f l a v i s c u t e l l a t a < s u p > a < / s u p > < / i >', '0', '0', '0', '4', '0', '0', '1 6', '0', '0', '2 0', '2 . 3 2', '< i > N y s s o m y i a   y u i l l i   y u i l l i < / i >', '4', '0', '0', '0', '0', '0', '0', '0', '0', '4', '0 . 4 6', '< i > P s a t h y r o m y i a   a r a g a o i < / i >']\n",
      "Character alphabet file created at: /Users/simon/Documents/neural_networks_ocr_project/neural_networks_seminar/data/PubTabNet_cropped/character_alphabet.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def create_alphabet_file(alphabet, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"START\\n\")\n",
    "        for char in alphabet:\n",
    "            f.write(char + \"\\n\")\n",
    "        f.write(\"END\\n\")\n",
    "\n",
    "# Replace these with your actual ground truth labels\n",
    "train_labels = train_dataset.labels\n",
    "val_labels = train_dataset.labels\n",
    "# Concat both dicts\n",
    "labels = train_labels + val_labels\n",
    "\n",
    "alph_labels = []\n",
    "for label in labels:\n",
    "    label_tokens = label.get('tokens')\n",
    "    label = \" \".join(\" \".join(tokens) for tokens in label_tokens)\n",
    "    label = label.replace(\"< b >\", \" \").replace(\"< / b >\", \" \")\n",
    "    alph_labels.append(label)\n",
    "print(\"Ground truth labels loaded: \", len(alph_labels), alph_labels[:100])\n",
    "\n",
    "unique_chars = set()\n",
    "unique_radicals = set()\n",
    "\n",
    "# Loop through ground truth labels to extract unique characters and radicals\n",
    "for label in alph_labels:\n",
    "    for char in label:\n",
    "        unique_chars.add(char)\n",
    "        # You might need to extract radicals from each character here if using decomposition\n",
    "\n",
    "# Define the file paths for character and radical alphabets\n",
    "char_alphabet_file_path = f\"{absolute_dir}/character_alphabet.txt\"\n",
    "radical_alphabet_file_path = f\"{absolute_dir}/radical_alphabet.txt\"\n",
    "\n",
    "# Create character alphabet file\n",
    "create_alphabet_file(unique_chars, char_alphabet_file_path)\n",
    "print(f\"Character alphabet file created at: {char_alphabet_file_path}\")\n",
    "\n",
    "# Create radical alphabet file (if needed)\n",
    "# create_alphabet_file(unique_radicals, radical_alphabet_file_path)\n",
    "# print(f\"Radical alphabet file created at: {radical_alphabet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss\n",
    "class CustomCTCLoss(torch.nn.Module):\n",
    "    # T x B x H => Softmax on dimension 2\n",
    "    def __init__(self, dim=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "\n",
    "    def forward(self, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        EPS = 1e-7\n",
    "        loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes)\n",
    "        loss = self.sanitize(loss)\n",
    "        return self.debug(loss, logits, labels, prediction_sizes, target_sizes)\n",
    "    \n",
    "    def sanitize(self, loss):\n",
    "        EPS = 1e-7\n",
    "        if abs(loss.item() - float('inf')) < EPS:\n",
    "            return torch.zeros_like(loss)\n",
    "        if math.isnan(loss.item()):\n",
    "            return torch.zeros_like(loss)\n",
    "        return loss\n",
    "\n",
    "    def debug(self, loss, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Loss:\", loss)\n",
    "            print(\"logits:\", logits)\n",
    "            print(\"labels:\", labels)\n",
    "            print(\"prediction_sizes:\", prediction_sizes)\n",
    "            print(\"target_sizes:\", target_sizes)\n",
    "            raise Exception(\"NaN loss obtained. But why?\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alpabet from file\n",
    "def load_alphabet(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        alphabet = f.read().splitlines()\n",
    "    return alphabet\n",
    "\n",
    "alphabet = load_alphabet(char_alphabet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum width in dataset: 454\n"
     ]
    }
   ],
   "source": [
    "def find_maximum_width(dataset):\n",
    "    # Initialize a list to store image widths\n",
    "    image_widths = []\n",
    "\n",
    "    # Iterate through the dataset and collect image widths\n",
    "    for sample in dataset:\n",
    "        image_width = sample['image'].shape[2]  # Get the width of the image\n",
    "        image_widths.append(image_width)\n",
    "\n",
    "    # Find the maximum width across all images\n",
    "    max_width = max(image_widths)\n",
    "\n",
    "    return max_width\n",
    "\n",
    "# Example usage:\n",
    "max_width_train = find_maximum_width(train_dataset)\n",
    "max_width_val = find_maximum_width(val_dataset)\n",
    "\n",
    "max_width = max(max_width_train, max_width_val)\n",
    "\n",
    "print(\"Maximum width in dataset:\", max_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(114, 256, bidirectional=True)\n",
      "Params: {'input_dim': 114, 'hidden_dim': 256, 'output_dim': 114, 'input_planes': 1, 'planes': 32, 'image_height': 32, 'number_channels': 1, 'number_hidden_layers': 2, 'len_alphabet': 113, 'learning_rate': 0.001, 'epochs': 4, 'batch_size': 10, 'model_dir': 'model_history', 'log_dir': 'logs', 'resume': False, 'cuda': False, 'schedule': False, 'max_width': 454}\n"
     ]
    }
   ],
   "source": [
    "# Setup environment for training\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from tqdm import *\n",
    "from cnn_lstm_ocr import CNNLSTM_OCR\n",
    "\n",
    "# params = {\n",
    "#     \"input_dim\": 256,\n",
    "#     \"hidden_dim\": 256,\n",
    "#     \"output_dim\": 1,\n",
    "#     \"input_planes\": 1,\n",
    "#     \"planes\": 1,\n",
    "#     \"schedule\": False,\n",
    "#     'image_height':32,\n",
    "#     'number_channels':1,\n",
    "#     'number_hidden_layers':256,\n",
    "#     'len_alphabet':len(alphabet),\n",
    "#     'learning_rate':0.001,\n",
    "#     'epochs':4,\n",
    "#     'batch_size':10,\n",
    "#     'model_dir':'model_history',\n",
    "#     'log_dir':'logs',\n",
    "#     'resume':False,\n",
    "#     'cuda':False,\n",
    "#     'schedule':False    \n",
    "# }\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": 1,  # Assuming grayscale images\n",
    "    \"hidden_dim\": 256,\n",
    "    \"output_dim\": len(alphabet) + 1,  # Number of classes (including a blank symbol)\n",
    "    \"input_planes\": 1,  # Assuming grayscale images\n",
    "    \"planes\": 32,  # Adjust this value as needed\n",
    "    'image_height': 32,  # Adjust as needed\n",
    "    'number_channels': 1,  # Assuming grayscale images\n",
    "    'number_hidden_layers': 2,  # Adjust as needed\n",
    "    'len_alphabet': len(alphabet),  # Number of classes (excluding a blank symbol)\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 4,\n",
    "    'batch_size': 10,\n",
    "    'model_dir': 'model_history',\n",
    "    'log_dir': 'logs',\n",
    "    'resume': False,\n",
    "    'cuda': False,\n",
    "    'schedule': False,\n",
    "    'max_width': max_width\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = CNNLSTM_OCR(params)\n",
    "criterion = CustomCTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'])\n",
    "batch_size = params['batch_size']\n",
    "count = 1\n",
    "epochs = params['epochs']\n",
    "cuda = params['cuda']\n",
    "print(\"Params:\", params)\n",
    "\n",
    "def train_dataloader():\n",
    "        # logging.info('training data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_train,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn,\n",
    "                shuffle=True)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import transforms\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from tqdm import tqdm\n",
    "# from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "# # Define a method for training the OCR model\n",
    "# def train_ocr_model(model, train_loader, optimizer, criterion, device, epoch, epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     progress_bar = tqdm(train_loader, desc='Epoch: [%d]/[%d] Training'%(epoch, \n",
    "#                 epochs), leave=True)\n",
    "\n",
    "#     for idx, batch in enumerate(progress_bar):\n",
    "#         optimizer.zero_grad()\n",
    "#         images, labels = batch['image'], batch['label']\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "        \n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         loss.backward()\n",
    "#         clip_grad_norm_(model.parameters(), max_norm=5)  # Clip gradients to prevent exploding gradients\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     return avg_loss\n",
    "\n",
    "# # Define a method for evaluating the OCR model on the validation dataset\n",
    "# def evaluate_ocr_model(model, val_loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(val_loader)\n",
    "#     return avg_loss\n",
    "\n",
    "# # Define a method for OCR inference\n",
    "# def ocr_inference(model, image, transform, device):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Preprocess the input image using the same transform as during training\n",
    "#         image = transform(image).unsqueeze(0).to(device)\n",
    "#         outputs = model(image)\n",
    "        \n",
    "#         # Perform any necessary post-processing on the model's outputs to get the OCR result\n",
    "#         # You may need to implement this part based on your specific OCR task\n",
    "    \n",
    "#     return ocr_result\n",
    "\n",
    "\n",
    "# # Set device (CPU or GPU) for training\n",
    "# device = torch.device(\"cuda\" if params['cuda'] and torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(params['epochs']):\n",
    "#     print(f\"Epoch {epoch + 1}/{params['epochs']}\")\n",
    "    \n",
    "#     # Train the model\n",
    "#     train_loss = train_ocr_model(model, train_loader, optimizer, criterion, device, epoch, params['epochs'])\n",
    "#     print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "#     # Validate the model\n",
    "#     val_loss = evaluate_ocr_model(model, val_loader, criterion, device)\n",
    "#     print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# # After training, you can use the trained model for inference on new images\n",
    "# # Example usage for OCR inference:\n",
    "# sample_image, sample_label = train_dataset[0]  # Load a sample image and label\n",
    "# ocr_result = ocr_inference(model, sample_image, transform, device)\n",
    "# print(\"OCR Result:\", ocr_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to Tensor: [20, 27, 96, 96, 23, 3, 101, 3, 65]\n",
      "Tensor to Text: HELLO 5 7\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping from characters to integers\n",
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "with open(char_alphabet_file_path, 'r') as f:\n",
    "    alphabet_lines = f.read().splitlines()\n",
    "\n",
    "for i, char in enumerate(alphabet_lines[1:-1]):  # Skip \"START\" and \"END\" lines\n",
    "    char_to_int[char] = i\n",
    "    int_to_char[i] = char\n",
    "\n",
    "# Define a special key for space character\n",
    "SPACE_INT = len(char_to_int)  # Use the last index in the alphabet\n",
    "\n",
    "# Implement text_to_tensor method\n",
    "def text_to_tensor(text):\n",
    "    # Convert text to a list of integers using the char_to_int mapping\n",
    "    tensor = [char_to_int[char] if char in char_to_int else SPACE_INT for char in text]\n",
    "    return tensor\n",
    "\n",
    "# Implement tensor_to_text method\n",
    "def tensor_to_text(tensor):\n",
    "    # Convert a tensor (list of integers) back to text using the int_to_char mapping\n",
    "    text = \"\".join([int_to_char[idx] if idx != SPACE_INT else ' ' for idx in tensor])\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "text = \"HELLO 5 7\"\n",
    "tensor = text_to_tensor(text)\n",
    "print(\"Text to Tensor:\", tensor)\n",
    "text_reconstructed = tensor_to_text(tensor)\n",
    "print(\"Tensor to Text:\", text_reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(114, 256, bidirectional=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/685 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: torch.Size([10, 1, 9, 59])\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/47/kf2nw7g97xj4bnnf6pdqwq640000gp/T/com.apple.shortcuts.mac-helper/ipykernel_28869/3749759233.py\", line 72, in <module>\n",
      "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/47/kf2nw7g97xj4bnnf6pdqwq640000gp/T/com.apple.shortcuts.mac-helper/ipykernel_28869/3749759233.py\", line 30, in train\n",
      "    outputs = model(inputs)\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/Documents/neural_networks_ocr_project/neural_networks_seminar/custom_ocr_cnn_lstm/../custom_ocr_cnn_lstm/cnn_lstm_ocr.py\", line 79, in forward\n",
      "    assert h == 1, \"the height of cnn must be 1\"\n",
      "           ^^^^^^\n",
      "AssertionError: the height of cnn must be 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Users/simon/opt/anaconda3/envs/s2s_learning_seminar/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from custom_ocr_cnn_lstm.cnn_lstm_ocr import CNNLSTM_OCR\n",
    "\n",
    "\n",
    "# Define your model, criterion, optimizer, and other parameters here\n",
    "\n",
    "# Initialize your model, criterion, and optimizer\n",
    "model = CNNLSTM_OCR(params)  # Assuming you've already defined CNNLSTM_OCR class\n",
    "criterion = nn.CTCLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'])\n",
    "\n",
    "# Define training loop\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = batch['image'].to(device)\n",
    "        targets = batch['label']\n",
    "        # Assuming targets need to be converted to tensor, e.g., using a function `text_to_tensor`\n",
    "        targets = text_to_tensor(targets) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        print(\"inputs.shape:\", inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate the CTC loss\n",
    "        output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "        loss = criterion(outputs, targets, input_lengths=None, target_lengths=output_lengths)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Define validation method\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            inputs = batch['image'].to(device)\n",
    "            targets = batch['label']\n",
    "            # Assuming targets need to be converted to tensor, e.g., using a function `text_to_tensor`\n",
    "            targets = text_to_tensor(targets) \n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate the CTC loss\n",
    "            output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "            loss = criterion(outputs, targets, input_lengths=None, target_lengths=output_lengths)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if params['cuda'] and torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_dataloader = train_loader  # Assuming you have train_loader defined\n",
    "val_dataloader = val_loader  # Assuming you have val_loader defined\n",
    "\n",
    "for epoch in range(params['epochs']):\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, val_dataloader, criterion, device)\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{params['epochs']} - Train Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), params['model_dir'] + '/model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s_learning_seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6a6a2f4e9b3d29343f27ef6aef311b54e340d4b6fc29835f858df5c984e5196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
