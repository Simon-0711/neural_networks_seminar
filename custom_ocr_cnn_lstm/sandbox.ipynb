{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Pytorch\n",
    "to create endocder decoder ocr model with cnn and lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar\n",
      "Absolute path: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/PubTabNet_cropped\n"
     ]
    }
   ],
   "source": [
    "# Setup path in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Get absolut path to proeject root\n",
    "load_dotenv()\n",
    "project_root_dir = os.path.dirname(os.path.abspath(\"./\"))\n",
    "print(\"Project root dir:\", project_root_dir)\n",
    "\n",
    "data_dir = os.getenv(\"PUBTABNET_CROPPED_DATA_DIR\")\n",
    "absolute_dir = project_root_dir + data_dir\n",
    "print(\"Absolute path:\", absolute_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_train = f\"{absolute_dir}/train\"\n",
    "image_dir_val = f\"{absolute_dir}/val\"\n",
    "label_file_train = f\"{absolute_dir}/PubTabNet_2.0.0_train_separated.json\"\n",
    "label_file_val = f\"{absolute_dir}/PubTabNet_2.0.0_val_separated.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from torch.utils.data import DataLoader\n",
    "label_file_small = f\"{absolute_dir}/subset_small.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_file_train = f\"{absolute_dir}/PubTabNet_2.0.0_train.json\"\n",
    "# label_file_val = f\"{absolute_dir}/PubTabNet_2.0.0_val.json\"\n",
    "\n",
    "# # Function to generate labels\n",
    "# import json\n",
    "# def generate_labels(json_data):\n",
    "#     labels = []\n",
    "#     for key, value in json_data.items():\n",
    "#         cells = value[\"html\"][\"cells\"]\n",
    "#         for i, cell in enumerate(cells):\n",
    "#             bbox = cell.get('bbox')\n",
    "#             tokens = cell.get('tokens')\n",
    "#             if bbox is None:\n",
    "#                 continue\n",
    "#             label = key.replace(\".png\", f\"_bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.png\")\n",
    "#             label = {\n",
    "#                 \"filename\": key.replace(\".png\", f\"_bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.png\"),\n",
    "#                 \"split\": value[\"split\"],\n",
    "#                 \"imgid\": value[\"imgid\"],\n",
    "#                 \"tokens\": tokens,\n",
    "#                 \"bbox\": bbox,\n",
    "#             }\n",
    "#             labels.append(label)\n",
    "#     return labels\n",
    "\n",
    "# # Generate labels\n",
    "# with open(label_file_val, 'r') as f:\n",
    "#         labels = json.load(f)\n",
    "#         result = generate_labels(labels)\n",
    "\n",
    "# # Specify the output file name\n",
    "# output_file_name = f\"{absolute_dir}/PubTabNet_2.0.0_val_separated.json\"\n",
    "\n",
    "# # Write the generated labels to a new JSON file\n",
    "# with open(output_file_name, 'w') as output_file:\n",
    "#     json.dump(result, output_file, indent=4)\n",
    "\n",
    "# print(f\"Generated labels have been written to {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform_list =  [transforms.Grayscale(1),\n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.5,), (0.5,))]\n",
    "transform = transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'label': 'Species', 'image': tensor([[[ 1.0000,  0.9451,  0.5765,  0.7569,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  0.9294,  0.8902,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.0196,  0.9686,  0.5922,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  0.8824,  0.8353,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000, -0.2941,  0.8353,  1.0000,  0.8980,  0.5765,  0.3961,\n",
      "           0.7412,  1.0000,  0.8039,  0.5529,  0.9608,  0.9294,  0.4745,\n",
      "           0.6392,  0.7490,  0.7804,  1.0000,  0.7020,  0.6078,  1.0000,\n",
      "           0.7020,  0.5608,  1.0000],\n",
      "         [ 1.0000,  0.6235, -0.3804,  0.4588,  0.7804, -0.2000,  0.6157,\n",
      "          -0.2706,  0.8980,  0.0118,  0.3490,  0.3725,  0.1765,  0.9765,\n",
      "           0.6863,  0.4275,  0.4039,  0.5608,  0.2314,  0.0588,  0.7020,\n",
      "           0.0588,  0.7255,  1.0000],\n",
      "         [ 0.9451,  0.9686,  0.9294, -0.2706,  0.9765, -0.0353,  1.0000,\n",
      "           0.0196,  0.6235,  0.2392,  1.0000,  0.9765, -0.1294,  1.0000,\n",
      "           1.0000,  0.5373,  0.4039,  0.2314,  0.6784,  1.0000,  1.0000,\n",
      "           0.2627, -0.1529,  1.0000],\n",
      "         [ 0.9137,  0.1294,  0.6078,  0.2627,  0.9765, -0.3333,  0.6314,\n",
      "           0.3725,  0.9686, -0.2471,  0.3333,  0.7569,  0.0824, -0.0275,\n",
      "           0.4745,  0.3333,  0.2078,  0.7098, -0.2078,  0.4275,  0.6941,\n",
      "           0.4275,  0.1294,  0.9686],\n",
      "         [ 1.0000,  0.9843,  0.9608,  1.0000,  0.9765, -0.0431,  0.9294,\n",
      "           1.0000,  1.0000,  0.9922,  0.9529,  1.0000,  1.0000,  0.9608,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  0.9686,  0.9843,  1.0000,\n",
      "           0.9529,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  0.7176,  0.0980,  0.9059,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000]]])}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "from custom_ocr_cnn_lstm.dataset import OCRDataset\n",
    "from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "train_dataset = OCRDatasetCropped(label_file_train, image_dir_train, transform=transform)\n",
    "val_dataset = OCRDatasetCropped(label_file_val, image_dir_val, transform=transform)\n",
    "\n",
    "# Accessing a sample\n",
    "sample = train_dataset[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Height: 84\n",
      "Maximum Width: 429\n"
     ]
    }
   ],
   "source": [
    "max_height = 0\n",
    "max_width = 0\n",
    "\n",
    "for item in train_dataset:\n",
    "    image = item['image']\n",
    "    height, width = image.shape[1], image.shape[2]\n",
    "    max_height = max(max_height, height)\n",
    "    max_width = max(max_width, width)\n",
    "\n",
    "print(f\"Maximum Height: {max_height}\")\n",
    "print(f\"Maximum Width: {max_width}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SynthCollator(object):\n",
    "    \n",
    "#     def __call__(self, batch):\n",
    "#         width = [item['image'].shape[2] for item in batch]\n",
    "#         indexes = [item['idx'] for item in batch]\n",
    "#         imgs = torch.ones([len(batch), batch[0]['image'].shape[0], batch[0]['image'].shape[1], \n",
    "#                            max(width)], dtype=torch.float32)\n",
    "#         for idx, item in enumerate(batch):\n",
    "#             try:\n",
    "#                 imgs[idx, :, :, 0:item['image'].shape[2]] = item['image']\n",
    "#             except:\n",
    "#                 continue\n",
    "#                 #print(imgs.shape)\n",
    "#         item = {'image': imgs, 'idx':indexes}\n",
    "#         if 'label' in batch[0].keys():\n",
    "#             labels = [item['label'] for item in batch]\n",
    "#             item['label'] = labels\n",
    "#         return item\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCollator(object):\n",
    "    \n",
    "    def __init__(self, target_height = max_height, target_width= max_width):\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        indexes = [item['idx'] for item in batch]\n",
    "        \n",
    "        # Resize images to the target size\n",
    "        resized_images = []\n",
    "        for item in batch:\n",
    "            image = item['image']\n",
    "            resized_image = F.interpolate(image.unsqueeze(0), size=(self.target_height, self.target_width), mode='bilinear', align_corners=False)\n",
    "            resized_images.append(resized_image.squeeze(0))\n",
    "        \n",
    "        # Stack resized images\n",
    "        imgs = torch.stack(resized_images, dim=0)\n",
    "        \n",
    "        item = {'image': imgs, 'idx': indexes}\n",
    "        \n",
    "        if 'label' in batch[0].keys():\n",
    "            labels = [item['label'] for item in batch]\n",
    "            item['label'] = labels\n",
    "        \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "from custom_ocr_cnn_lstm.dataset import OCRDataset\n",
    "from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=CustomCollator())\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=True, collate_fn=CustomCollator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "torch.Size([10, 1, 84, 429])\n",
      "0 tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]) →\n"
     ]
    }
   ],
   "source": [
    "# Vielleicht liegt der Fehler hier. \n",
    "# Batch enthält jeweils 3 items die dann 10 elemente jeweils enthalten\n",
    "for batch_idx, samples in enumerate(train_loader):\n",
    "    print(\"------\")\n",
    "    print(samples[\"image\"].shape)\n",
    "    print(batch_idx, samples[\"image\"][0], samples[\"label\"][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_xml_tags(input_string):\n",
    "    pattern = r'<[^>]+>'\n",
    "    return re.sub(pattern, '', input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth labels loaded:  13690 [' S p e c i e s ', ' A n a j a ́ s ', ' P o r t e l ', ' S S B V ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' T o t a l ', ' ( % ) ', ' E v a n d r o m y i a   w a l k e r i ', '4 4', '4 0', '1', '1 5 6', '2 9 6', '1', '5 1', '1', '2', '5 9 2', '6 8 . 8 4', ' E v a n d r o m y i a   i n f r a s p i n o s a ', '4 4', '0', '3', '8 2', '1', '0', '0', '0', '0', '1 3 0', '1 5 . 1 2', ' N y s s o m y i a   a n t u n e s i  a  ', '1 1', '3', '3', '2 0', '3', '0', '1', '0', '0', '4 1', '4 . 7 7', ' M i c r o p y g o m y i a   r o r o t a e n s i s ', '2 0', '1', '0', '4', '0', '0', '2', '0', '0', '2 7', '3 . 1 4', ' S c i o p e m y i a   s o r d e l l i i ', '7', '1', '0', '1 3', '2', '0', '2', '0', '0', '2 5', '2 . 9 1', ' B i c h r o m o m y i a   f l a v i s c u t e l l a t a  a  ', '0', '0', '0', '4', '0', '0', '1 6', '0', '0', '2 0', '2 . 3 2', ' N y s s o m y i a   y u i l l i   y u i l l i ', '4', '0', '0', '0', '0', '0', '0', '0', '0', '4', '0 . 4 6', ' P s a t h y r o m y i a   a r a g a o i ']\n",
      "Character alphabet file created at: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/PubTabNet_cropped/character_alphabet.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def create_alphabet_file(alphabet, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        # f.write(\"START\\n\")\n",
    "        for char in alphabet:\n",
    "            f.write(char + \"\\n\")\n",
    "        # f.write(\"END\\n\")\n",
    "\n",
    "# Replace these with your actual ground truth labels\n",
    "train_labels = train_dataset.labels\n",
    "val_labels = train_dataset.labels\n",
    "# Concat both dicts\n",
    "labels = train_labels + val_labels\n",
    "\n",
    "alph_labels = []\n",
    "for label in labels:\n",
    "    label_tokens = label.get('tokens')\n",
    "    label = \" \".join(\" \".join(tokens) for tokens in label_tokens)\n",
    "    label = remove_xml_tags(label)\n",
    "    alph_labels.append(label)\n",
    "print(\"Ground truth labels loaded: \", len(alph_labels), alph_labels[:100])\n",
    "\n",
    "unique_chars = set()\n",
    "unique_radicals = set()\n",
    "\n",
    "# Loop through ground truth labels to extract unique characters and radicals\n",
    "for label in alph_labels:\n",
    "    for char in label:\n",
    "        unique_chars.add(char)\n",
    "        # You might need to extract radicals from each character here if using decomposition\n",
    "\n",
    "# Define the file paths for character and radical alphabets\n",
    "char_alphabet_file_path = f\"{absolute_dir}/character_alphabet.txt\"\n",
    "radical_alphabet_file_path = f\"{absolute_dir}/radical_alphabet.txt\"\n",
    "\n",
    "# Create character alphabet file\n",
    "create_alphabet_file(unique_chars, char_alphabet_file_path)\n",
    "print(f\"Character alphabet file created at: {char_alphabet_file_path}\")\n",
    "\n",
    "# Create radical alphabet file (if needed)\n",
    "# create_alphabet_file(unique_radicals, radical_alphabet_file_path)\n",
    "# print(f\"Radical alphabet file created at: {radical_alphabet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss\n",
    "class CustomCTCLoss(torch.nn.Module):\n",
    "    # T x B x H => Softmax on dimension 2\n",
    "    def __init__(self, dim=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "\n",
    "    def forward(self, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        EPS = 1e-7\n",
    "        loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes)\n",
    "        loss = self.sanitize(loss)\n",
    "        return self.debug(loss, logits, labels, prediction_sizes, target_sizes)\n",
    "    \n",
    "    def sanitize(self, loss):\n",
    "        EPS = 1e-7\n",
    "        if abs(loss.item() - float('inf')) < EPS:\n",
    "            return torch.zeros_like(loss)\n",
    "        if math.isnan(loss.item()):\n",
    "            return torch.zeros_like(loss)\n",
    "        return loss\n",
    "\n",
    "    def debug(self, loss, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Loss:\", loss)\n",
    "            print(\"logits:\", logits)\n",
    "            print(\"labels:\", labels)\n",
    "            print(\"prediction_sizes:\", prediction_sizes)\n",
    "            print(\"target_sizes:\", target_sizes)\n",
    "            raise Exception(\"NaN loss obtained. But why?\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet loaded: 111 ['–', '0', 'Q', '`', 'n', ',', '”', 'A', 'T', 'i', 'D', '[', '6', 'V', 'y', '&', 'P', 'U', '×', '→', 'j', '8', '“', 'G', 'a', 'S', '†', 'v', ':', 'M', 'H', 'Y', 'k', 'R', '≥', '‐', 'e', ')', '*', '_', '—', '5', '3', '́', '.', 'λ', '$', 'C', 'F', '#', 'w', 'x', 'r', 'X', 'l', 's', 'm', 'B', 'p', 'γ', 'β', 'q', '§', '⁄', '7', '9', ';', '≤', \"'\", '−', 'Z', 'E', ' ', '/', '±', '·', 'J', 'O', '′', '4', '?', '1', 'N', '%', '=', 'K', 'u', 'f', 'g', 'o', '<', 't', '>', 'I', 'z', 'b', '’', ']', '2', '°', '+', '(', 'α', 'c', 'h', 'L', 'W', 'μ', 'Δ', 'd', '-']\n"
     ]
    }
   ],
   "source": [
    "# Load alpabet from file\n",
    "def load_alphabet(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        alphabet = f.read().splitlines()\n",
    "    return alphabet\n",
    "\n",
    "alphabet = load_alphabet(char_alphabet_file_path)\n",
    "print(\"Alphabet loaded:\", len(alphabet), alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum width in dataset: 454\n"
     ]
    }
   ],
   "source": [
    "def find_maximum_width(dataset):\n",
    "    # Initialize a list to store image widths\n",
    "    image_widths = []\n",
    "\n",
    "    # Iterate through the dataset and collect image widths\n",
    "    for sample in dataset:\n",
    "        image_width = sample['image'].shape[2]  # Get the width of the image\n",
    "        image_widths.append(image_width)\n",
    "\n",
    "    # Find the maximum width across all images\n",
    "    max_width = max(image_widths)\n",
    "\n",
    "    return max_width\n",
    "\n",
    "# Example usage:\n",
    "max_width_train = find_maximum_width(train_dataset)\n",
    "max_width_val = find_maximum_width(val_dataset)\n",
    "\n",
    "max_width = max(max_width_train, max_width_val)\n",
    "\n",
    "print(\"Maximum width in dataset:\", max_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'input_dim': 32, 'alphabet': ['–', '0', 'Q', '`', 'n', ',', '”', 'A', 'T', 'i', 'D', '[', '6', 'V', 'y', '&', 'P', 'U', '×', '→', 'j', '8', '“', 'G', 'a', 'S', '†', 'v', ':', 'M', 'H', 'Y', 'k', 'R', '≥', '‐', 'e', ')', '*', '_', '—', '5', '3', '́', '.', 'λ', '$', 'C', 'F', '#', 'w', 'x', 'r', 'X', 'l', 's', 'm', 'B', 'p', 'γ', 'β', 'q', '§', '⁄', '7', '9', ';', '≤', \"'\", '−', 'Z', 'E', ' ', '/', '±', '·', 'J', 'O', '′', '4', '?', '1', 'N', '%', '=', 'K', 'u', 'f', 'g', 'o', '<', 't', '>', 'I', 'z', 'b', '’', ']', '2', '°', '+', '(', 'α', 'c', 'h', 'L', 'W', 'μ', 'Δ', 'd', '-'], 'hidden_dim': 256, 'output_dim': 112, 'input_planes': 1, 'planes': 32, 'image_height': 32, 'number_channels': 1, 'number_hidden_layers': 2, 'len_alphabet': 111, 'learning_rate': 0.001, 'epochs': 4, 'batch_size': 10, 'model_dir': 'model_history', 'log_dir': 'logs', 'resume': False, 'cuda': False, 'schedule': False, 'max_width': 454}\n"
     ]
    }
   ],
   "source": [
    "# Setup environment for training\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from utils import OCRLabelConverter\n",
    "from tqdm import *\n",
    "from cnn_lstm_ocr import CNNLSTM_OCR\n",
    "\n",
    "# params = {\n",
    "#     \"input_dim\": 256,\n",
    "#     \"hidden_dim\": 256,\n",
    "#     \"output_dim\": 1,\n",
    "#     \"input_planes\": 1,\n",
    "#     \"planes\": 1,\n",
    "#     \"schedule\": False,\n",
    "#     'image_height':32,\n",
    "#     'number_channels':1,\n",
    "#     'number_hidden_layers':256,\n",
    "#     'len_alphabet':len(alphabet),\n",
    "#     'learning_rate':0.001,\n",
    "#     'epochs':4,\n",
    "#     'batch_size':10,\n",
    "#     'model_dir':'model_history',\n",
    "#     'log_dir':'logs',\n",
    "#     'resume':False,\n",
    "#     'cuda':False,\n",
    "#     'schedule':False    \n",
    "# }\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": 1,  # Assuming grayscale images\n",
    "    \"alphabet\": alphabet,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"output_dim\": len(alphabet) + 1,  # Number of classes (including a blank symbol)\n",
    "    \"input_planes\": 1,  # Assuming grayscale images\n",
    "    \"planes\": 32,  # Adjust this value as needed\n",
    "    'image_height': 32,  # Adjust as needed\n",
    "    'number_channels': 1,  # Assuming grayscale images\n",
    "    'number_hidden_layers': 2,  # Adjust as needed\n",
    "    'len_alphabet': len(alphabet),  # Number of classes (excluding a blank symbol)\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 4,\n",
    "    'batch_size': 10,\n",
    "    'model_dir': 'model_history',\n",
    "    'log_dir': 'logs',\n",
    "    'resume': False,\n",
    "    'cuda': False,\n",
    "    'schedule': False,\n",
    "    'max_width': max_width\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = CNNLSTM_OCR(params)\n",
    "converter = OCRLabelConverter(''.join(params['alphabet']))\n",
    "criterion = CustomCTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'])\n",
    "batch_size = params['batch_size']\n",
    "count = 1\n",
    "epochs = params['epochs']\n",
    "cuda = params['cuda']\n",
    "print(\"Params:\", params)\n",
    "\n",
    "def train_dataloader():\n",
    "        # logging.info('training data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_train,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn,\n",
    "                shuffle=True)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import transforms\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from tqdm import tqdm\n",
    "# from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "# # Define a method for training the OCR model\n",
    "# def train_ocr_model(model, train_loader, optimizer, criterion, device, epoch, epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     progress_bar = tqdm(train_loader, desc='Epoch: [%d]/[%d] Training'%(epoch, \n",
    "#                 epochs), leave=True)\n",
    "\n",
    "#     for idx, batch in enumerate(progress_bar):\n",
    "#         optimizer.zero_grad()\n",
    "#         images, labels = batch['image'], batch['label']\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "        \n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         loss.backward()\n",
    "#         clip_grad_norm_(model.parameters(), max_norm=5)  # Clip gradients to prevent exploding gradients\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     return avg_loss\n",
    "\n",
    "# # Define a method for evaluating the OCR model on the validation dataset\n",
    "# def evaluate_ocr_model(model, val_loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(val_loader)\n",
    "#     return avg_loss\n",
    "\n",
    "# # Define a method for OCR inference\n",
    "# def ocr_inference(model, image, transform, device):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Preprocess the input image using the same transform as during training\n",
    "#         image = transform(image).unsqueeze(0).to(device)\n",
    "#         outputs = model(image)\n",
    "        \n",
    "#         # Perform any necessary post-processing on the model's outputs to get the OCR result\n",
    "#         # You may need to implement this part based on your specific OCR task\n",
    "    \n",
    "#     return ocr_result\n",
    "\n",
    "\n",
    "# # Set device (CPU or GPU) for training\n",
    "# device = torch.device(\"cuda\" if params['cuda'] and torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(params['epochs']):\n",
    "#     print(f\"Epoch {epoch + 1}/{params['epochs']}\")\n",
    "    \n",
    "#     # Train the model\n",
    "#     train_loss = train_ocr_model(model, train_loader, optimizer, criterion, device, epoch, params['epochs'])\n",
    "#     print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "#     # Validate the model\n",
    "#     val_loss = evaluate_ocr_model(model, val_loader, criterion, device)\n",
    "#     print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# # After training, you can use the trained model for inference on new images\n",
    "# # Example usage for OCR inference:\n",
    "# sample_image, sample_label = train_dataset[0]  # Load a sample image and label\n",
    "# ocr_result = ocr_inference(model, sample_image, transform, device)\n",
    "# print(\"OCR Result:\", ocr_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a mapping from characters to integers\n",
    "# char_to_int = {}\n",
    "# int_to_char = {}\n",
    "# with open(char_alphabet_file_path, 'r') as f:\n",
    "#     alphabet_lines = f.read().splitlines()\n",
    "\n",
    "# for i, char in enumerate(alphabet_lines[1:-1]):  # Skip \"START\" and \"END\" lines\n",
    "#     char_to_int[char] = i\n",
    "#     int_to_char[i] = char\n",
    "\n",
    "# # Define a special key for space character\n",
    "# SPACE_INT = len(char_to_int)  # Use the last index in the alphabet\n",
    "\n",
    "# # Implement text_to_tensor method\n",
    "# def text_to_tensor(text):\n",
    "#     # Convert text to a list of integers using the char_to_int mapping\n",
    "#     tensor = [char_to_int[char] if char in char_to_int else SPACE_INT for char in text]\n",
    "#     length = [len(text)]\n",
    "#     # return tensor\n",
    "#     return (torch.IntTensor(tensor), torch.IntTensor(length))\n",
    "\n",
    "# # Implement tensor_to_text method\n",
    "# def tensor_to_text(tensor):\n",
    "#     # Convert a tensor (list of integers) back to text using the int_to_char mapping\n",
    "#     text = \"\".join([int_to_char[idx] if idx != SPACE_INT else ' ' for idx in tensor])\n",
    "#     return text\n",
    "\n",
    "# # Example usage:\n",
    "# text = \"HELLO 5 7\"\n",
    "# tensor = text_to_tensor(text)\n",
    "# print(\"Text to Tensor:\", tensor)\n",
    "# text_reconstructed = tensor_to_text(tensor)\n",
    "# print(\"Tensor to Text:\", text_reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/685 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len targets:  55\n",
      "cnn input x.shape: torch.Size([10, 1, 84, 429])\n",
      "cnn out.size(): 10, 32, 1, 2247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/685 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1171,  0.1550, -0.0878,  ..., -0.0068, -0.0416,  0.0238],\n",
      "        [ 0.1124,  0.1510, -0.0758,  ..., -0.0053, -0.0357,  0.0254],\n",
      "        [ 0.1097,  0.1577, -0.0755,  ...,  0.0024, -0.0299,  0.0206],\n",
      "        ...,\n",
      "        [ 0.0633,  0.0640, -0.0498,  ..., -0.0076,  0.0381,  0.0226],\n",
      "        [ 0.0592,  0.0633, -0.0422,  ..., -0.0059,  0.0301,  0.0217],\n",
      "        [ 0.0516,  0.0600, -0.0289,  ..., -0.0016,  0.0159,  0.0229]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "outputs shape: tensor([[[-4.6007, -4.5628, -4.8056,  ..., -4.7245, -4.7594, -4.6939],\n",
      "         [-4.6079, -4.5693, -4.7961,  ..., -4.7256, -4.7560, -4.6949],\n",
      "         [-4.6104, -4.5624, -4.7956,  ..., -4.7177, -4.7500, -4.6995],\n",
      "         ...,\n",
      "         [-4.6551, -4.6547, -4.7684,  ..., -4.7266, -4.6802, -4.6958],\n",
      "         [-4.6592, -4.6554, -4.7609,  ..., -4.7249, -4.6882, -4.6966],\n",
      "         [-4.6671, -4.6588, -4.7478,  ..., -4.7207, -4.7027, -4.6956]],\n",
      "\n",
      "        [[-4.6007, -4.5628, -4.8056,  ..., -4.7245, -4.7594, -4.6939],\n",
      "         [-4.6079, -4.5693, -4.7961,  ..., -4.7256, -4.7560, -4.6949],\n",
      "         [-4.6104, -4.5624, -4.7956,  ..., -4.7177, -4.7500, -4.6995],\n",
      "         ...,\n",
      "         [-4.6551, -4.6545, -4.7683,  ..., -4.7261, -4.6804, -4.6959],\n",
      "         [-4.6592, -4.6552, -4.7607,  ..., -4.7244, -4.6884, -4.6968],\n",
      "         [-4.6671, -4.6587, -4.7476,  ..., -4.7203, -4.7028, -4.6958]],\n",
      "\n",
      "        [[-4.6029, -4.5623, -4.8060,  ..., -4.7237, -4.7605, -4.6946],\n",
      "         [-4.6115, -4.5684, -4.7967,  ..., -4.7243, -4.7579, -4.6959],\n",
      "         [-4.6162, -4.5612, -4.7962,  ..., -4.7158, -4.7531, -4.7010],\n",
      "         ...,\n",
      "         [-4.6551, -4.6547, -4.7684,  ..., -4.7266, -4.6802, -4.6958],\n",
      "         [-4.6592, -4.6554, -4.7609,  ..., -4.7249, -4.6882, -4.6966],\n",
      "         [-4.6671, -4.6588, -4.7478,  ..., -4.7207, -4.7027, -4.6956]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.6247, -4.5662, -4.8033,  ..., -4.7228, -4.7778, -4.6916],\n",
      "         [-4.6393, -4.5847, -4.7915,  ..., -4.7218, -4.7851, -4.6900],\n",
      "         [-4.6808, -4.6072, -4.8162,  ..., -4.7444, -4.7647, -4.6676],\n",
      "         ...,\n",
      "         [-4.6551, -4.6547, -4.7684,  ..., -4.7266, -4.6802, -4.6958],\n",
      "         [-4.6592, -4.6554, -4.7609,  ..., -4.7249, -4.6882, -4.6966],\n",
      "         [-4.6671, -4.6588, -4.7478,  ..., -4.7207, -4.7027, -4.6956]],\n",
      "\n",
      "        [[-4.6007, -4.5628, -4.8056,  ..., -4.7245, -4.7594, -4.6939],\n",
      "         [-4.6079, -4.5693, -4.7961,  ..., -4.7256, -4.7560, -4.6949],\n",
      "         [-4.6104, -4.5625, -4.7956,  ..., -4.7177, -4.7500, -4.6995],\n",
      "         ...,\n",
      "         [-4.6595, -4.6556, -4.7676,  ..., -4.7270, -4.6815, -4.6947],\n",
      "         [-4.6621, -4.6559, -4.7596,  ..., -4.7241, -4.6890, -4.6964],\n",
      "         [-4.6691, -4.6592, -4.7464,  ..., -4.7194, -4.7034, -4.6960]],\n",
      "\n",
      "        [[-4.6007, -4.5628, -4.8056,  ..., -4.7245, -4.7594, -4.6939],\n",
      "         [-4.6079, -4.5693, -4.7961,  ..., -4.7256, -4.7560, -4.6949],\n",
      "         [-4.6104, -4.5624, -4.7956,  ..., -4.7177, -4.7500, -4.6995],\n",
      "         ...,\n",
      "         [-4.6551, -4.6547, -4.7684,  ..., -4.7266, -4.6802, -4.6958],\n",
      "         [-4.6592, -4.6554, -4.7609,  ..., -4.7249, -4.6882, -4.6966],\n",
      "         [-4.6671, -4.6588, -4.7478,  ..., -4.7207, -4.7027, -4.6956]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "pred_sizes shape: tensor([2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247])\n",
      "torch.Size([10, 2247, 112]) torch.Size([55]) tensor([2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247, 2247]) tensor([ 5,  2,  5, 20,  2,  1,  2,  7, 10,  1], dtype=torch.int32)\n",
      "10 10 55 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb Cell 23\u001b[0m line \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m val_dataloader \u001b[39m=\u001b[39m val_loader  \u001b[39m# Assuming you have val_loader defined\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(params[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, criterion, optimizer, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39m# val_loss = validate(model, val_dataloader, criterion, device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()  \u001b[39m# Adjust learning rate\u001b[39;00m\n",
      "\u001b[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb Cell 23\u001b[0m line \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mshape, targets\u001b[39m.\u001b[39mshape, pred_sizes, lengths)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(pred_sizes), \u001b[39mlen\u001b[39m(lengths), \u001b[39mlen\u001b[39m(targets), \u001b[39mlen\u001b[39m(outputs))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m), targets, pred_sizes, lengths)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb Cell 23\u001b[0m line \u001b[0;36mCustomCTCLoss.forward\u001b[0;34m(self, logits, labels, prediction_sizes, target_sizes)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m EPS \u001b[39m=\u001b[39m \u001b[39m1e-7\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctc_loss(logits, labels, prediction_sizes, target_sizes)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msanitize(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug(loss, logits, labels, prediction_sizes, target_sizes)\n",
      "\u001b[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb Cell 23\u001b[0m line \u001b[0;36mCustomCTCLoss.sanitize\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(loss\u001b[39m.\u001b[39mitem() \u001b[39m-\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39m<\u001b[39m EPS:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mzeros_like(loss)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m math\u001b[39m.\u001b[39misnan(loss\u001b[39m.\u001b[39mitem()):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mzeros_like(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X63sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from custom_ocr_cnn_lstm.cnn_lstm_ocr import CNNLSTM_OCR\n",
    "import math\n",
    "\n",
    "# Define your model, criterion, optimizer, and other parameters here\n",
    "\n",
    "# Initialize your model, criterion, and optimizer\n",
    "model = CNNLSTM_OCR(params)  # Assuming you've already defined CNNLSTM_OCR class\n",
    "# criterion = nn.CTCLoss()\n",
    "criterion = CustomCTCLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'])\n",
    "\n",
    "# Define training loop\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = batch['image'].to(device)\n",
    "        targets = batch['label']\n",
    "        # print(targets)\n",
    "        targets, lengths = converter.encode(targets)\n",
    "        print(\"len targets: \", len(targets))\n",
    "        # print(targets, lengths)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"cnn input inputs.shape:\", inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.contiguous().cpu()\n",
    "        outputs = torch.nn.functional.log_softmax(outputs, 2)\n",
    "        # b is batch size, T is input sequence and h is hidden size\n",
    "        b, T, h = outputs.size()\n",
    "        print(\"outputs shape:\", outputs)\n",
    "        # print(\"targets shape:\", targets.size())\n",
    "        pred_sizes = torch.LongTensor([T for i in range(b)])\n",
    "        print(\"pred_sizes shape:\", pred_sizes)\n",
    "        # Calculate the CTC loss\n",
    "        # output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "        print(outputs.shape, targets.shape, pred_sizes, lengths)\n",
    "        print(len(pred_sizes), len(lengths), len(targets), len(outputs))\n",
    "        loss = criterion(outputs.transpose(0, 1), targets, pred_sizes, lengths)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Define validation method\n",
    "# def validate(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "#             inputs = batch['image'].to(device)\n",
    "#             targets = batch['label']\n",
    "#             # Assuming targets need to be converted to tensor, e.g., using a function `text_to_tensor`\n",
    "#             targets = text_to_tensor(targets) \n",
    "\n",
    "#             outputs = model(inputs)\n",
    "            \n",
    "#             # Calculate the CTC loss\n",
    "#             output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "#             loss = criterion(outputs, targets, input_lengths=None, target_lengths=output_lengths)\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if params['cuda'] and torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_dataloader = train_loader  # Assuming you have train_loader defined\n",
    "val_dataloader = val_loader  # Assuming you have val_loader defined\n",
    "\n",
    "for epoch in range(params['epochs']):\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "    # val_loss = validate(model, val_dataloader, criterion, device)\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{params['epochs']} - Train Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), params['model_dir'] + '/model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s_learning_seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6a6a2f4e9b3d29343f27ef6aef311b54e340d4b6fc29835f858df5c984e5196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
