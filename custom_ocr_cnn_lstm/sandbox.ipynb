{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Pytorch\n",
    "to create endocder decoder ocr model with cnn and lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar\n",
      "Absolute path: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/PubTabNet_cropped\n"
     ]
    }
   ],
   "source": [
    "# Setup path in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Get absolut path to proeject root\n",
    "load_dotenv()\n",
    "project_root_dir = os.path.dirname(os.path.abspath(\"./\"))\n",
    "print(\"Project root dir:\", project_root_dir)\n",
    "\n",
    "data_dir = os.getenv(\"PUBTABNET_CROPPED_DATA_DIR\")\n",
    "absolute_dir = project_root_dir + data_dir\n",
    "print(\"Absolute path:\", absolute_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_train = f\"{absolute_dir}/train\"\n",
    "image_dir_val = f\"{absolute_dir}/val\"\n",
    "label_file_train = f\"{absolute_dir}/PubTabNet_2.0.0_train_separated.json\"\n",
    "label_file_val = f\"{absolute_dir}/PubTabNet_2.0.0_val_separated.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from torch.utils.data import DataLoader\n",
    "label_file_small = f\"{absolute_dir}/subset_small.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_file_train = f\"{absolute_dir}/PubTabNet_2.0.0_train.json\"\n",
    "# label_file_val = f\"{absolute_dir}/PubTabNet_2.0.0_val.json\"\n",
    "\n",
    "# # Function to generate labels\n",
    "# import json\n",
    "# def generate_labels(json_data):\n",
    "#     labels = []\n",
    "#     for key, value in json_data.items():\n",
    "#         cells = value[\"html\"][\"cells\"]\n",
    "#         for i, cell in enumerate(cells):\n",
    "#             bbox = cell.get('bbox')\n",
    "#             tokens = cell.get('tokens')\n",
    "#             if bbox is None:\n",
    "#                 continue\n",
    "#             label = key.replace(\".png\", f\"_bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.png\")\n",
    "#             label = {\n",
    "#                 \"filename\": key.replace(\".png\", f\"_bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}.png\"),\n",
    "#                 \"split\": value[\"split\"],\n",
    "#                 \"imgid\": value[\"imgid\"],\n",
    "#                 \"tokens\": tokens,\n",
    "#                 \"bbox\": bbox,\n",
    "#             }\n",
    "#             labels.append(label)\n",
    "#     return labels\n",
    "\n",
    "# # Generate labels\n",
    "# with open(label_file_val, 'r') as f:\n",
    "#         labels = json.load(f)\n",
    "#         result = generate_labels(labels)\n",
    "\n",
    "# # Specify the output file name\n",
    "# output_file_name = f\"{absolute_dir}/PubTabNet_2.0.0_val_separated.json\"\n",
    "\n",
    "# # Write the generated labels to a new JSON file\n",
    "# with open(output_file_name, 'w') as output_file:\n",
    "#     json.dump(result, output_file, indent=4)\n",
    "\n",
    "# print(f\"Generated labels have been written to {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform_list =  [transforms.Grayscale(1),\n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.5,), (0.5,))]\n",
    "transform = transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'label': 'Species', 'image': tensor([[[ 1.0000,  0.9451,  0.5765,  0.7569,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  0.9294,  0.8902,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.0196,  0.9686,  0.5922,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  0.8824,  0.8353,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000, -0.2941,  0.8353,  1.0000,  0.8980,  0.5765,  0.3961,\n",
      "           0.7412,  1.0000,  0.8039,  0.5529,  0.9608,  0.9294,  0.4745,\n",
      "           0.6392,  0.7490,  0.7804,  1.0000,  0.7020,  0.6078,  1.0000,\n",
      "           0.7020,  0.5608,  1.0000],\n",
      "         [ 1.0000,  0.6235, -0.3804,  0.4588,  0.7804, -0.2000,  0.6157,\n",
      "          -0.2706,  0.8980,  0.0118,  0.3490,  0.3725,  0.1765,  0.9765,\n",
      "           0.6863,  0.4275,  0.4039,  0.5608,  0.2314,  0.0588,  0.7020,\n",
      "           0.0588,  0.7255,  1.0000],\n",
      "         [ 0.9451,  0.9686,  0.9294, -0.2706,  0.9765, -0.0353,  1.0000,\n",
      "           0.0196,  0.6235,  0.2392,  1.0000,  0.9765, -0.1294,  1.0000,\n",
      "           1.0000,  0.5373,  0.4039,  0.2314,  0.6784,  1.0000,  1.0000,\n",
      "           0.2627, -0.1529,  1.0000],\n",
      "         [ 0.9137,  0.1294,  0.6078,  0.2627,  0.9765, -0.3333,  0.6314,\n",
      "           0.3725,  0.9686, -0.2471,  0.3333,  0.7569,  0.0824, -0.0275,\n",
      "           0.4745,  0.3333,  0.2078,  0.7098, -0.2078,  0.4275,  0.6941,\n",
      "           0.4275,  0.1294,  0.9686],\n",
      "         [ 1.0000,  0.9843,  0.9608,  1.0000,  0.9765, -0.0431,  0.9294,\n",
      "           1.0000,  1.0000,  0.9922,  0.9529,  1.0000,  1.0000,  0.9608,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  0.9686,  0.9843,  1.0000,\n",
      "           0.9529,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  0.7176,  0.0980,  0.9059,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000]]])}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "from custom_ocr_cnn_lstm.dataset import OCRDataset\n",
    "from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "train_dataset = OCRDatasetCropped(label_file_train, image_dir_train, transform=transform)\n",
    "val_dataset = OCRDatasetCropped(label_file_val, image_dir_val, transform=transform)\n",
    "\n",
    "# Accessing a sample\n",
    "sample = train_dataset[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Height: 84\n",
      "Maximum Width: 429\n"
     ]
    }
   ],
   "source": [
    "max_height = 0\n",
    "max_width = 0\n",
    "\n",
    "for item in train_dataset:\n",
    "    image = item['image']\n",
    "    height, width = image.shape[1], image.shape[2]\n",
    "    max_height = max(max_height, height)\n",
    "    max_width = max(max_width, width)\n",
    "\n",
    "print(f\"Maximum Height: {max_height}\")\n",
    "print(f\"Maximum Width: {max_width}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SynthCollator(object):\n",
    "    \n",
    "#     def __call__(self, batch):\n",
    "#         width = [item['image'].shape[2] for item in batch]\n",
    "#         indexes = [item['idx'] for item in batch]\n",
    "#         imgs = torch.ones([len(batch), batch[0]['image'].shape[0], batch[0]['image'].shape[1], \n",
    "#                            max(width)], dtype=torch.float32)\n",
    "#         for idx, item in enumerate(batch):\n",
    "#             try:\n",
    "#                 imgs[idx, :, :, 0:item['image'].shape[2]] = item['image']\n",
    "#             except:\n",
    "#                 continue\n",
    "#                 #print(imgs.shape)\n",
    "#         item = {'image': imgs, 'idx':indexes}\n",
    "#         if 'label' in batch[0].keys():\n",
    "#             labels = [item['label'] for item in batch]\n",
    "#             item['label'] = labels\n",
    "#         return item\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCollator(object):\n",
    "    \n",
    "    def __init__(self, target_height = max_height, target_width= max_width):\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        indexes = [item['idx'] for item in batch]\n",
    "        \n",
    "        # Resize images to the target size\n",
    "        resized_images = []\n",
    "        for item in batch:\n",
    "            image = item['image']\n",
    "            resized_image = F.interpolate(image.unsqueeze(0), size=(self.target_height, self.target_width), mode='bilinear', align_corners=False)\n",
    "            resized_images.append(resized_image.squeeze(0))\n",
    "        \n",
    "        # Stack resized images\n",
    "        imgs = torch.stack(resized_images, dim=0)\n",
    "        \n",
    "        item = {'image': imgs, 'idx': indexes}\n",
    "        \n",
    "        if 'label' in batch[0].keys():\n",
    "            labels = [item['label'] for item in batch]\n",
    "            item['label'] = labels\n",
    "        \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "from custom_ocr_cnn_lstm.dataset import OCRDataset\n",
    "from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=CustomCollator())\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=True, collate_fn=CustomCollator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "torch.Size([10, 1, 84, 429])\n",
      "0 tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]) 33.82 ± 0.76\n"
     ]
    }
   ],
   "source": [
    "# Vielleicht liegt der Fehler hier. \n",
    "# Batch enthält jeweils 3 items die dann 10 elemente jeweils enthalten\n",
    "for batch_idx, samples in enumerate(train_loader):\n",
    "    print(\"------\")\n",
    "    print(samples[\"image\"].shape)\n",
    "    print(batch_idx, samples[\"image\"][0], samples[\"label\"][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_xml_tags(input_string):\n",
    "    pattern = r'<[^>]+>'\n",
    "    return re.sub(pattern, '', input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth labels loaded:  13690 [' S p e c i e s ', ' A n a j a ́ s ', ' P o r t e l ', ' S S B V ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' W i l d ', ' R u r a l ', ' U r b a n ', ' T o t a l ', ' ( % ) ', ' E v a n d r o m y i a   w a l k e r i ', '4 4', '4 0', '1', '1 5 6', '2 9 6', '1', '5 1', '1', '2', '5 9 2', '6 8 . 8 4', ' E v a n d r o m y i a   i n f r a s p i n o s a ', '4 4', '0', '3', '8 2', '1', '0', '0', '0', '0', '1 3 0', '1 5 . 1 2', ' N y s s o m y i a   a n t u n e s i  a  ', '1 1', '3', '3', '2 0', '3', '0', '1', '0', '0', '4 1', '4 . 7 7', ' M i c r o p y g o m y i a   r o r o t a e n s i s ', '2 0', '1', '0', '4', '0', '0', '2', '0', '0', '2 7', '3 . 1 4', ' S c i o p e m y i a   s o r d e l l i i ', '7', '1', '0', '1 3', '2', '0', '2', '0', '0', '2 5', '2 . 9 1', ' B i c h r o m o m y i a   f l a v i s c u t e l l a t a  a  ', '0', '0', '0', '4', '0', '0', '1 6', '0', '0', '2 0', '2 . 3 2', ' N y s s o m y i a   y u i l l i   y u i l l i ', '4', '0', '0', '0', '0', '0', '0', '0', '0', '4', '0 . 4 6', ' P s a t h y r o m y i a   a r a g a o i ']\n",
      "Character alphabet file created at: /Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/PubTabNet_cropped/character_alphabet.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def create_alphabet_file(alphabet, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        # f.write(\"START\\n\")\n",
    "        for char in alphabet:\n",
    "            f.write(char + \"\\n\")\n",
    "        # f.write(\"END\\n\")\n",
    "\n",
    "# Replace these with your actual ground truth labels\n",
    "train_labels = train_dataset.labels\n",
    "val_labels = train_dataset.labels\n",
    "# Concat both dicts\n",
    "labels = train_labels + val_labels\n",
    "\n",
    "alph_labels = []\n",
    "for label in labels:\n",
    "    label_tokens = label.get('tokens')\n",
    "    label = \" \".join(\" \".join(tokens) for tokens in label_tokens)\n",
    "    label = remove_xml_tags(label)\n",
    "    alph_labels.append(label)\n",
    "print(\"Ground truth labels loaded: \", len(alph_labels), alph_labels[:100])\n",
    "\n",
    "unique_chars = set()\n",
    "unique_radicals = set()\n",
    "\n",
    "# Loop through ground truth labels to extract unique characters and radicals\n",
    "for label in alph_labels:\n",
    "    for char in label:\n",
    "        unique_chars.add(char)\n",
    "        # You might need to extract radicals from each character here if using decomposition\n",
    "\n",
    "# Define the file paths for character and radical alphabets\n",
    "char_alphabet_file_path = f\"{absolute_dir}/character_alphabet.txt\"\n",
    "radical_alphabet_file_path = f\"{absolute_dir}/radical_alphabet.txt\"\n",
    "\n",
    "# Create character alphabet file\n",
    "create_alphabet_file(unique_chars, char_alphabet_file_path)\n",
    "print(f\"Character alphabet file created at: {char_alphabet_file_path}\")\n",
    "\n",
    "# Create radical alphabet file (if needed)\n",
    "# create_alphabet_file(unique_radicals, radical_alphabet_file_path)\n",
    "# print(f\"Radical alphabet file created at: {radical_alphabet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://deepayan137.github.io/blog/markdown/2020/08/29/building-ocr.html#the-ctc-loss\n",
    "class CustomCTCLoss(torch.nn.Module):\n",
    "    # T x B x H => Softmax on dimension 2\n",
    "    def __init__(self, dim=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "\n",
    "    def forward(self, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        EPS = 1e-7\n",
    "        loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes)\n",
    "        loss = self.sanitize(loss)\n",
    "        return self.debug(loss, logits, labels, prediction_sizes, target_sizes)\n",
    "    \n",
    "    def sanitize(self, loss):\n",
    "        EPS = 1e-7\n",
    "        if abs(loss.item() - float('inf')) < EPS:\n",
    "            return torch.zeros_like(loss)\n",
    "        if math.isnan(loss.item()):\n",
    "            return torch.zeros_like(loss)\n",
    "        return loss\n",
    "\n",
    "    def debug(self, loss, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Loss:\", loss)\n",
    "            print(\"logits:\", logits)\n",
    "            print(\"labels:\", labels)\n",
    "            print(\"prediction_sizes:\", prediction_sizes)\n",
    "            print(\"target_sizes:\", target_sizes)\n",
    "            raise Exception(\"NaN loss obtained. But why?\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet loaded: 111 ['<', 'I', '&', 'O', 'Q', '‐', 'G', '–', '#', 'p', '“', 'A', 'P', 'l', 'D', '§', '≤', '⁄', '·', '+', '`', 'α', '.', ' ', ',', 'B', 'Δ', 'a', '7', '>', '≥', 'k', ']', '°', '6', '0', 'U', 'W', '%', '8', 'L', 'w', 'i', 'F', 'V', 'n', 'y', 'o', '=', 'd', 'H', 'z', 'v', '$', '[', 'S', '́', 'q', 'T', 'Y', '5', '(', 'c', 'j', 'J', 'b', 'x', 'Z', '′', 'E', '×', 'C', 't', '—', 'K', 'f', 's', '1', 'γ', '4', '”', '*', 'g', '’', '†', '/', '2', ')', 'u', \"'\", 'X', 'M', 'R', ':', 'e', '_', 'μ', 'λ', '→', 'h', '±', 'N', ';', '?', 'β', '9', 'r', '−', '-', '3', 'm']\n"
     ]
    }
   ],
   "source": [
    "# Load alpabet from file\n",
    "def load_alphabet(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        alphabet = f.read().splitlines()\n",
    "    return alphabet\n",
    "\n",
    "alphabet = load_alphabet(char_alphabet_file_path)\n",
    "print(\"Alphabet loaded:\", len(alphabet), alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum width in dataset: 454\n"
     ]
    }
   ],
   "source": [
    "def find_maximum_width(dataset):\n",
    "    # Initialize a list to store image widths\n",
    "    image_widths = []\n",
    "\n",
    "    # Iterate through the dataset and collect image widths\n",
    "    for sample in dataset:\n",
    "        image_width = sample['image'].shape[2]  # Get the width of the image\n",
    "        image_widths.append(image_width)\n",
    "\n",
    "    # Find the maximum width across all images\n",
    "    max_width = max(image_widths)\n",
    "\n",
    "    return max_width\n",
    "\n",
    "# Example usage:\n",
    "max_width_train = find_maximum_width(train_dataset)\n",
    "max_width_val = find_maximum_width(val_dataset)\n",
    "\n",
    "max_width = max(max_width_train, max_width_val)\n",
    "\n",
    "print(\"Maximum width in dataset:\", max_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'input_dim': 32, 'alphabet': ['<', 'I', '&', 'O', 'Q', '‐', 'G', '–', '#', 'p', '“', 'A', 'P', 'l', 'D', '§', '≤', '⁄', '·', '+', '`', 'α', '.', ' ', ',', 'B', 'Δ', 'a', '7', '>', '≥', 'k', ']', '°', '6', '0', 'U', 'W', '%', '8', 'L', 'w', 'i', 'F', 'V', 'n', 'y', 'o', '=', 'd', 'H', 'z', 'v', '$', '[', 'S', '́', 'q', 'T', 'Y', '5', '(', 'c', 'j', 'J', 'b', 'x', 'Z', '′', 'E', '×', 'C', 't', '—', 'K', 'f', 's', '1', 'γ', '4', '”', '*', 'g', '’', '†', '/', '2', ')', 'u', \"'\", 'X', 'M', 'R', ':', 'e', '_', 'μ', 'λ', '→', 'h', '±', 'N', ';', '?', 'β', '9', 'r', '−', '-', '3', 'm'], 'hidden_dim': 256, 'output_dim': 112, 'input_planes': 1, 'planes': 32, 'image_height': 32, 'number_channels': 1, 'number_hidden_layers': 2, 'len_alphabet': 111, 'learning_rate': 0.001, 'epochs': 1, 'batch_size': 10, 'model_dir': 'model_history', 'log_dir': 'logs', 'resume': False, 'cuda': False, 'schedule': False, 'max_width': 454}\n"
     ]
    }
   ],
   "source": [
    "# Setup environment for training\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from utils import OCRLabelConverter\n",
    "from tqdm import *\n",
    "from cnn_lstm_ocr import CNNLSTM_OCR\n",
    "\n",
    "# params = {\n",
    "#     \"input_dim\": 256,\n",
    "#     \"hidden_dim\": 256,\n",
    "#     \"output_dim\": 1,\n",
    "#     \"input_planes\": 1,\n",
    "#     \"planes\": 1,\n",
    "#     \"schedule\": False,\n",
    "#     'image_height':32,\n",
    "#     'number_channels':1,\n",
    "#     'number_hidden_layers':256,\n",
    "#     'len_alphabet':len(alphabet),\n",
    "#     'learning_rate':0.001,\n",
    "#     'epochs':4,\n",
    "#     'batch_size':10,\n",
    "#     'model_dir':'model_history',\n",
    "#     'log_dir':'logs',\n",
    "#     'resume':False,\n",
    "#     'cuda':False,\n",
    "#     'schedule':False    \n",
    "# }\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"input_dim\": 1,  # Assuming grayscale images\n",
    "    \"alphabet\": alphabet,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"output_dim\": len(alphabet) + 1,  # Number of classes (including a blank symbol)\n",
    "    \"input_planes\": 1,  # Assuming grayscale images\n",
    "    \"planes\": 32,  # Adjust this value as needed\n",
    "    'image_height': 32,  # Adjust as needed\n",
    "    'number_channels': 1,  # Assuming grayscale images\n",
    "    'number_hidden_layers': 2,  # Adjust as needed\n",
    "    'len_alphabet': len(alphabet),  # Number of classes (excluding a blank symbol)\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 1,\n",
    "    'batch_size': 10,\n",
    "    'model_dir': 'model_history',\n",
    "    'log_dir': 'logs',\n",
    "    'resume': False,\n",
    "    'cuda': False,\n",
    "    'schedule': False,\n",
    "    'max_width': max_width\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = CNNLSTM_OCR(params)\n",
    "converter = OCRLabelConverter(''.join(params['alphabet']))\n",
    "criterion = CustomCTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'])\n",
    "batch_size = params['batch_size']\n",
    "count = 1\n",
    "epochs = params['epochs']\n",
    "cuda = params['cuda']\n",
    "print(\"Params:\", params)\n",
    "\n",
    "def train_dataloader():\n",
    "        # logging.info('training data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_train,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn,\n",
    "                shuffle=True)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import transforms\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from tqdm import tqdm\n",
    "# from custom_ocr_cnn_lstm.dataset_cropped import OCRDatasetCropped\n",
    "\n",
    "# # Define a method for training the OCR model\n",
    "# def train_ocr_model(model, train_loader, optimizer, criterion, device, epoch, epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     progress_bar = tqdm(train_loader, desc='Epoch: [%d]/[%d] Training'%(epoch, \n",
    "#                 epochs), leave=True)\n",
    "\n",
    "#     for idx, batch in enumerate(progress_bar):\n",
    "#         optimizer.zero_grad()\n",
    "#         images, labels = batch['image'], batch['label']\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "        \n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         loss.backward()\n",
    "#         clip_grad_norm_(model.parameters(), max_norm=5)  # Clip gradients to prevent exploding gradients\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     return avg_loss\n",
    "\n",
    "# # Define a method for evaluating the OCR model on the validation dataset\n",
    "# def evaluate_ocr_model(model, val_loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(val_loader)\n",
    "#     return avg_loss\n",
    "\n",
    "# # Define a method for OCR inference\n",
    "# def ocr_inference(model, image, transform, device):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Preprocess the input image using the same transform as during training\n",
    "#         image = transform(image).unsqueeze(0).to(device)\n",
    "#         outputs = model(image)\n",
    "        \n",
    "#         # Perform any necessary post-processing on the model's outputs to get the OCR result\n",
    "#         # You may need to implement this part based on your specific OCR task\n",
    "    \n",
    "#     return ocr_result\n",
    "\n",
    "\n",
    "# # Set device (CPU or GPU) for training\n",
    "# device = torch.device(\"cuda\" if params['cuda'] and torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(params['epochs']):\n",
    "#     print(f\"Epoch {epoch + 1}/{params['epochs']}\")\n",
    "    \n",
    "#     # Train the model\n",
    "#     train_loss = train_ocr_model(model, train_loader, optimizer, criterion, device, epoch, params['epochs'])\n",
    "#     print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "#     # Validate the model\n",
    "#     val_loss = evaluate_ocr_model(model, val_loader, criterion, device)\n",
    "#     print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# # After training, you can use the trained model for inference on new images\n",
    "# # Example usage for OCR inference:\n",
    "# sample_image, sample_label = train_dataset[0]  # Load a sample image and label\n",
    "# ocr_result = ocr_inference(model, sample_image, transform, device)\n",
    "# print(\"OCR Result:\", ocr_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a mapping from characters to integers\n",
    "# char_to_int = {}\n",
    "# int_to_char = {}\n",
    "# with open(char_alphabet_file_path, 'r') as f:\n",
    "#     alphabet_lines = f.read().splitlines()\n",
    "\n",
    "# for i, char in enumerate(alphabet_lines[1:-1]):  # Skip \"START\" and \"END\" lines\n",
    "#     char_to_int[char] = i\n",
    "#     int_to_char[i] = char\n",
    "\n",
    "# # Define a special key for space character\n",
    "# SPACE_INT = len(char_to_int)  # Use the last index in the alphabet\n",
    "\n",
    "# # Implement text_to_tensor method\n",
    "# def text_to_tensor(text):\n",
    "#     # Convert text to a list of integers using the char_to_int mapping\n",
    "#     tensor = [char_to_int[char] if char in char_to_int else SPACE_INT for char in text]\n",
    "#     length = [len(text)]\n",
    "#     # return tensor\n",
    "#     return (torch.IntTensor(tensor), torch.IntTensor(length))\n",
    "\n",
    "# # Implement tensor_to_text method\n",
    "# def tensor_to_text(tensor):\n",
    "#     # Convert a tensor (list of integers) back to text using the int_to_char mapping\n",
    "#     text = \"\".join([int_to_char[idx] if idx != SPACE_INT else ' ' for idx in tensor])\n",
    "#     return text\n",
    "\n",
    "# # Example usage:\n",
    "# text = \"HELLO 5 7\"\n",
    "# tensor = text_to_tensor(text)\n",
    "# print(\"Text to Tensor:\", tensor)\n",
    "# text_reconstructed = tensor_to_text(tensor)\n",
    "# print(\"Tensor to Text:\", text_reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 685/685 [26:05<00:00,  2.28s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb Cell 23\u001b[0m line \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X64sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39m# val_loss = validate(model, val_dataloader, criterion, device)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X64sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()  \u001b[39m# Adjust learning rate\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X64sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mparams[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m - Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m - Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X64sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39m# Save the trained model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#X64sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), params[\u001b[39m'\u001b[39m\u001b[39mmodel_dir\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/model.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_loss' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from custom_ocr_cnn_lstm.cnn_lstm_ocr import CNNLSTM_OCR\n",
    "import math\n",
    "\n",
    "# Define your model, criterion, optimizer, and other parameters here\n",
    "\n",
    "# Initialize your model, criterion, and optimizer\n",
    "model = CNNLSTM_OCR(params)  # Assuming you've already defined CNNLSTM_OCR class\n",
    "# criterion = nn.CTCLoss()\n",
    "criterion = CustomCTCLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=params['epochs'])\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Define training loop\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = batch['image'].to(device)\n",
    "        targets = batch['label']\n",
    "\n",
    "        targets_encoded = []\n",
    "        targets_encoded_length = []\n",
    "        for target in targets: \n",
    "            target, length = converter.encode(target)\n",
    "            targets_encoded.append(target)\n",
    "            targets_encoded_length.append(len(length))\n",
    "        \n",
    "        # Find the length of the longest tensor\n",
    "        max_len = max(len(t) for t in targets_encoded)\n",
    "\n",
    "        # Pad tensors to the length of the longest tensor\n",
    "        padded_tensors = [F.pad(t, (0, max_len - len(t)), value=112) for t in targets_encoded]\n",
    "\n",
    "        # Stack the padded tensors\n",
    "        targets_encoded = torch.stack(padded_tensors)\n",
    "\n",
    "        # target lengths as tensor\n",
    "        targets_encoded_length = torch.IntTensor(targets_encoded_length)\n",
    "\n",
    "        #print(\"#\"*50)\n",
    "        #print(f\"Input shape: {inputs.shape}\")\n",
    "        #print(f\"size targets: {targets_encoded.shape}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"cnn input inputs.shape:\", inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.contiguous().cpu()\n",
    "        outputs = torch.nn.functional.log_softmax(outputs, 2)\n",
    "        # b is batch size, T is input sequence and h is hidden size\n",
    "        b, T, h = outputs.size()\n",
    "        #print(\"outputs shape:\", outputs.shape)\n",
    "        #print(outputs.transpose(0, 1).shape)\n",
    "        # print(\"targets shape:\", targets.size())\n",
    "        pred_sizes = torch.LongTensor([T for i in range(b)])\n",
    "        #print(\"pred_sizes shape:\", pred_sizes.shape)\n",
    "        # Calculate the CTC loss\n",
    "        # output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "        #print(outputs.shape, targets.shape, pred_sizes, lengths)\n",
    "        #print(len(pred_sizes), len(lengths), len(targets), len(outputs))\n",
    "        loss = criterion(outputs.transpose(0, 1), targets_encoded, pred_sizes, targets_encoded_length)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Define validation method\n",
    "# def validate(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "#             inputs = batch['image'].to(device)\n",
    "#             targets = batch['label']\n",
    "#             # Assuming targets need to be converted to tensor, e.g., using a function `text_to_tensor`\n",
    "#             targets = text_to_tensor(targets) \n",
    "\n",
    "#             outputs = model(inputs)\n",
    "            \n",
    "#             # Calculate the CTC loss\n",
    "#             output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "#             loss = criterion(outputs, targets, input_lengths=None, target_lengths=output_lengths)\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if params['cuda'] and torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_dataloader = train_loader  # Assuming you have train_loader defined\n",
    "val_dataloader = val_loader  # Assuming you have val_loader defined\n",
    "\n",
    "for epoch in range(params['epochs']):\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "    # val_loss = validate(model, val_dataloader, criterion, device)\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{params['epochs']} - Train Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), params['model_dir'] + '/model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), params['model_dir'] + '/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textdistance\n",
      "  Downloading textdistance-4.5.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: textdistance\n",
      "Successfully installed textdistance-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, converter, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            inputs = batch['image'].to(device)\n",
    "            targets = batch['label']\n",
    "\n",
    "            targets_encoded = []\n",
    "            targets_encoded_length = []\n",
    "            for target in targets: \n",
    "                target, length = converter.encode(target)\n",
    "                targets_encoded.append(target)\n",
    "                targets_encoded_length.append(len(length))\n",
    "            \n",
    "            # Find the length of the longest tensor\n",
    "            max_len = max(len(t) for t in targets_encoded) + 2\n",
    "            \n",
    "            # Pad tensors to the length of the longest tensor\n",
    "            padded_tensors = [F.pad(t, (0, max_len - len(t)), value=0) for t in targets_encoded]\n",
    "\n",
    "            # Stack the padded tensors\n",
    "            # targets_encoded = torch.stack(padded_tensors)\n",
    "            targets_encoded = torch.stack(padded_tensors)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"cnn input inputs.shape:\", inputs.shape)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.contiguous().cpu()\n",
    "            outputs = torch.nn.functional.log_softmax(outputs, 2)\n",
    "            # b is batch size, T is input sequence and h is hidden size\n",
    "            b, T, h = outputs.size()\n",
    "            # print(\"outputs shape:\", outputs.shape)\n",
    "            # print(outputs.transpose(0, 1).shape)\n",
    "            # print(\"targets shape:\", targets.size())\n",
    "            pred_sizes = torch.LongTensor([T for i in range(b)])\n",
    "            pred_sizes = torch.LongTensor([T for i in range(b)]).to(device=device)\n",
    "\n",
    "            print(outputs)\n",
    "\n",
    "            #print(\"pred_sizes shape:\", pred_sizes.shape)\n",
    "            # Calculate the CTC loss\n",
    "            # output_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long)\n",
    "            #print(outputs.shape, targets.shape, pred_sizes, lengths)\n",
    "            #print(len(pred_sizes), len(lengths), len(targets), len(outputs))\n",
    "            total_loss += criterion(outputs.transpose(0, 1), targets_encoded, pred_sizes, targets_encoded_length)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "agv_loss_training = validate(model, val_dataloader, criterion=criterion, converter=converter, device=device)\n",
    "print(agv_loss_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Eval\n",
    "import matplotlib as plt\n",
    "def get_accuracy(loader, model, converter):\n",
    "    model.eval()\n",
    "    evaluator = Eval()\n",
    "    labels, predictions, images = [], [], []\n",
    "    for iteration, batch in enumerate(tqdm(loader)):\n",
    "        input_, targets = batch['image'].to(device), batch['label']\n",
    "        images.extend(input_.squeeze().detach())\n",
    "        labels.extend(targets)\n",
    "        targets, lengths = converter.encode(targets)\n",
    "        logits = model(input_).transpose(1, 0)\n",
    "        # print(logits)\n",
    "        logits = torch.nn.functional.log_softmax(logits, 2)\n",
    "        logits = logits.contiguous().cpu()\n",
    "        T, B, H = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T for i in range(B)])\n",
    "        probs, pos = logits.max(2)\n",
    "        print(probs, pos)\n",
    "\n",
    "        print(pos)\n",
    "        pos = pos.transpose(1, 0).contiguous().view(-1)\n",
    "        sim_preds = converter.decode(pos.data, pred_sizes.data, raw=False)\n",
    "        print(sim_preds)\n",
    "        predictions.extend(sim_preds)\n",
    "        \n",
    "#     make_grid(images[:10], nrow=2)\n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    columns = 4\n",
    "    rows = 5\n",
    "    pairs = list(zip(images, predictions))\n",
    "    indices = np.random.permutation(len(pairs))\n",
    "    for i in range(1, columns*rows +1):\n",
    "        img = images[indices[i]]\n",
    "        img = (img - img.min())/(img.max() - img.min())\n",
    "        img = np.array(img * 255.0, dtype=np.uint8)\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.title(predictions[indices[i]])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "    ca = np.mean((list(map(evaluator.char_accuracy, list(zip(predictions, labels))))))\n",
    "    wa = np.mean((list(map(evaluator.word_accuracy_line, list(zip(predictions, labels))))))\n",
    "    return ca, wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_history/model.pth\n",
      "Loading model model_history/model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/613 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/613 [00:00<05:54,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0499, -0.0509, -0.0498,  ..., -0.0503, -0.0498, -0.0498],\n",
      "        [-0.0067, -0.0062, -0.0067,  ..., -0.0068, -0.0066, -0.0066],\n",
      "        [-0.0015, -0.0016, -0.0015,  ..., -0.0014, -0.0015, -0.0015],\n",
      "        ...,\n",
      "        [-0.0033, -0.0064, -0.0033,  ..., -0.0033, -0.0033, -0.0033],\n",
      "        [-0.0159, -0.0215, -0.0159,  ..., -0.0159, -0.0159, -0.0159],\n",
      "        [-0.0629, -0.0620, -0.0629,  ..., -0.0629, -0.0629, -0.0629]],\n",
      "       grad_fn=<MaxBackward0>) tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "['', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/613 [00:01<06:35,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1892, -0.0498, -0.0498,  ..., -0.0498, -0.0499, -0.0515],\n",
      "        [-0.0628, -0.0067, -0.0066,  ..., -0.0066, -0.0067, -0.0065],\n",
      "        [-0.0122, -0.0015, -0.0015,  ..., -0.0015, -0.0015, -0.0020],\n",
      "        ...,\n",
      "        [-0.0033, -0.0033, -0.0033,  ..., -0.0033, -0.0033, -0.0033],\n",
      "        [-0.0159, -0.0159, -0.0159,  ..., -0.0159, -0.0159, -0.0159],\n",
      "        [-0.0629, -0.0629, -0.0629,  ..., -0.0629, -0.0629, -0.0629]],\n",
      "       grad_fn=<MaxBackward0>) tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "['', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/613 [00:01<06:27,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0498, -0.2000, -0.0498,  ..., -0.1892, -0.0498, -0.0498],\n",
      "        [-0.0066, -0.0789, -0.0066,  ..., -0.0628, -0.0067, -0.0066],\n",
      "        [-0.0015, -0.0178, -0.0015,  ..., -0.0122, -0.0015, -0.0015],\n",
      "        ...,\n",
      "        [-0.0033, -0.0144, -0.0033,  ..., -0.0033, -0.0033, -0.0033],\n",
      "        [-0.0159, -0.0462, -0.0159,  ..., -0.0159, -0.0159, -0.0159],\n",
      "        [-0.0629, -0.0987, -0.0629,  ..., -0.0629, -0.0629, -0.0629]],\n",
      "       grad_fn=<MaxBackward0>) tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "['', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/613 [00:02<06:22,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0498, -0.0498, -0.0498,  ..., -0.0498, -0.0499, -0.1892],\n",
      "        [-0.0066, -0.0067, -0.0066,  ..., -0.0066, -0.0067, -0.0628],\n",
      "        [-0.0015, -0.0015, -0.0015,  ..., -0.0015, -0.0015, -0.0122],\n",
      "        ...,\n",
      "        [-0.0033, -0.0033, -0.0033,  ..., -0.0033, -0.0033, -0.0059],\n",
      "        [-0.0159, -0.0159, -0.0159,  ..., -0.0159, -0.0159, -0.0250],\n",
      "        [-0.0629, -0.0629, -0.0629,  ..., -0.0629, -0.0629, -0.0760]],\n",
      "       grad_fn=<MaxBackward0>) tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "['', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/613 [00:03<06:17,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1892, -0.0498, -0.2021,  ..., -0.0499, -0.0498, -0.0498],\n",
      "        [-0.0628, -0.0066, -0.0823,  ..., -0.0067, -0.0066, -0.0066],\n",
      "        [-0.0122, -0.0015, -0.0192,  ..., -0.0015, -0.0015, -0.0015],\n",
      "        ...,\n",
      "        [-0.0059, -0.0033, -0.0033,  ..., -0.0033, -0.0033, -0.0033],\n",
      "        [-0.0250, -0.0159, -0.0159,  ..., -0.0159, -0.0159, -0.0159],\n",
      "        [-0.0760, -0.0629, -0.0629,  ..., -0.0629, -0.0629, -0.0629]],\n",
      "       grad_fn=<MaxBackward0>) tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "['', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/613 [00:03<07:25,  1.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb Cell 28\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#Y100sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model \u001b[39m=\u001b[39m CNNLSTM_OCR(params\u001b[39m=\u001b[39mparams)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#Y100sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(state_dict\u001b[39m=\u001b[39mstate_dict)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#Y100sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     ca, wa \u001b[39m=\u001b[39m get_accuracy(val_loader, model, converter)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#Y100sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCharacter Accuracy: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mWord Accuracy: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m(ca, wa))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#Y100sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb Cell 28\u001b[0m line \u001b[0;36mget_accuracy\u001b[0;34m(loader, model, converter)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#Y100sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m labels\u001b[39m.\u001b[39mextend(targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m targets, lengths \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39mencode(targets)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#Y100sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m logits \u001b[39m=\u001b[39m model(input_)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#Y100sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# print(logits)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leonremke/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/sandbox.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(logits, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/../custom_ocr_cnn_lstm/cnn_lstm_ocr.py:102\u001b[0m, in \u001b[0;36mCNNLSTM_OCR.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# [w, b, c]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m# Apply the LSTM decoder\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm_decoder(out)\n\u001b[1;32m    103\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)  \u001b[39m# [b, w, c]\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GIT_REPOS/UNI/neural_networks_seminar/custom_ocr_cnn_lstm/../custom_ocr_cnn_lstm/cnn_lstm_ocr.py:72\u001b[0m, in \u001b[0;36mLSTM_Decoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm\u001b[39m.\u001b[39mflatten_parameters()\n\u001b[1;32m     71\u001b[0m \u001b[39m# input dimensions of x: (sequence_length, batch_size, input_dim)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[1;32m     74\u001b[0m \u001b[39m# T = length sequence, b = batch size, h = hidden size\u001b[39;00m\n\u001b[1;32m     75\u001b[0m T, b, h \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msize()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DSTA-Ex/lib/python3.9/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    775\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_state_dir_path = params['model_dir']+\"/model.pth\"\n",
    "print(model_state_dir_path)\n",
    "if os.path.isfile(model_state_dir_path):\n",
    "    print('Loading model %s'%model_state_dir_path)\n",
    "    state_dict = torch.load(model_state_dir_path)\n",
    "    model = CNNLSTM_OCR(params=params)\n",
    "    model.load_state_dict(state_dict=state_dict)\n",
    "    ca, wa = get_accuracy(val_loader, model, converter)\n",
    "    print(\"Character Accuracy: %.2f\\nWord Accuracy: %.2f\"%(ca, wa))\n",
    "else:\n",
    "    print('Exiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s_learning_seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6a6a2f4e9b3d29343f27ef6aef311b54e340d4b6fc29835f858df5c984e5196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
