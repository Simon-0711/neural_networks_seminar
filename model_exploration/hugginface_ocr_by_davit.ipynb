{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image from the IAM dataset\n",
    "url = \"test_value.jpg\"\n",
    "#image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "image = Image.open(url).convert(\"RGB\")\n",
    "\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values)\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['42.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TableTransformerModel, TableTransformerConfig\n",
    "\n",
    "# Initializing a Table Transformer microsoft/table-transformer-detection style configuration\n",
    "table_configuration = TableTransformerConfig()\n",
    "\n",
    "# Initializing a model from the microsoft/table-transformer-detection style configuration\n",
    "table_model = TableTransformerModel(table_configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "table_configuration = table_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I518302/opt/anaconda3/envs/linguistik/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "Downloading model.safetensors: 100%|██████████| 46.8M/46.8M [00:00<00:00, 47.1MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TableTransformerModelOutput(last_hidden_state=tensor([[[ 0.4487, -0.7559, -0.2180,  ...,  0.1938,  0.4595, -1.1296],\n",
       "         [ 0.2736, -0.5557,  0.0671,  ...,  0.6335,  0.9419, -0.9986],\n",
       "         [ 0.6326, -0.9232, -0.4263,  ...,  0.3511,  0.5909, -1.2945],\n",
       "         ...,\n",
       "         [ 0.5576, -0.8320, -0.2713,  ...,  0.3339,  0.6743, -1.2074],\n",
       "         [ 0.5476, -0.9040, -0.3348,  ...,  0.2389,  0.5789, -1.2797],\n",
       "         [ 0.4587, -0.8317,  0.0977,  ...,  0.9194,  1.2264, -0.9535]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.0834,  0.0919,  0.0184,  ...,  0.6114,  0.0822, -0.2726],\n",
       "         [-0.0707,  0.0313,  0.0669,  ...,  0.8984,  0.1130, -0.2027],\n",
       "         [-0.0032,  0.0394,  0.0899,  ...,  0.8158,  0.1352, -0.3390],\n",
       "         ...,\n",
       "         [-0.0119,  0.1041, -0.0511,  ..., -0.3826,  0.1573, -0.6994],\n",
       "         [ 0.0343,  0.1397, -0.0409,  ..., -0.3188,  0.1878, -0.8159],\n",
       "         [ 0.0301,  0.1778, -0.0517,  ..., -0.4538,  0.2636, -0.7641]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None, intermediate_hidden_states=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, TableTransformerModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "\n",
    "file_path = hf_hub_download(repo_id=\"nielsr/example-pdf\", repo_type=\"dataset\", filename=\"example_pdf.png\")\n",
    "image = Image.open(file_path).convert(\"RGB\")\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/table-transformer-detection\")\n",
    "table_model = TableTransformerModel.from_pretrained(\"microsoft/table-transformer-detection\")\n",
    "\n",
    "# prepare image for the model\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# forward pass\n",
    "outputs = table_model(**inputs)\n",
    "\n",
    "# the last hidden states are the final query embeddings of the Transformer decoder\n",
    "# these are of shape (batch_size, num_queries, hidden_size)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': 'letter'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "# load document image\n",
    "#dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n",
    "#image = dataset[1][\"image\"]\n",
    "\n",
    "url = \"test_value.jpg\"\n",
    "image = Image.open(url).convert(\"RGB\")\n",
    "\n",
    "# prepare decoder inputs\n",
    "task_prompt = \"<s_rvlcdip>\"\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    decoder_input_ids=decoder_input_ids.to(device),\n",
    "    max_length=model.decoder.config.max_position_embeddings,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    num_beams=1,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "print(processor.token2json(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/I518302/.cache/huggingface/datasets/hf-internal-testing___parquet/hf-internal-testing--example-documents-2350012fcbf685c6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Input length of decoder_input_ids is 8, but `max_length` is set to 8. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_sequence': '<presentation/>'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n",
    "image = dataset[2][\"image\"]\n",
    "\n",
    "task_prompt = \"<s_cord-v2>\"\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    decoder_input_ids=decoder_input_ids.to(device),\n",
    "    max_length=model.decoder.config.max_position_embeddings,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    num_beams=1,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "print(processor.token2json(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|██████████| 362/362 [00:00<00:00, 335kB/s]\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 536/536 [00:00<00:00, 600kB/s]\n",
      "Downloading (…)tencepiece.bpe.model: 100%|██████████| 1.30M/1.30M [00:00<00:00, 20.5MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 4.02M/4.02M [00:00<00:00, 37.9MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 1.52k/1.52k [00:00<00:00, 1.09MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 335/335 [00:00<00:00, 390kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 4.74k/4.74k [00:00<00:00, 4.28MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 806M/806M [00:16<00:00, 49.5MB/s] \n",
      "Found cached dataset parquet (/Users/I518302/.cache/huggingface/datasets/hf-internal-testing___parquet/hf-internal-testing--example-documents-2350012fcbf685c6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'menu': {'nm': 'CINNAMON SUGAR', 'unitprice': '17,000', 'cnt': '1 x', 'price': '17,000'}, 'sub_total': {'subtotal_price': '17,000'}, 'total': {'total_price': '17,000', 'cashprice': '20,000', 'changeprice': '3,000'}}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "# load document image\n",
    "dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n",
    "image = dataset[2][\"image\"]\n",
    "\n",
    "# prepare decoder inputs\n",
    "task_prompt = \"<s_cord-v2>\"\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    decoder_input_ids=decoder_input_ids.to(device),\n",
    "    max_length=model.decoder.config.max_position_embeddings,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    num_beams=1,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "print(processor.token2json(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"test_table.jpg\"\n",
    "image = Image.open(url).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'menu': [{'nm': 'Pour 100 g', 'unitprice': 'portion - 200 g', 'cnt': '1', 'price': 'des AQR*'}, {'nm': 'energie 829 KJ-197 kcal 1658 KJ - 394 kcal', 'unitprice': '9,4g', 'cnt': '1', 'price': '20%'}, {'nm': 'Maiberes grasses', 'unitprice': '9,4g', 'cnt': '4,7g', 'price': '13%'}, {'nm': 'Dont acides gras satures', 'unitprice': '2,9g', 'cnt': '5,8g', 'price': '29%'}, {'nm': 'Glucides', 'cnt': '2,9', 'price': '20%'}, {'nm': 'Dont sucres', 'unitprice': '4,2g', 'cnt': '2,9', 'price': '5%'}, {'nm': 'Flores aimentaires', 'unitprice': '8,2g', 'cnt': '4,9', 'price': '40'}, {'nm': 'Sele', 'unitprice': '22g', 'cnt': '11g', 'price': '45 %'}], 'sub_total': {'subtotal_price': '3g', 'discount_price': '49%'}, 'total': {'total_price': '8 400 kJ/2000 kcal)'}}\n"
     ]
    }
   ],
   "source": [
    "# prepare decoder inputs\n",
    "task_prompt = \"<s_cord-v2>\"\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    decoder_input_ids=decoder_input_ids.to(device),\n",
    "    max_length=model.decoder.config.max_position_embeddings,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    num_beams=1,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "print(processor.token2json(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linguistik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
